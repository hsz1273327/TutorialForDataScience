# 数据的生命周期

和其他任何东西一样,数据也有生命周期,数据生命周期管理(data life cycle management,DLM)则是一种基于策略的方法,用于管理信息系统的数据在整个生命周期内的流动--从创建和初始存储到它过时被删除整个过程.大致可以分为如下几个大块

1. 创建阶段,从业务信息中提取有价值的部分构造成数据并放入整个数系统中进行流转

2. 热数据阶段,一般数据在越新的时候越有价值,刚进入系统的数据往往可以作为消息使用(流处理),稍微过时一点的数据也会被查询使用.再过期一些,使用近期数据的一些统计量往往可以比较客观的反映当前的业务状况.

3. 冷数据阶段,当数据失去了时效性,往往我们不会再直接使用它,而是将它作为一些离线统计任务或者预测模型的训练样本使用.

4. 删除阶段,当数据已经几乎不再被任何地方用到了,那从成本考虑我们往往会将其从这个数据系统中删除.

## 不同生命周期时数据的载体

由于机器成本和数据的价值的不同,我们通常会按上面的顺序选择从贵到便宜,从高性能到低性能的工具作为载体.


### 创建阶段

创建阶段时,我们希望数据尽量有时效性,因此主要的载体是消息队列或者广播器,如果对实时性要求很高,但可以接受消息丢失,那么我们可以选择redis的PUB/SUB模式;如果要保证消息的完整性和顺序性,则可以使用kafka来做.当然我们也可以将其组合使用.

### 热数据阶段

热数据阶段通常由两种载体

1. 流数据库,比如pg的插件piplinedb,kafka/redis+spark-streaming/dask+streamz,这些方案并没有太大区别,只是做好消费端的并行化处理防止消息不能及时处理玩阻塞就好,流数基本不会直接落地到数据库,而是被处理成需要的聚合量后再存入指定位置(业务数据库,特征库,模型库甚至直接推送给客户等).

2. 缓存数据库,由于常见的热数据都与时间密切相关,通常可以是时间序列数据库.这里就需要做出选择了,influxdb开源版本只有单机,配套工具并不多,而且查询不是完整的sql语言,表现力不足,限制也多,更关键的是数据量一大,写入一多就极易oom,因此不建议使用,更加推荐的是postgresql的插件timescaledb.它的内存使用率增长相对平缓,而且支持完整sql,虽然在吞吐量,数据压缩性能上比不上influxdb但应付热数据阶段的需求就足够了

### 冷数据阶段

数据在什么时候由热数据变成冷数据这个要看业务需求,但基本可以这样判定:

1. 数据不会再被单独取出来使用

2. 数据不会再被频繁的取出来使用,使用间隔以天计数.

有以上特征的数据基本就可以作为冷数据处理了.通常我们会使用ETL工具定期的将数据从缓存库转移到存放冷数据的媒介,比较推荐的ETL工具有airflow.和[easy scheduler](https://github.com/apache/incubator-dolphinscheduler)尤其是后者,虽然年轻,但确实方便管理,目前的版本还有些小bug但已经足够支持业务使用了.
而冷数据通常比较大,其存放一般也是使用的相对廉价的介质,比如aws上的s3对象存储,或者是hdfs集群.其存储形式一般也以文件为主,通常为了压缩率和查询效率会使用列存储格式,比如`Parquet`,每次存储一般都是以时间段为划分,比如按小时,按天来划分文件,具体如何划分需要考虑业务和单文件的容量

### 删除阶段

删除阶段通常是为了用更加低廉的方式保存过期数据.比如我们保留1年的数据作为冷数据那比较可能的操作是--在每年的1月一日将去年一整年的数据压缩打包后下载到本地的廉价大容量硬盘或者磁带同时每个月初固定删除上一年上个月的数据.

通常存储的数据格式和冷数据阶段一样,但其中数据的间隔时间更长,比如是按月记录,然后再进行压缩.



## 业务数据生命周期

业务数据往往分为两种:

1. 静态数据

这种数据往往是业务的起点,比如音乐软件库中的音乐就是静态数据.这种数据的特点是更新不频繁,一直需要保持可访问状态,同时一有上架下架消息就会影响到许多的业务,因此比较合适的处理方法是在创建阶段创建的是上架/下架/修改的事件数据,通过在中间件中广播上架下架修改事件让依赖对应数据的业务进行处理.

这种数据最好更多的关注其并发性能,因为他们往往再业务上需要被频繁访问术语OLTP业务,这种比较好的方式有两个:

    1. 数据库读写分离,一写多读,分摊查询压力
    2. 使用redis缓存,查询访问全部走redis缓存
    
如果要针对静态数据做OLAP业务,那么比较好的方法是将对饮表复制到OLAP工具.这种数据一般也不用区分冷热,也都不会删除.

2. 事件数据

这种数据就是比较典型的了,它一般会走完从创建到删除的整个流程.通常它们不会被直接取出来用,而是分批的做统计数据.




## 统计数据生命周期

统计数据通常是由事件数据得来,很多公司为了省钱只会保留最近的一次切面数据,但实际上统计数据的变化情况对分析业务状况非常有价值,所以也应该像事件数据一样走完整个生命周期.


