{"./":{"url":"./","title":"Introduction","keywords":"","body":"数据科学攻略 数据科学主要以统计学,机器学习,数据可视化以及领域知识为理论基础,其主要研究内容包括数据科学基础理论,数据预处理,数据计算和数据管理等.是一门很杂的交叉学科.需要同时具备计算机,数学,统计作为前置知识,因此还是算比较高阶的学问. 数据科学总的来说和机器学习一样,是应用统计学的一个细分学科,这两个学科要时常有交叉应用.总的来说数据科学更加偏向工程,机器学习则更加偏向研究. 数据科学研究的对象和要解决的问题 数据科学研究的对象是数据,数据则是人工从事物中提取出来存在信息空间中的对象.万事万物只要不是静止不变的就必然会留下信息,而这个宇宙唯一不变的只有变化,因此数据科学的研究对象可以是任何事物;但这么说又不准确,更准确的说法是研究的是任何事物留下信息的数据. 那么数据科学要解决的问题就包括了: 如何将事物信息提取为数据 如何将数据保存在信息空间 如何利用保存在信息空间的数据解决实际问题. 目标读者 这篇文章主要的目标读者是有一定计算机和统计基础,希望对数据科学有所了解的工程和研究人员.本文也相对更加偏向工程,理论的部分更多的是在术语表中以词条的形式做介绍. 非目标读者 新手慎入,这个坑很大,如果纯新人建议先看我的python攻略,有一定python编程基础后再入坑. Java程序员慎入,本人不喜欢Java,因此相关技术栈将鲜有涉及,主要介绍的技术是围绕python展开的,因此上面才会建议新人看python攻略.希望通过本文学习hadoop/spark技术栈的人恐怕要失望了. 本教程的内容范围和组织结构 文章的目标是做成数据科学领域最佳实践的形式,由于本人不喜欢java,因此java技术栈的内容将鲜有涉及,由于数据科学是一门相当杂的学科,因此文章内容也会看起来比较杂乱.我会尽量让各个部分的内容相对独立,如果一定要有联系也会使用超链接做关联,这样以提高可读性. 文章将会划分为篇,每一篇下面的单位是章,章下面是节,篇下面可以没有章,这种情况就顺延.篇,章下面都会开头的介绍文字和结尾的结语.介绍用于总体上概括其所属的内容而结语用于介绍一些豆知识,相关的好用第三方库和个人的相关看法,希望读者喜欢. 每一节则是单独的一篇文章.我尽量让各个部分内聚避免耦合,这样可以不用按顺序,但有些确实需要有其他方面基础的那就没办法了,我会在每节开始的部分给出预备知识的超链接方便查看. 每一节中都会有1~3级标题,不会再往下分出4级标题.同时1级标题只会是一节的标题. 总结下就是如下的树状结构 篇-| |-章-| |-节-| |- 1级标题-| |-2级标题-| |-3级标题 或者 篇--| |-节-| |- 1级标题-| |-2级标题-| |-3级标题 在文末会有一个术语表附录,将文中出现的术语做一个总结,如果有的话也会给出wiki链接 下面是每篇的简单介绍 工具链篇 介绍数据科学领域常见的各种工具.比如python的数据科学工具箱,数据库,消息中间件,搜索引擎等. 数据采集篇 介绍从什么地方以什么形式采集数据 数据清洗篇 介绍如何过滤数据中的噪音,调整由于采集操作造成的误差,使其可以更好更准确的描绘对象的信息 数据管理篇 介绍如何将采集来,清洗好的数据保存在信息空间以便于使用和复现模拟. 数据分析篇 介绍如何分析各种数据,找到其特点用于辅助决策 数据挖掘篇 介绍如何使用探索数据以发现其更多的价值 数据预测篇 介绍如何利用机器学习等算法技术构造模型,用于预测数据的一些位置特点 贯穿全文的排版约定 难度分级 这边给出的分级方案是相对分级,具体说就是在篇/章/节/中1~3级标题中命名以*为开头,为标题的就是相对其他同级的来说进阶一些的内容,这些内容可以选择着看;而使用**开头的的则是相对最基础最重点的内容.需要加深记忆 篇/章/的标题在每个篇/章的介绍页, ps 一些比较关键的点会使用加粗斜体的PS:字样标识出来 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:53:59 "},"工具链篇/存储工具/使用postgresql做数据存储/":{"url":"工具链篇/存储工具/使用postgresql做数据存储/","title":"使用postgresql做数据存储","keywords":"","body":"使用postgresql做数据存储 postgresql(简称pg)是目前最先进的开源关系型通用数据库,它完全支持标准sql语句,在其上还有一定的功能扩展;天生支持插件,有许多额外的实用功能由一些第三方组织已经做了很不错的实现;并且支持自定义数据包装器,将pg仅作为interface,数据则实际存在被包装的数据库上. 在性能上,pg从来不在任何一个场景下是最好的选择.但永远是第二好的选择,加上它是个多面手,可以一套工具应付绝大多数场景,不用考虑多种工机的异构系统集成问题,因此技术选型选它还是比较合适的. 应用领域 pg生态下大致的应用领域有: OLTP: 事务处理是PostgreSQL的本行 OLAP: 并行计算,数据分区,ANSI SQL兼容，窗口函数，CTE，CUBE等高级分析功能，任意语言写UDF 时序数据:timescaledb时序数据库插件,分区表,BRIN索引 流处理: PipelineDB扩展，Notify-Listen，物化视图，规则系统，灵活的存储过程与函数编写 搜索引擎: 全文搜索索引足以应对简单场景;配合插件zhparser等分词工具可以实现中文支持,丰富的索引类型,支持函数索引,条件索引 文档数据库: JSON,JSONB,XML,HStore原生支持,可以替代mongodb 图数据库: 递归查询,更有AgensGraph扩展实现完整的图数据库功能 空间数据: PostGIS扩展(杀手锏),内建的几何类型支持,GiST索引. 缓存: 物化视图 作为交互界面包装外部数据: 可以自定FDW包装外部数据,也就是说pg可以什么也不存,只是作为对其他外部数据对象的sql交互界面 本文会按顺序逐次介绍 本文使用的工具 本文使用docker作为pg的部署工具,我会在docker中模拟pg和插件的运行环境,部署用的docker-compose文件会放在code文件夹下 本文使用的pg版本为11.5 SQL语句,基本不用不合规范的SQL语句 执行SQL语句这边使用jupyter notebook的postgres_kernel Helloworld 按照惯例,我们来先写个helloworld,在安装好pg后,默认会有一个数据库postgres,我们可以进入它来实现这个helloworld -- connection: postgres://postgres:postgres@localhost:5432/postgres -- autocommit: true switched autocommit mode to True select 'hello world' as welcome 1 row(s) returned. welcome hello world postgresql的基本概念和使用 pg是一个关系型数据库,其数据基本以行为单位,下面是pg使用到的基本概念汇总: database database 一词含义宽泛,既可表示广义的数据库系统,又可以表示某些特定数据库系统中的某一级数据存储单位,如表述不当极易造成混淆.此处的database为最粗颗粒度的一级数据存储单位,通常以业务范围区分,每个pg服务实例可以包含多个database. 创建一个database CREATE DATABASE test database \"test\" already exists 删除database 删除database可以使用可选命令IF EXISTS,这样如果数据库并不存在就不会执行删除操作,避免错误 DROP DATABASE IF EXISTS test database \"test\" is being accessed by other users DETAIL: There are 4 other sessions using the database. schema 数据库业界对于schema有多种译法:纲要,模式,方案等等.但各种译法都不能准确直观地表达出其原本的含义--即位于一个独立命名空间内的一组相关数据库对象的集合. ANSI SQL标准中对 schema 有着明确的定义--database的下一层逻辑结构就是 schema. 如果把database比作一个国家,那么 schema 就是一些独立的省,大多数对象是隶属于某个schema的,然后schema又隶属于某个database.在创建一个新的 database时,PostgreSQL会自动为其创建一个名为public的schema(类似首都).如果未设置search_path变量,那么 PostgreSQL 会将你创建的所有对象默认放入public schema中.如果表的数量较少这是没问题的,但如果你有几千张表,那么还是建议将它们分门别类放入不同的 schema 中. 创建一个schema CREATE SCHEMA test_schema 删除一个schema 与删除database类似的我们也可以删除schema DROP SCHEMA IF EXISTS test_schema 表 任何一个数据库中表都是最核心的对象类型,在 PostgreSQL 中表首先属于某个 schema,而 schema 又属于某个 database,这样就构成了一种三级存储结构(可以类比国家-省-市) PostgreSQL的表支持两种很强大的功能: 表继承,即一张表可以有父表和子表.这种层次化的结构可以极大地简化数据库设计,还可以为你省掉大量的重复查询代码. 创建一张表的同时系统会自动为此表创建一种对应的自定义数据类型 创建表 与许多关系数据库不同,pg可以没有主键,主键只是一种约束而已,因此在定义表时并不需要指定主键.建表操作非常容易重复表明,为了避免重复我们可以使用命令IF NOT EXISTS避免. 创建表的格式为: CREATE TABLE IF NOT EXISTS {表名}( {字段名} {字段类型} [DEFAULT {默认值} [...{字段约束}]], ..., [...{表约束}] ) 通常我个人不建议将约束和建表混在一起写,而更建议分为创建和定义约束两步,这样更加清晰. CREATE TABLE IF NOT EXISTS my_test( a text, b text ) NOTICE: relation \"my_test\" already exists, skipping 修改表名 ALTER TABLE my_test RENAME TO mytest1 relation \"mytest1\" already exists 字段(列) 在创建一个表的时候我们就必须先定义好这个表有什么字段,这些字段分别是什么类型,有什么约束条件制约,有什么默认值,怎么加索引等. 比较常见的字段操作是修改字段,修改字段: 新增字段 ALTER TABLE mytest1 ADD c float 修改字段的数据类型 ALTER TABLE mytest1 ALTER COLUMN c TYPE text 修改字段名 ALTER TABLE mytest1 RENAME COLUMN c TO d 删除字段 ALTER TABLE mytest1 DROP COLUMN d 约束 表中除了字段还有字段间的关系--约束.pg中的约束包括这么几种: 主键约束,非空约束和唯一约束的结合,确保某列(或两个列多个列的结合)有唯一标识,有助于更容易更快速地找到表中的一个特定的记录.使用ALTER TABLE {表名} ADD CONSTRAINT {约束名} PRIMARY KEY ({...字段名})创建 唯一约束,确保某列的值都是唯一的,使用ALTER TABLE {表名} ADD CONSTRAINT {约束名} unique({字段名})创建 非空约束,指示某列不能存储 NULL 值,使用ALTER TABLE {表名} MODIFY {字段名} {字段类型} NOT NULL创建 外键约束,保证一个表中的数据匹配另一个表中的值的参照完整性 CHECK约束,保证列中的值符合指定的条件,使用ALTER TABLE {表名} ADD CONSTRAINT {约束名} CHECK ({条件}) 排他约束,保证如果将任何两行的指定列或表达式使用指定操作符进行比较,至少其中一个操作符比较将会返回 false 或空值.使用条件ALTER TABLE {表名} ADD CONSTRAINT {约束名} EXCLUSION ({条件}) 删除约束只能删除有名字的越是,使用语句ALTER TABLE {表名} DROP CONSTRAINT {约束名}. 默认值 关系数据库也允许为字段设置默认值,这样如果插入的行中对应字段没有值,插入就会用默认值替换空,默认值的处理类似修改字段名,也使用alter语句. ALTER TABLE {表名} ALTER COLUMN {字段名} SET DEFAULT {默认值}; 序列号生成器 序列号生成器用于实现serial数据类型值的自动递增分配.在创建 serial 字段时PostgreSQL 会自动为其创建一个相应的序列号生成器,但用户也可以很方便地更改其初始值,步长和下一个值.因为序列号生成器是独立对象,所以多个表可以共享同一个序列号生成器.基于该机制用户可以实现跨越多个表的唯一键. 索引 索引是用来加快表查找速度的对象,它必须依赖表和字段,使用CREATE INDEX {index_name} ON {table_name} [USING {index_type}] ({字段名}[,{字段名}])语法创建. pg提供了多种索引类型: B-tree可以在可排序数据上的处理等值和范围查询.特别地pg的查询规划器会在任何一种涉及到以下操作符的已索引列上考虑使用B-tree索引: = >= > 将上述操作符组合起来的操作,例如BETWEEN和IN IS NULL或IS NOT NULL判断操作 like和~等在模式是一个常量且被固定在字符串的开头时 B-tree索引也可以用于检索排序数据.这并不会总是比简单扫描和排序更快但是总是有用的 Hash只能处理简单等值比较 GiST二维几何图形信息的基础索引.可以使用基本的二维几何图形操作符,如: & &> >> & |&> |>> @> ~= && 同时也有能力优化最近邻搜索 SP-GiST,和GiST相似,SP-GiST索引为支持多种搜索提供了一种基础结构.SP-GiST允许实现众多不同的非平衡的基于磁盘的数据结构例如四叉树,k-d树和radix树,比如下面的操作符: >> ~= >^ GIN,倒排索引,它适合于包含多个组成值的数据值,例如数组.倒排索引中为每一个组成值都包含一个单独的项,它可以高效地处理测试指定组成值是否存在的查询.GIN可以用于支持数组使用下列操作符的索引化查询： @> = && BRIN块范围索引,存储有关存放在一个表的连续物理块范围上的值摘要信息.对于具有线性排序顺序的数据类型,被索引的数据对应于每个块范围的列中值的最小值和最大值,BRIN支持的操作符可以在[http://www.postgres.cn/docs/11/brin-builtin-opclasses.html#BRIN-BUILTIN-OPCLASSES-TABLE]查看 每一种索引类型使用了 一种不同的算法来适应不同类型的查询.默认情况下CREATE INDEX命令创建适合于大部分情况的B-tree索引. B-tree,GiST,GIN和BRIN支持多列索引,最多可以指定32个列.一个B-tree索引可以用于条件中涉及到任意索引列子集的查询,但是当先导列(即最左边的那些列)上有约束条件时索引最为有效;一个GIN索引可以用于条件中涉及到任意索引列子集的查询.GIN的搜索效率与查询条件中使用哪些索引列无关;多列BRIN索引可以被用于涉及该索引被索引列的任意子集的查询条件.同样索引搜索效率与查询条件使用哪个索引列无关. CREATE INDEX test1_id_index ON mytest1 (a); 全文检索 全文检索(full text search,FTS)是一种基于自然语言的搜索机制.这种搜索机制有一些\"智能\"成分.与正则表达式搜索不同,全文检索能够基于语义来进行匹配查找,而不仅仅是纯粹的语法匹配.例如用户需要在一段长文本中搜索running这个词,那么命中的结果可能包含run,running,jog,sprint,dash等词. 全文检索功能依赖于FTS配置库,FTS 词典,FTS解析器这三个部件.有了它们PostgreSQL 原生的FTS功能即可正常使用. 一般场景下的全文检索靠这三个原生部件已经足够,但在涉及专业场景时搜索目标文本中会包括该领域专有词汇和特殊语法规则,此时需要用专门的FTS 部件来替换原生FTS部件. 删除索引 可以使用DROP INDEX {index_name}语句 DROP INDEX test1_id_index 视图 大多数关系型数据库都支持视图,视图是基于表的一种抽象,通过它可以实现一次性查询多张表,也可以实现通过复杂运算来构造出虚拟字段.视图一般是只读的,但如果该视图基于单张实体表构建,pg支持对其进行修改.如果需要修改基于多张表关联而来的视图则可以针对视图编写触发器 可以理解为视图可以将查询和实际存储解耦. 创建视图可以使用: CREATE [ OR REPLACE ] [ TEMP | TEMPORARY ] [ RECURSIVE ] VIEW {view_name} AS SELECT column1, column2..... FROM table_name WHERE [condition]; 指明TEMP/TEMPORARY则创建的时临时视图,指明RECURSIVE则创建的是递归视图 删除视图 DROP VIEW {view name} 物化视图 9.3 版还引入了对物化视图的支持,该机制通过对视图数据进行缓存来实现对常用查询的加速,缺点是查到的数据可能不是最新的. 其创建语法为: materialized CREATE [ OR REPLACE ] MATERIALIZED VIEW {view_name} AS SELECT column1, column2..... FROM table_name WHERE [condition]; 物化视图不会自动更新,需要手动刷新,需要更新时使用命令REFRESH MATERIALIZED VIEW {view_name}刷新 删除表 DROP TABLE IF EXISTS mytest1 DROP TABLE IF EXISTS my_test 行 表中的每一条数据被称为行,行是记录完整数据的最小单位.行可以执行插入,更新,删除操作,也可以执行查询操作.我们在test数据库中创建一个test表用于演示 -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True CREATE TABLE IF NOT EXISTS test( a int4 PRIMARY KEY, b text, c numeric(12) ) 插入行 INSERT INTO test (a,b,c) VALUES (1,'2',123.541) INSERT INTO test (a,b,c) VALUES (2,'3',13.41) INSERT INTO test (a,b,c) VALUES (3,'4',23.54) 修改行 UPDATE test SET b = '22' WHERE a = 1 删除行 DELETE FROM test WHERE a = 3 查询行 SELECT * FROM test 2 row(s) returned. a b c 2 3 13 1 22124 事务 事务是一次有限个数sql语句执行的全过程,事务具有以下四个标准属性,通常根据首字母缩写为ACID： 原子性(Atomicity):事务作为一个整体被执行,包含在其中的对数据库的操作要么全部被执行,要么都不执行. 一致性(Consistency):事务应确保数据库的状态从一个一致状态转变为另一个一致状态.一致状态的含义是数据库中的数据应满足完整性约束. 隔离性(Isolation):多个事务并发执行时,一个事务的执行不应影响其他事务的执行. 持久性(Durability):已被提交的事务对数据库的修改应该永久保存在数据库中 PG中所有sql操作都是事务.我们可以通过关键字BEGIN/BEGIN TRANSACTION-COMMIT-ROLLBACK构造事务 BEGIN; INSERT INTO test (a,b,c) VALUES (4,'22',41); INSERT INTO test (a,b,c) VALUES (5,'32',123.1); INSERT INTO test (a,b,c) VALUES (6,'24',1.541); INSERT INTO test (a,b,c) VALUES (7,'12',141); COMMIT; SELECT * FROM test 6 row(s) returned. a b c 2 3 13 1 22124 4 22 41 5 32123 6 24 2 7 12141 接下来的部分类似元编程一样,是用户可以对pg做自定义的部分 扩展 开发人员可以通过该机制将一组相关的函数,数据类型,数据类型转换器,用户自定义索引,表以及属性变量等对象打包成一个功能扩展包,该扩展包可以整体安装和删除.许多工具如piplinedb,timescaledb都是以扩展包的形式存在的.从 PostgreSQL 9.1版本之后一般推荐使用该机制来为数据库提供功能扩展. 安装扩展包时可以指定该包中所含有的成员对象安装到哪个schema,若不指定则默认会安装到 public schema 中。不建议采用默认设置,因为这会导致 public schema变得庞大复杂且难以管理,尤其是如果你将自己的数据表对象也都存入 public schema 中,那么情况会变得更糟糕.我们建议你创建一个独立的 schema 用于存放所有扩展包的对象,甚至为规模较大的扩展包单独创建一个 schema.为避免出现找不到新增扩展包对象的问题,请将这些新增的 schema 名称加入search_path变量中,这样就可以直接使用扩展包的功能而无须关注它到底安装到了哪个schema中. 也有一些扩展包明确要求必须安装到某个 schema 下,这种情况下你就不能自行指定了. 多个扩展包之间可能存在依赖关系.在PostgreSQL 9.6之前,我们需要了解这个依赖关系并把被依赖包先装好,但从9.6版开始,只需在安装时加上 cascade关键字PostgreSQL 就会自动安装当前扩展包所依赖的扩展包. 函数 pg内置了大量函数用于字符串处理,时间处理,或者统计数据,都保存在默认的 postgres 库中,同时用户可以编写自定义函数来对数据进行新增,修改,删除和复杂计算等操作.可以使用 PostgreSQL所支持的各种过程式语言来编码.函数支持返回以下数据类型: 标量值(也就是单个值) 数组 单条记录以及记录集 其他数据库(比如mysql)将对数据进行增删改操作的函数称为\"存储过程\",把不进行增删改的函数叫作\"函数\",但 PostgreSQL 中并不区分. 内置编程语言 函数是以过程式语言编写的.PostgreSQL 默认支持三种内置编程语言:SQL,PL/pgSQL 以及 C 语言.可以通过CREATE EXTENSION或者CREATE PRODCEDURAL LANGUAGE命令来添加其他语言.目前较常用的语言是 PL/Python,PL/V8(即 JavaScript)以及 PL/R. 运算符 运算符本质上是以简单符号形式呈现的函数别名,例如 =,&& 等.PostgreSQL 支持自定义运算符.如果用户定义了自己的数据类型，那么一般来说需要再自定义一些运算符来与之配合工作.比如你定义了一个复数类型,那么你很有可能需要自定义+、-、*、/这几个运算符来对复数进行运算. 外部表和外部数据封装器 外部表是一些虚拟表,通过它们可以直接在本地数据库中访问来自外部数据源的数据.只要数据映射关系配置正确外部表的用法就与普通表没有任何区别.外部表支持映射到以下类型的数据源： CSV 文件 另一个服务器上的 PostgreSQL 表 SQL Server 或 Oracle 这些异构数据库中的表 Redis 这样的 NoSQL 数据库 甚至像 Twitter 或 Salesforce 这样的 Web 服务。 外部表映射关系的建立是通过配置外部数据封装器(foreign data wrapper，FDW)实现的.FDW是 PostgreSQL和外部数据源之间的一架\"魔法桥\",可实现两边数据的互联互通.其内部实现机制遵循 SQL 标准中的 MED(Management of External Data)规范。 许多无私的开发者已经为当下大部分流行的数据源开发了FDW 并已免费共享出来.你也可以通过创建自己的FDW来练习.FDW是通过扩展包机制实现的 触发器和触发器函数 绝大多数企业级数据库都支持触发器机制,该机制可以侦测到数据修改事件的发生.在 PostgreSQL 中当一个触发器被触发后系统会自动调用用户定义好的触发器函数.触发器的触发时机是可设置的,可以是语句级触发或者记录级触发也可以是修改前触发或修改后触发. 定义触发器时需要定义对应的触发器函数,这类函数与前面介绍过的普通函数有所不同,主要差异在于触发器函数可以通过系统内置变量来同时访问到修改前和修改后的数据.这样就可以实现对于非法的数据修改行为的识别和拦截.因此触发器函数一般会用于编写复杂校验逻辑,这类复杂逻辑通过check 约束是无法实现的. PostgreSQL的触发器技术正在快速的演进之中.9.0 版引入了对WITH子句的支持,通过它可以实现带条件的记录级触发,即只有当某条记录符合指定的 WHEN条件时触发器才会被调用.9.0 版还引入了UPDATE OF子句.通过它可以实现精确到字段级的触发条件设置.仅当指定的字段内容被更改时才会激活触发器.9.1 版支持了针对视图的触发器.9.3 版支持了针对DDL的触发器.最后值得一提的是从 9.4 版开始针对外部表的触发器也获得了支持. catalog catalog 的译法与 schema 存在相同的问题,翻译为\"目录\"后并不能让读者准确地理解其原意,反而容易造成混淆.catalog是系统级的schema,用于存储系统函数和系统元数据.每个database创建好以后默认都会含有两个catalog: 一个名为 pg_catalog,用于存储 PostgreSQL 系统自带的函数,表,系统视图,数据类型转换器以及数据类型定义等元数据 另一个是 information_schema,用于存储 ANSI 标准中所要求提供的元数据查询视图,这些视图遵从 ANSI SQL 标准的要求,以指定的格式向外界提供 PostgreSQL 元数据信息. 一直以来,PostgreSQL数据库的发展都严格地遵循着其\"自由与开放\"的核心理念.如果你足够了解这款数据库,会发现它几乎是一种可以\"自我生长\"的数据库.比如,它所有的核心设置都保存在系统表中,用户可以不受限地查看和修改这些数据,这为PostgreSQL提供了远超任何一种商业数据库的巨大灵活性(不过从另一个角度看,将这种灵活性称为\"可破坏性\"也未尝不可).只要仔细地研究一下pg_catalog你就可以了解到 PostgreSQL 这样一个庞大的系统是如何基于各种部件构建起来的.如果你有超级用户权限,那么可以直接修改pg_catalog的内容(当然,如果改得不对,那你的行为就跟搞破坏没什么两样了). Information_schema catalog 在 MySQL 和 SQL Server 中也有.PostgreSQL 的 Information_schema 中最常用的视图一般有以下几个:columns 视图,列出了数据库中的所有字段;tables视图,列出了数据库中的所有表(包括视图);views 视图,列出了所有视图以及用于创建该视图的原始 SQL. 类型 类型是数据类型的简称.每种数据库产品和每种编程语言都会支持一系列的数据类型,比如整型,字符型,数组,二进制大对象(blob)等.除前述常见类型外,PostgreSQL还支持复合数据类型,这种类型可以是多种数据类型的一个组合,比如复数,极坐标,向量,张量等都是复合数据类型. PostgreSQL 会自动为用户自己创建的表定义一个同名的复合数据类型.这样就可以把表记录当作对象实例来处理.当用户需要在函数中遍历表记录时,该特性特别有用. 数据类型转换器 数据类型转换器可以将一种数据类型转换为另一种,其底层通过调用转换函数来实现真正的转换逻辑.PostgreSQL支持用户自定义转换器或者重载,加强默认的转换器. 转换器可以被隐式调用也可以被显式调用.隐式转换是系统自动执行的,一般来说将一种特定数据类型转为更通用的数据类型(比如数字转换为字符串)时就会发生隐式类型转换.如果进行隐式转换时系统找不到合适的转换器你就必须显式执行转换动作. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:08:09 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/","title":"功能与特性","keywords":"","body":"功能与特性 Postgresql作为最先进的开源关系型数据库,除了支持标准sql语言外,还有许多额外的能力.除此之外由于pg易于扩展的特性,pg的插件和自定义功能也非常强大,可以自定义许多功能自己做扩展,因此pg是一种上限非常高的数据库技术.本文就将从pg的特色功能和自定义扩展两个方面进行深入介绍. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:27:08 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/","title":"特色功能","keywords":"","body":"特色功能 pg自带强大的特色功能,主要包括 复杂查询,包括递归查询,窗口函数 消息中间件,包括事件发布监听功能等 特殊类型,包括json,二维图形等 高性能专用索引,包括GIST索引,全文索引等 业务数据模型优化,包括物化视图,表继承等 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:41:54 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/复杂数据类型及类型转换.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/复杂数据类型及类型转换.html","title":"复杂数据类型及类型转换","keywords":"","body":"数据类型 PG有丰富的数据类型,可以满足几乎所有的需求,同时还支持自定义数据类型.所有数据类型都一样没有特殊. -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True 常规数据类型 这边的常规数据类型指标准sql语法下的数据类型,主要包含: 数 字符串 布尔值 时间 数 通常数值类型分为两种--整型和浮点型,主要的指标是存储尺寸和取值范围. 数据类型 说明 存储尺寸 范围 smallint(int2) 小范围整型数 2字节 [-32768, +32767] integer(int4) 整数的典型选择 4字节 [-2147483648,+2147483647] bigint(int8) 大范围整数 8字节 [-9223372036854775808,+9223372036854775807] real(float4) 可变精度，不精确 4字节 6位十进制精度 double precision(float8) 可变精度，不精确 8字节 15位十进制精度 decimal(decimal(n)) 用户指定精度,精确,需要声明存储尺寸 可变 最高小数点前131072位,以及小数点后16383位 numeric(numeric(n)) 用户指定精度,精确,需要申明存储尺寸 可变 最高小数点前131072位,以及小数点后16383位 字符串 字符串型在pg中有三类 数据类型 说明 character varying(n) (varchar(n)) 有限制的变长 character(n)(char(n)) 定长,长度不足空格填充补位 text(text) 无限变长 字符串操作 pg同样内置了大量字符串操作,下面是常见的10个字符串操作 string || string拼接 select 'this is'|| ' my name' as result 1 row(s) returned. result this is my name select 'this is'|| 18 as result 1 row(s) returned. result this is18 bit_length(string)/char_length(string)/character_length(string)获取字符串长度.bit_length(string)获取字节长度,另外俩获取字符长度 select bit_length('神马都是浮云呐!') as result 1 row(s) returned. result 176 select char_length('神马都是浮云呐!') as result 1 row(s) returned. result 8 lower(string)/upper(string)将英文转为大写/小写 select upper('asDF') as result 1 row(s) returned. result ASDF substring(string [from int] [for int])求字符串中的子串 select substring('神马都是浮云呐!',3,2) as result 1 row(s) returned. result 都是 reverse(str)反转字符串 select reverse('神马都是浮云呐!') as result 1 row(s) returned. result !呐云浮是都马神 regexp_match(string text, pattern text [, flags text])/regexp_matches(string text, pattern text [, flags text])使用re查找匹配的字符串 select regexp_matches('foobarbequebaz', 'ba.', 'g') as result 2 row(s) returned. result ['bar'] ['baz'] regexp_replace(string text, pattern text, replacement text [, flags text])使用re做替换 select regexp_replace('Thomas', '.[mN]a.', 'M') as result 1 row(s) returned. result ThM regexp_split_to_table(string text, pattern text [, flags text])/regexp_split_to_array(string text, pattern text [, flags text ])使用re做字符串拆分 select regexp_split_to_array('hello world', E'\\\\s+') as result 1 row(s) returned. result ['hello', 'world'] select regexp_split_to_table('hello world', E'\\\\s+') as result 2 row(s) returned. result hello world md5(string) 求字符串的md5hash select md5('神马都是浮云呐!') as result 1 row(s) returned. result 84153abdfd8776b3263694915149b345 to_hex(number int or bigint) 将int型的数字转成16进制字符串 select to_hex(123423543215) as result 1 row(s) returned. result 1cbc9dcfaf 布尔值 pg提供标准的SQL类型boolean,可以有多个状态 true（真)合法的输入有TRUE/'t'/'true'/'y'/'yes'/'on'/'1' false(假)合法的输入有FALSE/'f'/'false'/'n'/'no'/'off'/'0' 第三种状态unknown(未知),未知状态由SQL空值表示. 时间 pg支持各种时间表示,主要的相关类型包括 名字 别名 存储尺寸 说明 最小值 最大值 解析度 timestamp [ (p) ] [ without time zone ] timestamp 8字节 包括日期和时间,无时区 4713 BC 294276 AD 1微秒/14位 timestamp [ (p) ] with time zone timestampz 8字节 包括日期和时间,有时区 4713 BC 294276 AD 1微秒/14位 date date 4字节 日期(没有一天中的时间) 4713 BC 5874897 AD 1日 time [ (p) ] [ without time zone ] time 8字节 一天中的时间(无日期) 00:00:00 24:00:00 1微秒/14位 time [ (p) ] with time zone timez 12字节 一天中的时间(不带日期),带有时区 00:00:00+1459 24:00:00-1459 1微秒/14位 interval [ fields ] [ (p) ] interval 16字节 时间间隔 -178000000年 178000000年 1微秒/14位 time/timestamp和interval接受一个可选的精度值p,这个精度值声明在秒域中小数点之后保留的位数.缺省情况下在精度上没有明确的边界,p允许的范围是从 0 到 6. timestamp和time在输入时区后会自动忽略,timestampz和timez在缺省时区时认为时utc时区,因此比较靠谱的方法是使用timestampz和timez并全局使用utc时间. interval类型有一个附加选项，它可以通过写下面之一的短语来限制存储的fields的集合： YEAR MONTH DAY HOUR MINUTE SECOND YEAR TO MONTH DAY TO HOUR DAY TO MINUTE DAY TO SECOND HOUR TO MINUTE HOUR TO SECOND MINUTE TO SECOND 注意如果fields和p被指定,fields必须包括SECOND,因为精度只应用于秒. 时间的输入 pg中时间的输入和字符串一样使用单引号'.下面是时间,日期,时区的可选形式 日期 例子 描述 1999-01-08 ISO 8601; 任何模式下的1月8日,推荐格式 January 8, 1999 在任何datestyle输入模式下都无歧义 1/8/1999 MDY模式中的1月8日；DMY模式中的8月1日 1/18/1999 MDY模式中的1月18日；在其他模式中被拒绝 01/02/03 MDY模式中的2003年1月2日； DMY模式中的2003年2月1日； YMD模式中的2001年2月3日 1999-Jan-08 任何模式下的1月8日 Jan-08-1999 任何模式下的1月8日 08-Jan-1999 任何模式下的1月8日 99-Jan-08 YMD模式中的1月8日，否则错误 08-Jan-99 1月8日，除了在YMD模式中错误 Jan-08-99 1月8日，除了在YMD模式中错误 19990108 ISO 8601; 任何模式中的1999年1月8日 990108 ISO 8601; 任何模式中的1999年1月8日 1999.008 年和一年中的日子 J2451187 儒略日期 January 8, 99 BC 公元前99年 时间 例子 描述 04:05:06.789 ISO 8601 04:05:06 ISO 8601 04:05 ISO 8601 040506 ISO 8601 04:05 AM 和04:05一样，AM并不影响值 04:05 PM 和16:05一样，输入的小时必须为 04:05:06.789-8 ISO 8601 04:05:06-08:00 ISO 8601 04:05-08:00 ISO 8601 040506-08 ISO 8601 04:05:06 PST 缩写指定的时区 2003-04-12 04:05:06 America/New_York 全名指定的时区 时区 例子 描述 PST 缩写（太平洋标准时间） America/New_York 完整时区名 PST8PDT POSIX风格的时区声明 -8:00 PST的ISO-8601偏移 -800 PST的ISO-8601偏移 -8 PST的ISO-8601偏移 zulu UTC的军方缩写 z zulu的短形式 复杂数据类型 pg提供了许多额外的数据类型支持,可以应付多样的需求,比较常见的复杂数据类型包括 array json/jsonb 二进制数据类型 序列 bitmap money uuid 网络地址 文本搜索类型 几何图形 同时pg还支持自定义类型 array PG支持定义变长/定长多维数组,这一特性让他成了保存矩阵数据的优秀工具. array字段的定义 array定义类似C语言中的array,使用type[]表示不定长的数组,用type[n]表示定长数组,同时array可以是多维数组. 另一种表示一维数组的方式是使用关键词ARRAY. 下面的例子可以用来展示如何定义包含array类型字段的表 CREATE TABLE IF NOT EXISTS array_test( a int4 PRIMARY KEY, b int4[][], c text ARRAY ) NOTICE: relation \"array_test\" already exists, skipping array字段的输入表示 array默认输入表示类似C语言,使用花括号{}包裹元素,但整体需要使用单引号'包裹 INSERT INTO array_test (a,b,c) VALUES ( 1, '{{1,2,3},{4,5,6},{7,8,9}}', '{\"a\",\"b\",\"d\"}' ) select * from array_test 1 row(s) returned. ab c 1[[1, 2, 3], [4, 5, 6], [7, 8, 9]]['a', 'b', 'd'] array的操作 array基本操作就是 查询 替换 搜索 查询 array的按位查询和大多数编程语言中中几乎一样,就是用[]选择要查看的下标即可,需要注意的是array的第一个元素为1而不是大多数编程语言的0.同时也可以使用类似python中的切片操作来获取一个新的子array,同样不同于python中,:后的下标也会被访问到 select b[1][1] as result from array_test 1 row(s) returned. result 1 select c[1:2] as result from array_test 1 row(s) returned. result ['a', 'b'] select c[1:3] as result from array_test 1 row(s) returned. result ['a', 'b', 'd'] 替换 数组的替换可以是整体替换,也可以是按位替换,都得使用update语句 UPDATE array_test SET b = '{{2,4,6,8,10},{1,3,5,7,9}}'where a = 1 select * from array_test 1 row(s) returned. ab c 1[[2, 4, 6, 8, 10], [1, 3, 5, 7, 9]]['a', 'b', 'd'] UPDATE array_test SET b[2][2] = 100 where a = 1 select * from array_test 1 row(s) returned. ab c 1[[2, 4, 6, 8, 10], [1, 100, 5, 7, 9]]['a', 'b', 'd'] 搜索 如果只是简单的判断元素是否在数组内,可以使用ANY和ALL;如果是判断特定位置是否为特定值,则直接用=号即可 select * from array_test where b[1][1] = 2 1 row(s) returned. ab c 1[[2, 4, 6, 8, 10], [1, 100, 5, 7, 9]]['a', 'b', 'd'] select * from array_test where 11 = ANY(b) 0 row(s) returned. select * from array_test where 10 = ANY(b) 1 row(s) returned. ab c 1[[2, 4, 6, 8, 10], [1, 100, 5, 7, 9]]['a', 'b', 'd'] json/jsonb Json这种数据也可以被存储为text,事实上许多公司使用mysql时就是直接将json存在text类型中通过代码解析的,但JSON数据类型的优势在于能强制要求每个被存储的值符合JSON规则,同时支持很多JSON相关的函数和操作符可以用于存储在这些数据类型中的数据. Pg支持两种Json数据类型--json和jsonb.两者几乎接受完全相同的值集合作为输入,区别在于效率. json是对输入的完整拷贝,使用时再去解析,所以它会保留输入的空格,重复键以及顺序等,因此json类型存储快，查询慢; jsonb是解析输入后保存的二进制,它在解析时会删除不必要的空格和重复的键,顺序和输入可能也不相同.使用时不用再次解析.jsonb也支 持索引,这也是一个令人瞩目的优势,因此jsonb类型存储稍慢，查询较快. 使用json数据类型最好保证数据库使用的时utf-8编码,否则可能出现编码不匹配的问题. json中类型对应pg中的类型 JSON 基本类型 相应的PostgreSQL类型 补充 string text 不允许\\u0000,如果数据库编码不是UTF8,非ASCII的Unicode转义会出错 number numeric 不允许NaN 和infinity值 boolean boolean 只接受小写true和false拼写 null (无) SQL NULL是一个不同的概念 json字段的定义 json/jsonb可以直接作为类型关键字用于声明表中字段.下面的例子可以用来展示如何定义包含json类型字段的表 CREATE TABLE IF NOT EXISTS json_test( a json, b jsonb ) json的输入 json无法直接输入,必须先输入为字符串,然后转化为json或者jsonb INSERT INTO json_test (a,b) VALUES ('[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::json, '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::jsonb) SELECT * from json_test 1 row(s) returned. a b [{'a': 'foo'}, {'b': 'bar'}, {'c': 'baz'}][{'a': 'foo'}, {'b': 'bar'}, {'c': 'baz'}] json的操作符与函数 json支持的操作符和函数很多,下面列出的时最常用的10个: -> [int|text]获取json中Array对象对应下标的数据或者Object对象的对应key的值 SELECT a->1 as result from json_test 1 row(s) returned. result {'b': 'bar'} SELECT '{\"a\": {\"a1\":\"foo\"},\"b\":{\"b1\":\"bar\"}}'::json->'b' as result 1 row(s) returned. result {'b1': 'bar'} SELECT b->1 as result from json_test 1 row(s) returned. result {'b': 'bar'} ->> [int|text]获取json中Array对象对应下标的数据的文本形式或者Object对象的对应key的值的文本形式 SELECT '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::json->>1 as result 1 row(s) returned. result {\"b\":\"bar\"} SELECT '{\"a\": {\"a1\":\"foo\"},\"b\":{\"b1\":\"bar\"}}'::json->>'b' as result 1 row(s) returned. result {\"b1\":\"bar\"} @> [jsonb]左边的JSON值是否包含顶层右边JSON路径/值项(限jsonb) SELECT '[1, 2, 3]'::jsonb @> '[1, 3]'::jsonb as result 1 row(s) returned. result True 左边的JSON路径/值是否包含在顶层右边JSON值中(限jsonb) select '{\"b\":2}'::jsonb 1 row(s) returned. result True ?text判断字符串是否作为顶层键值存在于JSON中(限jsonb) select '{\"b\":2}'::jsonb ?'b' as result 1 row(s) returned. result True #>text[]/#>>text[]获取在指定路径的 JSON 对象 SELECT '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::json#>'{1,b}' as result 1 row(s) returned. result bar SELECT '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::json#>>'{1}' as result 1 row(s) returned. result {\"b\":\"bar\"} #-text[]删除指定路径下的元素(限jsonb) SELECT '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"baz\"}]'::jsonb#-'{1}' as result 1 row(s) returned. result [{'a': 'foo'}, {'c': 'baz'}] jsonb_insert(target jsonb, path text[], new_value jsonb, [insert_after boolean])向json中插入元素(限jsonb) select jsonb_insert('{\"a\": [0,1,2]}', '{a, 1}', '\"new_value\"') as result 1 row(s) returned. result {'a': [0, 'new_value', 1, 2]} array_to_json(anyarray [, pretty_bool])将数组当作json对象返回 SELECT array_to_json('{1,2,3,4,5}'::int[]) as result 1 row(s) returned. result [1, 2, 3, 4, 5] json_to_record(json)/jsonb_to_record(jsonb)将json扩展为一行数据 select * from json_to_record('{\"a\":1,\"b\":[1,2,3],\"c\":[1,2,3],\"e\":\"bar\",\"r\": {\"a\": 123, \"b\": \"a b c\"}}') as x(a int, b text, c int[], d text, r jsonb) 1 row(s) returned. ab c d r 1[1,2,3][1, 2, 3] {'a': 123, 'b': 'a b c'} 为jsonb建立索引 GIN索引可以被用来有效地搜索在大量jsonb文档(数据)中出现的键或者键值对.提供了两种 GIN '操作符类',它们在性能和灵活性方面做出了不同的平衡. jsonb的默认 GIN 操作符类支持使用@>、 ?、?&以及?|操作符的查询（这些 操作符实现的详细语义请见表 9.44）。 使用这种操作符类创建一个索引的例子： 简单索引jsonb_ops,jsonb建立gin索引默认的配置,使用语句CREATE INDEX c_index ON json_test USING gin (c).这个索引支持索引@>和 ?和?&和?|操作符,这种索引为数据中的每一个键和值创建独立的索引项.因此空间占用更大,性能不及下面那种索引,但胜在灵活. jsonb_path_ops索引,使用语句CREATE INDEX c_index ON json_test USING gin (c jsonb_path_ops).这个索引只支持索引@>操作符. 这种索引为该数据中的每个值创建索引项,这种索引的一个不足是它不会为不包含任何值的JSON结构创建索引项. 二进制数据类型 PG中使用bytea类型存储二进制数据,它是变长二进制串,其长度为1或4字节外加真正的二进制串的长度.实际使用中虽然pg可以支持1g大小的数据,但大文件使用pg作为最终的大文件存储方式依然不是一个好主意,数据库资源宝贵,更加适合存储那些结构化数据而不是单出作为存储手段,如果只是想要存储手段,mongodb的gridfs,nfs或者hdfs或许是更好的选择 更多的时候数据库的二进制数据类型存储的是密钥,加密信息,一些数据的压缩数据,一些数据的序列化数据等. SQL标准定义了一种不同的二进制串类型叫做BLOB或者BINARY LARGE OBJECT.其输入格式和bytea不同,但是提供的函数和操作符大多一样. bytea字段的定义 bytea可以直接作为类型关键字用于声明表中字段.下面的例子可以用来展示如何定义包含bytea类型字段的表 CREATE TABLE IF NOT EXISTS bytesa_test( a int4 PRIMARY KEY, b bytea ) bytea的输入 bytea的输入通常使用16进制字符串的形式. INSERT INTO bytesa_test (a,b) VALUES (1, E'\\\\xDEADBEEF' ) select b::text from bytesa_test 1 row(s) returned. b \\xdeadbeef 序列 序列是pg中的特色类型,它类似python中的counter生成器,会一次抛出一个正整数,下次抛出上一次的数值加1,pg中没有自增关键字,序列就成了自增的替代方案,因此很多需要主键约束的表就会使用序列作为主键. pg支持的序列类型按取值范围分为: 数据类型 说明 存储尺寸 范围 smallserial(serial2) 自动增加的小整数 2字节 1到32767 serial(serial4) 自动增加的整数 4字节 1到2147483647 bigserial(serial8) 自动增长的大整数 8字节 1到9223372036854775807 序列字段的定义和使用 序列可以直接作为类型关键字用于声明表中字段.并且一般不需要给它赋值 CREATE TABLE IF NOT EXISTS serial_test( a serial4 PRIMARY KEY, b text ) INSERT INTO serial_test (b) VALUES ( '测试' ) INSERT INTO serial_test (b) VALUES ( '测试测试' ) select * from serial_test 2 row(s) returned. ab 1测试 2测试测试 bitmap bitmap又叫位串,即一串0/1组成的序列.这个数据结构通常用于做标识符或者去重,著名的布隆过滤器就可以使用位串实现. 在pg中又两种bitmap 数据类型 说明 bit(bit(n)) 数据必须准确匹配长度n. bit varying(varbit(n)) 数据是最长n的变长类型 通常我们用bit的比较多 位串的定义和使用 位串可以直接使用类型关键字(n)用于声明表中字段. CREATE TABLE IF NOT EXISTS varbit_test( a serial4 PRIMARY KEY, b varbit(10) ) INSERT INTO varbit_test (b) VALUES ( B'101' ) select * from varbit_test 1 row(s) returned. a b 1101 常见的操作可以查看官方文档.比较重要的有: get_bit(column,index)取位 select get_bit(b,2) from varbit_test where a=1 1 row(s) returned. get_bit 1 set_bit(column,index,value)存位 UPDATE varbit_test SET b = ( SELECT set_bit(b,2,0) FROM varbit_test WHERE a=1 ) where a=1 select get_bit(b,2) from varbit_test where a=1 1 row(s) returned. get_bit 0 &,|,#,~按位二进制操作,分别是与,或,异或,非 用他们就可以做一个简单的权限控制系统了,设置权限就是原数据|权限,删除权限就是原数据&~权限,检查权限就是用原数据&权限==权限 SELECT CASE WHEN b& B'101' = B'101' THEN 'true' ELSE 'false' END AS result FROM varbit_test WHERE a=1 1 row(s) returned. result false SELECT CASE WHEN b& B'101' = B'100' THEN 'true' ELSE 'false' END AS result FROM varbit_test WHERE a=1 1 row(s) returned. result true money 货币类型也是pg中定义的一种特殊数值类型,它占用8 bytes,取值范围为-92233720368547758.08到+92233720368547758.07.数据类型numeric,int和bigint的值可以被造型成money.从数据类型real和double precision的转换则必须先造型成numeric再构造为money.money的输出默认为美元符号. 执行整数值对money值的除法时小数部分将截断为零.为了得到一个四舍五入的结果,可以通过除以一个浮点值,或者在除法计算之前将money值转换为numeric,然后返回到money.(后者可以避免精确度损失的风险)当一个money值被另一个money值除时,结果是double precision(即一个纯数字,而不是金额). SELECT '12.34'::float8::numeric::money 1 row(s) returned. money $12.34 uuid 字符串型的扩展,存储符合RFC 4122,ISO/IEC 9834-8:2005以及相关标准定义的通用唯一标识符(UUID).uuid常用作分布式系统中作为唯一标识符,它空间占用比较大,索引效率也比较低但基本可以保证不会重复. uuid通常可以用在分表或者分布式存储的情况下做唯一性约束(或者主键),但用它做hash来分表就很不合适,一般用它做唯一性约束的情况更多的是使用时间日期作为分表的key. pg中支持用如下几种方式输入uuid: a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11 A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11 {a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11} a0eebc999c0b4ef8bb6d6bb9bd380a11 a0ee-bc99-9c0b-4ef8-bb6d-6bb9-bd38-0a11 {a0eebc99-9c0b4ef8-bb6d6bb9-bd380a11} select '{a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11}'::uuid 1 row(s) returned. uuid a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11 网络地址 PG提供用于存储 IPv4/IPv6 和 MAC 地址的数据类型,用这些数据类型存储网络地址比用纯文本类型好,因为pg为这些类型提供输入错误检查以及特殊的操作符和函数. pg支持的类型包括: 名字 存储尺寸 描述 cidr 7或19字节 IPv4和IPv6网络 inet 7或19字节 IPv4和IPv6主机以及网络 macaddr 6字节 MAC地址 macaddr8 8 字节 MAC 地址(EUI-64 格式) net和cidr类型之间的本质区别是inet接受右边有非零位的网络掩码,而cidr不接受. 例如192.168.0.1/24对inet来说是有效的,但是cidr来说是无效的 select '192.168.0.1/24'::inet 1 row(s) returned. inet 192.168.0.1/24 select '192.168.0.1'::inet 1 row(s) returned. inet 192.168.0.1 select '192.168.0.1'::cidr 1 row(s) returned. cidr 192.168.0.1/32 文本搜索类型 PostgreSQL提供两种数据类型,它们被设计用来支持全文搜索,全文搜索是一种在自然语言的文档集合中搜索以定位那些最匹配一个查询的文档的活动.tsvector类型表示一个为文本搜索优化的形式下的文档,tsquery类型表示一个文本查询.全文搜索相关内容我们后面单独说 几何图形 pg的一大特色就是对几何图形数据的支持,它支持的图形包括: 类型名 存储尺寸 图形 输入形式 point 16字节 平面上的点 (x,y) line 32字节 无限长的线即y=-(A/B)*x-(C/B) {A,B,C} lseg 32字节 有限线段 ((x1,y1),(x2,y2)) box 32字节 矩形框 ((x1,y1),(x2,y2)) path 16+16n字节 封闭路径(类似于多边形) ((x1,y1),...) path 16+16n字节 开放路径 [(x1,y1),...] polygon 40+16n字节 多边形(类似于封闭路径) ((x1,y1),...) circle 24字节 圆,即以(x,y)为圆心,r为半径的圆 同时几何图形间pg有相应的运算符和函数.几何图形操作时pg在地理信息数据领域应用的基础.后面会详细介绍 select '((0,0),(1,1))'::box + '(2.0,0)'::point as result 1 row(s) returned. result (3,1),(2,0) 类型转换 pg是强类型数据库,多数时候需要显式的使用::做类型转换,上面已经有演示.::可以做链式转换. select '12.32'::float8::numeric::money 1 row(s) returned. money $12.32 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:43:24 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/递归查询.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/递归查询.html","title":"递归查询","keywords":"","body":"递归查询 递归查询是pg对sql语句with语句的扩展.它可以用于构造一些复杂查询.尤其是适合查找比如关注,比如亲缘关系这样的社会网络关系,构造图结构. 简单with语句 WITH提供了一种方式来书写在一个大型查询中使用的辅助语句.这些语句通常被称为公共表表达式或CTE,它们可以被看成是定义只在一个查询中存在的临时表,在WITH子句中的每一个辅助语句可以是一个SELECT,INSERT,UPDATE或DELETE,并且WITH子句本身也可以被附加到一个主语句,主语句也可以是SELECT,INSERT,UPDATE或DELETE.我们可以将其看作定义了一个只在一次查询中使用的函数 -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True 下面这个例子我们使用with语句先查出了Tom买糖的所有记录,然后再在这个些记录中做聚合,这个例子当然完全可以一条sql解决,但这只是一个示例,目的是介绍with语句的用法.更复杂的查询都可以使用With构造中间值. WITH tom_buy AS ( SELECT * FROM buy_candy WHERE name='Tom' ) SELECT buy,count(*) AS times from tom_buy GROUP BY buy 15 row(s) returned. buy times 11 12 9 13 3 8 5 12 4 13 0 9 10 13 6 7 14 13 13 17 2 14 7 17 12 10 1 10 8 12 递归查询 递归查询利用with语句,使用RECURSIVE修饰,它的一般结构是: 一个非递归项， UNION或者UNION ALL 一个递归项 下面这个例子我们用递归查询斐波那契数列,我们使用limit来指定递归计算的次数,也就是数列第几位,注意这个方法很危险,此处只是演示. WITH RECURSIVE t(n1,n2) AS ( select 0,1 UNION ALL SELECT n2, n1+n2 FROM t ) select max(foo.n2) as fib8 from (SELECT n2 FROM t limit 8) as foo 1 row(s) returned. fib8 21 递归查询的执行步骤大致如下: 计算非递归项.如果使用的是UNION不是UNION ALL则抛弃重复行.把所有剩余的行包括在递归查询的结果中,并且也把它们放在一个临时的工作表中. 只要工作表不为空,重复下列步骤： 计算递归项.如果使用的是UNION不是UNION ALL则抛弃重复行,抛弃那些与之前结果行重复的行,将剩下的所有行包括在递归查询的结果中,并且也把它们放在一个临时的中间表中. 用中间表的内容替换工作表的内容,然后清空中间表. 当工作表为空则递归将停止. 实用些的例子 一个实用的例子是找出一个员工的所有下属,通常一个公司里员工关系可以表现为树状: Michael North--| |--Megan Berry--| | |--Bella Tucker | |--Ryan Metcalfe--| | | |--Piers Paige | | |--Ryan Henderson | | | |--Max Mills--| | | |--Frank Tucker | | |--Nathan Ferguson | | |--Kevin Rampling | | | |--Benjamin Glover | |--Sarah Berry--| | |--Carolyn Henderson | |--Nicola Kelly | |--Alexandra Climo | |--Dominic King | | |--Zoe Black--| | |--Leonard Gray | |--Eric Rampling | |--Tim James 都画成图了我们自然可以很轻易的找出来,但在数据库中就没那么容易了 CREATE TABLE employees ( employee_id serial PRIMARY KEY, full_name VARCHAR NOT NULL, manager_id INT ) INSERT INTO employees ( employee_id, full_name, manager_id ) VALUES (1, 'Michael North', NULL), (2, 'Megan Berry', 1), (3, 'Sarah Berry', 1), (4, 'Zoe Black', 1), (5, 'Tim James', 1), (6, 'Bella Tucker', 2), (7, 'Ryan Metcalfe', 2), (8, 'Max Mills', 2), (9, 'Benjamin Glover', 2), (10, 'Carolyn Henderson', 3), (11, 'Nicola Kelly', 3), (12, 'Alexandra Climo', 3), (13, 'Dominic King', 3), (14, 'Leonard Gray', 4), (15, 'Eric Rampling', 4), (16, 'Piers Paige', 7), (17, 'Ryan Henderson', 7), (18, 'Frank Tucker', 8), (19, 'Nathan Ferguson', 8), (20, 'Kevin Rampling', 8); SELECT * FROM employees 20 row(s) returned. employee_idfull_name manager_id 1Michael North 2Megan Berry 1 3Sarah Berry 1 4Zoe Black 1 5Tim James 1 6Bella Tucker 2 7Ryan Metcalfe 2 8Max Mills 2 9Benjamin Glover 2 10Carolyn Henderson 3 11Nicola Kelly 3 12Alexandra Climo 3 13Dominic King 3 14Leonard Gray 4 15Eric Rampling 4 16Piers Paige 7 17Ryan Henderson 7 18Frank Tucker 8 19Nathan Ferguson 8 20Kevin Rampling 8 我们希望通过递归查询的方法找到Megan Berry的所有下级(当然包括他自己) WITH RECURSIVE subordinates AS ( ( SELECT employee_id, manager_id, full_name FROM employees WHERE employee_id = 2 ) UNION ( SELECT e.employee_id, e.manager_id, e.full_name FROM employees e INNER JOIN subordinates s ON s.employee_id = e.manager_id ) ) SELECT * FROM subordinates 10 row(s) returned. employee_id manager_idfull_name 2 1Megan Berry 6 2Bella Tucker 7 2Ryan Metcalfe 8 2Max Mills 9 2Benjamin Glover 16 7Piers Paige 17 7Ryan Henderson 18 8Frank Tucker 19 8Nathan Ferguson 20 8Kevin Rampling Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:42:25 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/事件监听.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/事件监听.html","title":"事件监听","keywords":"","body":"事件监听 pg除了作为数据库,也可以作为一个轻量级的应用间通信机制,有了它,具有访问数据库能力的应用可以轻易的利用PG实现互操作.当然由于消息队列是存放在内存里面的,在发生实例宕机等问题时消息将丢失.对可靠性要求高的应用需要自己进行消息持久化(如利用PG存储消息,进行持久化). NOTIFY {channel} [,message]/select pg_notify(channel text, message text)向频道发送消息(消息必须是小于8000字节的字符串). 使用LISTEN channel监听频道的消息 使用UNLISTEN { channel | * }取消监听状态 使用函数pg_listening_channels()可以查询当前session已注册了哪些消息监听 需注意如果在事务中调用notify发送消息,实际消息要在事务提交时才会被发送,如果事务回滚,消息将不会被发送. 如果在一个事务中发送两条消息的通道名称相同,消息字符串也完全相同,实际上只有一条消息被发送出去.notify能保证来自同一个事务的信息按照发送时的顺序交付,也能保证来自不同事务的信息按照事务提交的顺序交付. 消息队列持有被发送但是未被监听会话处理的消息,这些消息太多会导致该队列变满,此时若调用notify命令会在提交时失败.但队列空间通常很大,在默认安装中是8GB,因此一般不会满.如果一个会话执行listen命令后,长时间处于一个事务中，不清理消息则可能导致队列变满. -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True LISTEN virtual; NOTIFY virtual, 'qwer'; UNLISTEN virtual; NOTIFY virtual; Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:43:41 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/物化视图.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/物化视图.html","title":"物化视图","keywords":"","body":"物化视图 视图基本是所有关系数据库的标配,但物化视图算是pg的特色功能了.PG中的所谓物化视图实际是一种缓存机制,与一般的view由本质上的不同,物化视图是物理实际存在的表.我们可以通过使用语句REFRESH MATERIALIZED VIEW手动刷新更新这张表中的内容.这个特性在目标表特别大查询效率特别低而且使用传统方法(例如索引)无法显著提高效率;但对查询速度有要求,对数据的时效性没有那么敏感的情况下十分有用. 当然了另一个更加通用的方法是将数据缓存到redis中通过设置过期时间实现类似功能.这个是后话咱会在后面介绍redis时详细说. 创建物化视图的语句 物化视图使用CREATE MATERIALIZED VIEW语句创建 CREATE MATERIALIZED VIEW [ IF NOT EXISTS ] table_name [ (column_name [, ...] ) ] [ USING method ] [ WITH ( storage_parameter [= value] [, ... ] ) ] [ TABLESPACE tablespace_name ] AS query [ WITH [ NO ] DATA ] 物化视图有很多和表相同的属性,但是不支持临时物化视图以及自动生成OID. 物化视图的更改使用ALTER MATERIALIZED VIEW语句其规则也和修改表类似 物化视图的删除使用DROP MATERIALIZED VIEW语句其规则也和删除表表类似 物化视图刷新数据 物化视图中的数据使用REFRESH MATERIALIZED VIEW语句刷新 pg的物化视图按刷新的方式可以分为如下几种: 物化视图类型 特点 快照物化视图(snapshot materialized view) 最容易实现,物化视图中的数据需要手动刷新 积极物化视图(Eager materialized view) 物化视图在数据库被更新时同步更新,可以通过系统触发器实现 惰性物化视图(Lazy materialized view) 物化视图在事务提交时更新 非常消极物化视图(Very Lazy materialized view) 类似快照物化视图,区别在于变化都会被增量式地记录下来并在手动刷新时被应用 阻塞更新与非阻塞更新 物化视图的更新是阻塞操作,在更行的同时不能进行查询.虽然刷新够快就没太大问题,但要知道物化视图很多时候就是缓存大查询结果用的,我们可以使用 refresh materialized view concurrently {viewname}这个语句,注意关键是增加了concurrently命令,这个命令的使用是有条件的--这个物化视图上必须有唯一索引. 例子 下面是一个简单的例子,有Tom,Jack,Lucy3个人,我们用一张随机生成的表模拟他们一年时间购买15种糖果的行为记录.假设Tom每天40%的几率会在15种糖果种买一个,Jack则为20%,Lucy则为55%.我们用pandas生成这样一张表然后填入pg ps:这个部分使用的是python import pandas as pd from random import random,choice def make_row(name,rate): b = random() if b 使用pg构建查询 -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True 我们来构建一个物化视图buy_candy_mview,用它来统计每种糖果被谁买了多少次,首先是简单的查询 SELECT buy,name,count(*) AS times FROM buy_candy GROUP BY buy,name ORDER BY times DESC 45 row(s) returned. buyname times 0Lucy 18 7Lucy 17 13Lucy 16 2Lucy 16 3Lucy 16 9Lucy 16 5Lucy 15 7Tom 15 12Lucy 15 2Tom 13 11Tom 12 13Tom 12 4Lucy 12 10Lucy 12 9Tom 12 1Lucy 11 4Tom 11 8Tom 11 10Tom 11 11Lucy 11 14Tom 11 5Jack 10 5Tom 10 1Tom 9 0Tom 9 14Lucy 8 6Lucy 8 12Tom 7 11Jack 7 6Tom 6 3Jack 6 6Jack 5 8Jack 5 3Tom 5 4Jack 5 1Jack 5 10Jack 4 13Jack 4 7Jack 4 12Jack 4 9Jack 4 0Jack 3 14Jack 3 8Lucy 3 2Jack 3 然后我们利用这个查询语句构建一个物化视图 CREATE MATERIALIZED VIEW IF NOT EXISTS buy_candy_mview AS SELECT buy,name,count(*) AS times FROM buy_candy GROUP BY buy,name ORDER BY times DESC SELECT * FROM buy_candy_mview limit 10 10 row(s) returned. buyname times 0Lucy 18 7Lucy 17 13Lucy 16 2Lucy 16 3Lucy 16 9Lucy 16 5Lucy 15 7Tom 15 12Lucy 15 2Tom 13 我们甚至可以给这个物化视图创建索引来提高查询效率 CREATE INDEX IF NOT EXISTS buy_candy_mview_name_buy ON buy_candy_mview (name, buy) NOTICE: relation \"buy_candy_mview_name_buy\" already exists, skipping 接着我们切换回python,为其新增2个月的数据 import pandas as pd from random import random,choice def make_row(name,rate): b = random() if b -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True 我们来观察下原表和这个物化视图的变化 SELECT buy,name,count(*) AS times FROM buy_candy GROUP BY buy,name ORDER BY times DESC limit 10 10 row(s) returned. buyname times 0Lucy 20 2Lucy 20 7Lucy 20 12Lucy 19 9Lucy 18 3Lucy 18 7Tom 17 13Tom 17 13Lucy 17 5Lucy 16 SELECT * FROM buy_candy_mview limit 10 10 row(s) returned. buyname times 0Lucy 18 7Lucy 17 13Lucy 16 2Lucy 16 3Lucy 16 9Lucy 16 5Lucy 15 7Tom 15 12Lucy 15 2Tom 13 可以清晰的看到原表的变化不会引起物化视图的变化.我们这会儿刷新下物化视图 REFRESH MATERIALIZED VIEW buy_candy_mview SELECT * FROM buy_candy_mview limit 10 10 row(s) returned. buyname times 0Lucy 20 2Lucy 20 7Lucy 20 12Lucy 19 9Lucy 18 3Lucy 18 13Lucy 17 13Tom 17 7Tom 17 5Lucy 16 这样数据就是最新的了 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:43:01 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/表继承.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/特色功能/表继承.html","title":"表继承","keywords":"","body":"表继承 pg是面向对象数据库,表继承就是其特性之一,使用表继承数据库设计可以有新的可能性. 通常我们称被继承的表为父表,继承的表为子表,由于可以多继承,因此一个子表可以有多个父表.下面是表继承的行为特点: 子表里面的数据是在父表的基础上进行扩展 子表里面的数据会汇聚到父表 修改子表里面的数据，父表对应的数据会进行变化 修改父表中对应子表的数据，子表的数据也会变化 所有父表的检查约束和非空约束都会自动被所有子表继承.不过其它类型的约束(唯一/主键/外键约束)不会被继承 一个子表可以从多个父表继承,这种情况下它将拥有所有父表字段的总和,并且子表中定义的字段也会加入其中.如果同一个字段名出现在多个父表中,或者同时出现在父表和子表的定义里,那么这些字段就会被\"融合\",这样在子表里就只有一个这样的字段.要想融合,字段的数据类型必须相同否则就会抛出一个错误.融合的字段将会拥有其父字段的所有检查约束,并且如果某个父字段存在非空约束,那么融合后的字段也必须是非空的. 任何存在子表的父表都不能被删除,同样,子表中任何从父表继承的字段或约束也不能被删除或修改.如果你想删除一个表及其所有后代,最简单的办法是使用CASCADE选项删除父表 ALTER TABLE会把所有数据定义和检查约束传播到后代里面去.另外只有在使用CASCADE选项的情况下才能删除依赖于其他表的字段.ALTER TABLE在重复字段融合和拒绝方面和CREATE TABLE的规则相同. 访问父表会自动访问在子表中的数据,而不需要更多的访问权限检查.这保留了父表中数据的表现.然而直接访问子表不会自动允许访问父表.要访问父表需要更进一步的权限被授予. 表继承定义的语法 定义继承关系有3种方法: 显式的声明 定义子表通常使用语法CREATE TABLE tablename () INHERITS(parent...), 修改表约束 一个已经定义过的子表可以使用带INHERIT的ALTER TABLE 命令添加一个新父表 例子 继承可以用在描述一些互相包含的概念上,比如一个很实际的例子-员工. 我们假设有这样一个需求:要创建一个描述员工的表,它包含姓名,性别,职位;也要描述有股份的员工,他们回多出一个股份属性. 如果没有表继承我们该怎样设计呢? 一个比较朴素的方法是设计一张表包含姓名,性别,职位和股份,只是非股东都是0.这个设计当然可以用,但并不一定股份是0的就不是股东,而且非股东全部多一个为0的字段且明显非股东数量会远大于股东,这样的设计比较冗余称不上优雅. 另一个方法是设计两张表,一张非股东只包含姓名,性别,职位,另一张股东包含姓名,性别,职位和股份,然后构造一个view将两张表并联起来专门用作查询.这样对于查询和简单的更新操作没啥问题,但当我们需要有较为复杂的更新操作时就会比较麻烦,并不利于扩展. 使用表继承的话我们可以简单定义两个表--员工和股东,股东继承员工并增加一行股份字段. -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True CREATE TYPE gender AS ENUM ('male', 'female') CREATE TABLE staff ( name text, gender gender, position text ) CREATE TABLE shareholder( share int4 ) INHERITS (staff) BEGIN; INSERT INTO staff (name,gender,position) VALUES ('Tom','male','工程师'); INSERT INTO staff (name,gender,position) VALUES ('Jim','male','设计师'); INSERT INTO staff (name,gender,position) VALUES ('Jack','male','研究员'); INSERT INTO staff (name,gender,position) VALUES ('Lucy','female','行政管理'); COMMIT; SELECT * FROM staff 4 row(s) returned. name gender position Tom male 工程师 Jim male 设计师 Jack male 研究员 Lucy female 行政管理 SELECT * FROM shareholder 0 row(s) returned. BEGIN; INSERT INTO shareholder (name,gender,position,share) VALUES ('Tony','male','首席技术官',12); INSERT INTO shareholder (name,gender,position,share) VALUES ('Jiny','male','首席设计师',12); INSERT INTO shareholder (name,gender,position,share) VALUES ('Jacky','male','首席科学家',12); INSERT INTO shareholder (name,gender,position,share) VALUES ('Lucif','female','首席财务官',12); COMMIT; SELECT * FROM staff 8 row(s) returned. name gender position Tom male 工程师 Jim male 设计师 Jack male 研究员 Lucy female 行政管理 Tony male 首席技术官 Jiny male 首席设计师 Jacky male 首席科学家 Lucif female 首席财务官 SELECT * FROM shareholder 4 row(s) returned. name gender position share Tony male 首席技术官 12 Jiny male 首席设计师 12 Jacky male 首席科学家 12 Lucif female 首席财务官 12 SELECT * FROM ONLY staff 4 row(s) returned. name gender position Tom male 工程师 Jim male 设计师 Jack male 研究员 Lucy female 行政管理 可以看到如果我们想只查询表专有的数据,只要在表名前加上ONLY描述即可.SELECT,UPDATE和DELETE都支持ONLY描述. SELECT tableoid,name FROM staff 8 row(s) returned. tableoidname 90139Tom 90139Jim 90139Jack 90139Lucy 90145Tony 90145Jiny 90145Jacky 90145Lucif 每个表都有个默认字段为tableoid,我们如果想知道某条数据来自哪个表也可以显式的查找这个字段 表继承的应用 表继承主要还是应用在业务上为主.我们的业务通常都是会扩展的,每次扩展业务后往往都会新增一些字段,但这些字段又往往和原来的业务无关,修改原来业务使用的表显然非常不合理,毕竟既不优雅,也容易造成数据治理的混乱,这种时候表继承就会非常有用.新的业务新建一张表继承原来业务的表这样就完美解决问题了. 表继承的另一个用处在分区,一些比较大的表比如一些用户行为的log数据都是按时间做只增写入的这种表一般会很大,如果要查询也会非常的慢,这个时候可以考虑按时间做分表.不过现在有时序数据库插件timescaledb这个用途其实已经没必要了,后面我们会介绍这个插件 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:42:42 "},"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/自定义扩展/自定义数据类型.html":{"url":"工具链篇/存储工具/使用postgresql做数据存储/功能与特性/自定义扩展/自定义数据类型.html","title":"自定义数据类型","keywords":"","body":"自定义类型 pg支持使用CREATE TYPE语句自定义类型,可以定义的类型有两种 枚举型类型 复合类型 -- connection: postgres://postgres:postgres@localhost:5432/test -- autocommit: true switched autocommit mode to True 枚举型类型 枚举(enum)类型是由一个静态,值为有序集合的数据类型.它们等效于很多编程语言所支持的enum类型.枚举类型的一个例子可以是一周中的日期,或者一个数据的状态值集合. CREATE TYPE mood AS ENUM ('sad', 'ok', 'happy') type \"mood\" already exists CREATE TABLE IF NOT EXISTS enum_test( a serial4 PRIMARY KEY, b mood ) NOTICE: relation \"enum_test\" already exists, skipping INSERT INTO enum_test (b) VALUES ( 'happy') select * from enum_test 2 row(s) returned. ab 1happy 2happy INSERT INTO enum_test (b) VALUES ( 'happyly') invalid input value for enum mood: \"happyly\" LINE 1: INSERT INTO enum_test (b) VALUES ( 'happyly') ^ 复合类型 一个复合类型表示一行或一个记录的结构,它本质上就是一个域名和它们数据类型的列表,一个典型的应用就是复数. 复合类型本质上就是一个表,要插入一个复合类型可以使用ROW()函数 CREATE TYPE complex AS ( r float8, i float8 ) type \"complex\" already exists CREATE TABLE IF NOT EXISTS complex_test( a serial4 PRIMARY KEY, b complex ) NOTICE: relation \"complex_test\" already exists, skipping INSERT INTO complex_test (b) VALUES (ROW(1.8,3.3)) select * from complex_test 2 row(s) returned. ab 1(1.8,3.3) 2(1.8,3.3) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 23:44:50 "},"工具链篇/计算工具/":{"url":"工具链篇/计算工具/","title":"计算工具","keywords":"","body":"使用python做计算 python本身并不以执行速度见长,但由于它非常容易扩展,加上语言语法设计优雅易学,许多高性能计算框架都是使用C或者其他高性能语言编写,然后封装为python模块,将python作为交互用的脚本来使用的.因此实际上python的高性能计算工具开发和工具使用实际上是脱钩的两个技术线路.本章介绍当前最优质成熟的python计算工具. 本部分内容分为: 符号计算 数值计算 概率推理 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/":{"url":"工具链篇/计算工具/符号计算/","title":"符号计算","keywords":"","body":"符号计算 符号计算又称计算机代数,通俗地说就是用计算机推导数学公式,如对表达式进行因式分解,化简,微分,积分,解代数方程,求解常微分方程等. 符号计算主要是操作数学对象与表达式.这些数学对象与表达式可以直接表现自己,它们不是估计/近似值.表达式/对象是未经估计的变量只是以符号形式呈现. 使用SymPy进行符号计算 SymPy是python环境下的符号计算库,他可以用于: 简化数学表达式 计算微分,积分与极限 求方程的解 矩阵运算以及各种数学函数. 所有这些功能都通过数学符号完成. 下面是使用SymPy做符号计算与一般计算的对比: 一般的计算 import math math.sqrt(3) 1.7320508075688772 math.sqrt(27) 5.196152422706632 使用SymPy进行符号计算 import sympy sympy.sqrt(3) $\\displaystyle \\sqrt{3}$ sympy.sqrt(27) $\\displaystyle 3 \\sqrt{3}$ SymPy程序库由若干核心能力与大量的可选模块构成.SymPy的主要功能: 包括基本算术与公式简化,以及模式匹配函数,如三角函数/双曲函数/指数函数与对数函数等(核心能力) 支持多项式运算,例如基本算术/因式分解以及各种其他运算(核心能力) 微积分功能,包括极限/微分与积分等(核心能力) 各种类型方程式的求解,例如多项式求解/方程组求解/微分方程求解(核心能力) 离散数学(核心能力) 矩阵表示与运算功能(核心能力) 几何函数(核心能力) 借助pyglet外部模块画图 物理学支持 统计学运算，包括概率与分布函数 各种打印功能 LaTeX代码生成功能 使用SymPy的工作流 使用SymPy做符号计算不同于一般计算,它的流程是: 在构建算式前申明符号,然后利用声明的符号构建算式 利用算式进行推导,计算等符号运算操作 输出结果 下面是一个简单的例子,就当作SymPy的helloworld吧 import sympy as sp x, y = sp.symbols('x y') #声明符号x,y expr = x + 2*y # 构造算式 expr $\\displaystyle x + 2 y$ expr + 1 # 在算式之上构建新算式 $\\displaystyle x + 2 y + 1$ expr + x # 新构建的算式可以明显的化简就会自动化简 $\\displaystyle 2 x + 2 y$ x*(expr) # 新算式不能明显的化简,比如这个例子,就不会自动化简 $\\displaystyle x \\left(x + 2 y\\right)$ expand_expr = sp.expand(x*(expr)) # 手动化简新算式 expand_expr $\\displaystyle x^{2} + 2 x y$ sp.factor(expand_expr) # 将化简的式子做因式分解 $\\displaystyle x \\left(x + 2 y\\right)$ sp.latex(expand_expr) # 输出符号的latex代码 'x^{2} + 2 x y' Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/声明符号和内置符号类型.html":{"url":"工具链篇/计算工具/符号计算/声明符号和内置符号类型.html","title":"声明符号和内置符号类型","keywords":"","body":"声明符号和内置符号类型 sympy是符号运算,与一般情况下用python做数值计算的最大区别在于,在构建算式前需要先声明符号,这些符号当然可以是有多重含义的,sympy内置了几种数据类型来区别. import sympy as sp 符号类型 symbols定义了浮点数,整数,有理数,复数4种合法的数据类型,以及一些特殊的数值类型 浮点数 1/2 0.5 整数 1 1 有理数Rational() sp.Rational(1,3) $\\displaystyle \\frac{1}{3}$ 无理数 sqrt() 例如开根号,一个数如果是一个完全平方数,开根号后就是有理数；反之,是无理数. sp.sqrt(2) $\\displaystyle \\sqrt{2}$ 同时像一些常见的无理数常数比如e,\\pi也都支持 也可以简单的用幂来表示 2**(sp.Rational(1,2)) $\\displaystyle \\sqrt{2}$ 3**(sp.Rational(1,3)) $\\displaystyle \\sqrt[3]{3}$ 复数 除了复数单元I是虚数，符号可以被用属性创建(例如 real,positive,complex,等等)这将影响它们的表现： 1+1j (1+1j) 内置符号声明 符号可以分为变量和常量,sympy中的常量可以直接引入,我们看一个典型的常数符号的等式,欧拉公式 e^{i\\pi}+1=0 ps:欧拉恒等式是复分析中的欧拉公式的特例,欧拉公式如下: e^{ix}=\\cos x + i\\sin x \\\\ \\text x为任意实数 from sympy import E,I,pi#从包中导入所需常数符号 E**(I*pi)+1 $\\displaystyle 0$ 自定义符号声明 声明符号使用函数symbols(names, **args)函数 x = sp.symbols(\"x\") 同时声明多个变量 x,y,z = sp.symbols(\"x,y,z\") 为符号指定类型 x,y,z = sp.symbols(\"x,y,z\",integer=True) 声明大量符号变量 如果用到大量的符号可以使用一些简便的方法定义有规律的符号 sp.symbols('x:10') (x0, x1, x2, x3, x4, x5, x6, x7, x8, x9) sp.symbols('y5:10') (y5, y6, y7, y8, y9) sp.symbols('x(:c)') (xa, xb, xc) sp.symbols('a:z') (a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z) sp.symbols('x:2(1:3)') (x01, x02, x11, x12) sp.symbols('f((a:b))') (f(a), f(b)) sp.symbols('x(:1\\,:2)') # or 'x((:1)\\,(:2))' (x(0,0), x(0,1)) 声明一个复数 复数的声明比较复杂,事实上复数是由实部虚部两部分组成的,因此必须要由两部分符号组成,而虚部的符号i或者j可以使用sympy自带的常量I表示,因此可以将复数作为一个算式来处理 x = sp.symbols('x') y = sp.symbols('y') c = x+I*y c $\\displaystyle x + i y$ 声明一个函数符号 我们也可以指定符号为函数 f = sp.symbols(\"f\",cls=sp.Function) f = x+2*y+z f $\\displaystyle x + 2 y + z$ 构建表达式 算式是由符号组成的运算过程,其构造过程可以有2种 使用符号构建表达式 符号间直接使用运算符或数学函数连接就可以构造表达式 x+2*y+z $\\displaystyle x + 2 y + z$ 使用字符串构建表达式 这种方式需要使用函数sympify(exp_str,evaluate:bool)其中evaluate这个参数决定是否进行化简 sp.sympify(\"x+2*y+z\") $\\displaystyle x + 2 y + z$ exp1 = sp.sympify(\"x+y+z+y\",evaluate=True) exp1 $\\displaystyle x + 2 y + z$ exp2 = sp.sympify(\"x+y+z+y\",evaluate=False) exp2 $\\displaystyle x + y + y + z$ 一个例子: 推导欧拉公式 e^{ix}=\\cos x + i\\sin x 这里用到了如下几个符号: 运算符 from sympy import cos,sin,exp 常量 from sympy import I,pi 变量 from sympy import symbols x = symbols('x',real=False) 左边为: exp(I*x) exp(I*x) 右边算式为: right = cos(x)+I*sin(x) right I*sin(x) + cos(x) 我们用泰勒级数展开左边 from sympy import series#泰勒展开函数 series(exp(I*x), x, 0, 10)#公式,变量 1 + I*x - x**2/2 - I*x**3/6 + x**4/24 + I*x**5/120 - x**6/720 - I*x**7/5040 + x**8/40320 + I*x**9/362880 + O(x**10) 右边可以分为两部分,I*sin(x)和cos(x) p1 = series(I*sin(x), x, 0, 10) p1 I*x - I*x**3/6 + I*x**5/120 - I*x**7/5040 + I*x**9/362880 + O(x**10) p2 = series(cos(x), x, 0, 10) p2 1 - x**2/2 + x**4/24 - x**6/720 + x**8/40320 + O(x**10) 两部分的和为 p1+p2 1 + I*x - x**2/2 - I*x**3/6 + x**4/24 + I*x**5/120 - x**6/720 - I*x**7/5040 + x**8/40320 + I*x**9/362880 + O(x**10) 可见左右两边相同 series(exp(I*x), x, 0, 10) == p1+p2 True Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/表达式操作/表达式操作.html":{"url":"工具链篇/计算工具/符号计算/符号计算/表达式操作/表达式操作.html","title":"表达式操作","keywords":"","body":"表达式操作 符号操作系统最有用的特征之一是对表达式的展开/化简等简化数学表达式的能力.SymPy有几十个功能来执行各种简化. 还有一个名为simplify()的通用函数可以以智能方式应用所有这些函数,以获得最简单形式的表达式.这里有些例子 from sympy import init_printing init_printing(use_unicode=True) 通用的启发式化简simplify() 这个函数可以隐式的化简数学表达式 from sympy import symbols,simplify,sin,cos,gamma x, y, z = symbols('x y z') simplify(sin(x)**2 + cos(x)**2) 1 simplify((x**3 + x**2 - x - 1)/(x**2 + 2*x + 1)) x - 1 simplify(gamma(x)/gamma(x - 2)) \\left(x - 2\\right) \\left(x - 1\\right) ps: gamma(x)是伽马函数 $\\Gamma x$ 需要注意的是: 我们看到simplify()能够处理大多数表达式,但是simplify()有一个缺陷--它只是应用SymPy中的所有主要的简化操作,并隐式的确定最简单的结果.这个最简单不是一个定义明确的术语.因此有时候我们还是得手动操作 例如，假设我们想简化$ x^2+ 2x+1 $到$(x + 1)^2$,但实际上结果却不对 simplify(x**2 + 2*x + 1) x^{2} + 2 x + 1 我们没有得到我们想要的.有一个函数来执行这个简化,称为factor()因式分解. from sympy import factor factor(x**2 + 2*x + 1) \\left(x + 1\\right)^{2} 此外simplify函数速度比较慢,因此,条件允许的话最好使用特定的函数处理简化问题而不是使用通用的simplify()函数. 除上面的优点外,使用特定函数简化还有以下优点: 特定函数对其输出的形式具有明确形式保证 例如:当对具有有理系数的多项式进行调用时,factor()保证将多项式因子代入不可约因子;simplify()没有这一保证. 在交互式使用时,当你只想将表达式简化为更简单的形式时,simplify()是个好选择.但如果要更加精确的结果,simplify就不合适了 多项式/有理函数的展开和因式分解 将几个多项式的积构建成变量的每一次都只有一个系数的形式被称为多项式的展开; 把一个多项式在一个范围(如实数范围内分解,即所有项均为实数)化为几个整式的积的形式被称作这个多项式的因式分解; SymPy支持这些操作 expand()函数用来展开多项式 from sympy import expand expand((x + 1)**2) x^{2} + 2 x + 1 expand((x + 2)*(x - 3)) x^{2} - x - 6 factor()函数用来将多项式因式分解 from sympy import factor factor(expand(x**2 + 2*x + 1)) \\left(x + 1\\right)^{2} collect()函数聚合表达式中公因子 collect()可以将多元多项式中的一个指定变量作为变量,其他作为常量 from sympy import collect expr = x*y + x - 3 + 2*x**2 - z*x**2 + x**3 expr x^{3} - x^{2} z + 2 x^{2} + x y + x - 3 collected_expr = collect(expr, x) collected_expr x^{3} + x^{2} \\left(- z + 2\\right) + x \\left(y + 1\\right) - 3 cancel()将取任何有理函数,并将其放入标准规范形式$ \\frac p q$ 其中p和q是没有公因子的扩展多项式,p和q的前导系数没有分母(即是整数) from sympy import cancel expr = (x**2 + 2*x + 1)/(x**2 + x) expr \\frac{x^{2} + 2 x + 1}{x^{2} + x} cancel(expr) \\frac{x + 1}{x} expr = 1/x + (3*x/2 - 2)/(x - 4) expr \\frac{\\frac{3 x}{2} - 2}{x - 4} + \\frac{1}{x} cancel(expr) \\frac{3 x^{2} - 2 x - 8}{2 x^{2} - 8 x} expr = (x*y**2 - 2*x*y*z + x*z**2 + y**2 - 2*y*z + z**2)/(x**2 - 1) expr \\frac{x y^{2} - 2 x y z + x z^{2} + y^{2} - 2 y z + z^{2}}{x^{2} - 1} cancel(expr) \\frac{y^{2} - 2 y z + z^{2}}{x - 1} 需要注意的是cancel不会自己因式分解,依然需要使用factor函数来做 factor(expr) \\frac{\\left(y - z\\right)^{2}}{x - 1} apart()对有理函数执行部分分数分解 from sympy import apart expr = (4*x**3 + 21*x**2 + 10*x + 12)/(x**4 + 5*x**3 + 5*x**2 + 4*x) expr \\frac{4 x^{3} + 21 x^{2} + 10 x + 12}{x^{4} + 5 x^{3} + 5 x^{2} + 4 x} apart(expr) \\frac{2 x - 1}{x^{2} + x + 1} - \\frac{1}{x + 4} + \\frac{3}{x} 三角函数分解 反三角函数 SymPy遵循Python对反三角函数的命名约定,即将一个a附加到函数名称的前面.例如:反余弦或反余弦称为acos() from sympy import sin,cos,tan,sec,asin,acos,atan,cosh,sinh,tanh trigsimp()使用三角标识简化表达式 这个方法它同样可以对双曲三角函数有效 from sympy import trigsimp expr = sin(x)**2 + cos(x)**2 expr \\sin^{2}{\\left (x \\right )} + \\cos^{2}{\\left (x \\right )} trigsimp(expr) 1 expr = sin(x)**4 - 2*cos(x)**2*sin(x)**2 + cos(x)**4 expr \\sin^{4}{\\left (x \\right )} - 2 \\sin^{2}{\\left (x \\right )} \\cos^{2}{\\left (x \\right )} + \\cos^{4}{\\left (x \\right )} trigsimp(expr) \\frac{\\cos{\\left (4 x \\right )}}{2} + \\frac{1}{2} expr = sin(x)*tan(x)/sec(x) expr \\frac{\\sin{\\left (x \\right )} \\tan{\\left (x \\right )}}{\\sec{\\left (x \\right )}} trigsimp(expr) \\sin^{2}{\\left (x \\right )} expr = cosh(x)**2 + sinh(x)**2 expr \\sinh^{2}{\\left (x \\right )} + \\cosh^{2}{\\left (x \\right )} trigsimp(expr) \\cosh{\\left (2 x \\right )} expr = sinh(x)/tanh(x) expr \\frac{\\sinh{\\left (x \\right )}}{\\tanh{\\left (x \\right )}} trigsimp(expr) \\cosh{\\left (x \\right )} expand_trig()展开三角函数 from sympy import expand_trig expr = sin(x + y) expr \\sin{\\left (x + y \\right )} expand_trig(expr) \\sin{\\left (x \\right )} \\cos{\\left (y \\right )} + \\sin{\\left (y \\right )} \\cos{\\left (x \\right )} expr = tan(2*x) expr \\tan{\\left (2 x \\right )} expand_trig(expr) \\frac{2 \\tan{\\left (x \\right )}}{- \\tan^{2}{\\left (x \\right )} + 1} 指数简化 powsimp()从上到下,从左到右的合并指数 from sympy import symbols,powsimp,sqrt a, b = symbols('a b', real=True) z, t, c, n,m,k = symbols('z t c n m k') expr = x**a*x**b expr x^{a} x^{b} powsimp(expr) x^{a + b} expr = x**a*y**a expr x^{a} y^{a} powsimp(expr) x^{a} y^{a} 可以使用参数force=true将底数合并 expr = t**c*z**c expr t^{c} z^{c} powsimp(expr) t^{c} z^{c} powsimp(expr,force=True) \\left(t z\\right)^{c} 指数合并同样可以用在开根上,毕竟$ \\sqrt 2 = 2^{\\frac 1 2}$ (z*t)**2 t^{2} z^{2} sqrt(x*y) \\sqrt{x y} expand_power_exp() 和 expand_power_base() 展开指数或者底 from sympy import expand_power_exp,expand_power_base expr = x**(a + b) expr x^{a + b} expand_power_exp(expr) x^{a} x^{b} expr = (x*y)**a expr \\left(x y\\right)^{a} expand_power_base(expr) \\left(x y\\right)^{a} powdenest()将指数叠高 from sympy import powdenest expr = (x**a)**b expr \\left(x^{a}\\right)^{b} powdenest(expr) \\left(x^{a}\\right)^{b} expr = (z**a)**b expr \\left(z^{a}\\right)^{b} powdenest(expr) \\left(z^{a}\\right)^{b} powdenest(expr, force=True) z^{a b} 对数分解 expand_log() 对数展开 from sympy import expand_log,log expand_log(log(x*y)) \\log{\\left (x y \\right )} expand_log(log(x/y)) \\log{\\left (\\frac{x}{y} \\right )} expand_log(log(x**2)) \\log{\\left (x^{2} \\right )} expand_log(log(x**n)) \\log{\\left (x^{n} \\right )} expand_log(log(z*t)) \\log{\\left (t z \\right )} 它同样可以使用force=True参数 expand_log(log(z**2)) \\log{\\left (z^{2} \\right )} expand_log(log(z**2), force=True) 2 \\log{\\left (z \\right )} logcombine()将对数求和整合 from sympy import logcombine logcombine(log(x) + log(y)) \\log{\\left (x \\right )} + \\log{\\left (y \\right )} logcombine(n*log(x)) n \\log{\\left (x \\right )} logcombine(n*log(z)) n \\log{\\left (z \\right )} 同样也可以用force=True logcombine(n*log(z), force=True) \\log{\\left (z^{n} \\right )} 特殊函数 阶乘 from sympy import factorial factorial(n) n! 二项式系数函数 binomial coefficient from sympy import binomial binomial(n, k) {\\binom{n}{k}} 伽马函数 阶乘函数与伽马函数伽马密切相关. 对于正整数z,$(z-1)!$与伽马函数$ \\Gamma (z)= \\int_0^\\infty t^{z−1}e^{−t}dt$相同 from sympy import gamma gamma(z) \\Gamma\\left(z\\right) hyper([a_1, ..., a_p], [b_1, ..., b_q], z)广义超几何函数和其展开hyperexpand() from sympy import hyper,hyperexpand,meijerg hyper([1, 2], [3], z) {}_{2}F_{1}\\left(\\begin{matrix} 1, 2 \\\\ 3 \\end{matrix}\\middle| {z} \\right) hyperexpand(hyper([1, 2], [3], z)) - \\frac{2}{z} - \\frac{2 \\log{\\left (- z + 1 \\right )}}{z^{2}} hyperexpand()也适用于更通用的Meijer G函数 expr = meijerg([[1],[1]], [[1],[]], -z) expr {G_{2, 1}^{1, 1}\\left(\\begin{matrix} 1 & 1 \\\\1 & \\end{matrix} \\middle| {- z} \\right)} hyperexpand(expr) e^{\\frac{1}{z}} rewrite方法 处理特殊功能的常见方法是将它们彼此重写.这适用于SymPy中的任何函数,而不只是特殊函数.要根据函数重写表达式,这有点像静态类型语言中的强制类型转换的意思.一个简单的例子就是三角函数中tan(x)可以转化为sin(x)的表达式 tan(x).rewrite(sin) \\frac{2 \\sin^{2}{\\left (x \\right )}}{\\sin{\\left (2 x \\right )}} factorial(x).rewrite(gamma) \\Gamma\\left(x + 1\\right) expand_func函数 要根据某些标识扩展特殊函数可以使用expand_func方法 from sympy import expand_func expand_func(gamma(x + 3)) x \\left(x + 1\\right) \\left(x + 2\\right) \\Gamma\\left(x\\right) combsimp()简化组合表达式 from sympy import combsimp combsimp(factorial(n)/factorial(n - 3)) n \\left(n - 2\\right) \\left(n - 1\\right) combsimp(binomial(n+1, k+1)/binomial(n, k)) \\frac{n + 1}{k + 1} combsimp(gamma(x)*gamma(1 - x)) \\frac{\\pi}{\\sin{\\left (\\pi x \\right )}} 例子: 连续分数化简 $ a_0 + \\frac 1 {a_1 + \\frac 1 {a_2 + \\frac 1 {... + \\frac 1 a_n}}}$ from sympy import Integer 这是一个典型的递归函数,我们可以如此定义 def list_to_frac(l): expr = Integer(0) for i in reversed(l[1:]): expr += i expr = 1/expr return l[0] + expr list_to_frac([x, y, z]) x + \\frac{1}{y + \\frac{1}{z}} list_to_frac([1, 2, 3, 4]) \\frac{43}{30} 每个有限连续分数是一个有理数,但我们对符号感兴趣,所以让我们创建一个符号的连续子符号.我们使用的symbols()函数来创建这些符号 syms = symbols('a0:5') frac = list_to_frac(syms) frac a_{0} + \\frac{1}{a_{1} + \\frac{1}{a_{2} + \\frac{1}{a_{3} + \\frac{1}{a_{4}}}}} 这种形式对于理解连续的分数是有用的,可以更方便的编程,但我们需要将其化简为标准的有理函数形式. frac = cancel(frac) frac \\frac{a_{0} a_{1} a_{2} a_{3} a_{4} + a_{0} a_{1} a_{2} + a_{0} a_{1} a_{4} + a_{0} a_{3} a_{4} + a_{0} + a_{2} a_{3} a_{4} + a_{2} + a_{4}}{a_{1} a_{2} a_{3} a_{4} + a_{1} a_{2} + a_{1} a_{4} + a_{3} a_{4} + 1} 假设我们知道它可以重写为连续分数. 我们如何用SymPy做到这一点? 连续分数递归地为$c + \\frac 1 f$,其中c是整数,f是(较小的)连续分数. 如果我们可以用这种形式写出表达式,我们可以递归地拉出每个c,并将它添加到列表中. 然后我们可以使用我们的list_to_frac()函数获得连续分数 a0,a1,a2,a3,a4 = syms l = [] frac = apart(frac, a0) frac a_{0} + \\frac{a_{2} a_{3} a_{4} + a_{2} + a_{4}}{a_{1} a_{2} a_{3} a_{4} + a_{1} a_{2} + a_{1} a_{4} + a_{3} a_{4} + 1} l.append(a0) frac = 1/(frac - a0) frac \\frac{a_{1} a_{2} a_{3} a_{4} + a_{1} a_{2} + a_{1} a_{4} + a_{3} a_{4} + 1}{a_{2} a_{3} a_{4} + a_{2} + a_{4}} 我们可以重复这一过程 frac = apart(frac, a1) frac a_{1} + \\frac{a_{3} a_{4} + 1}{a_{2} a_{3} a_{4} + a_{2} + a_{4}} l.append(a1) frac = 1/(frac - a1) frac = apart(frac, a2) frac a_{2} + \\frac{a_{4}}{a_{3} a_{4} + 1} l.append(a2) frac = 1/(frac - a2) frac = apart(frac, a3) frac a_{3} + \\frac{1}{a_{4}} l.append(a3) frac = 1/(frac - a3) frac = apart(frac, a4) frac a_{4} l.append(a4) list_to_frac(l) a_{0} + \\frac{1}{a_{1} + \\frac{1}{a_{2} + \\frac{1}{a_{3} + \\frac{1}{a_{4}}}}} Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/微积分/微积分.html":{"url":"工具链篇/计算工具/符号计算/符号计算/微积分/微积分.html","title":"微积分","keywords":"","body":"微积分 SymPy支持微分和积分操作,也支持推导极限 from sympy import init_printing init_printing(use_unicode=True) from sympy import symbols x, y, z = symbols('x y z') diff()微分(求导) from sympy import diff diff(x**3+x**2+x+1) $\\displaystyle 3 x^{2} + 2 x + 1$ diff(exp,var,level)可以求多阶导数,需要指定变量和阶数 diff(x**3+x**2+x+1,x,2) $\\displaystyle 2 \\left(3 x + 1\\right)$ 同样的,也可以求偏导 diff(x**3+x*y**2+x*y+1,x) $\\displaystyle 3 x^{2} + y^{2} + y$ 要创建未化简的的导数,需要使用导数类.它具有与diff相同的语法,但必须显式的指定是谁的微分 from sympy import Derivative exp = diff(x**3+x**2+x+1) exp $\\displaystyle 3 x^{2} + 2 x + 1$ Derivative(exp,x) $\\displaystyle \\frac{d}{d x} \\left(3 x^{2} + 2 x + 1\\right)$ 要推导导数类的实例,可以使用算式的doit()方法 Derivative(exp,x).doit() $\\displaystyle 6 x + 2$ integrate(exp,var)积分 from sympy import integrate from sympy import cos 不定积分 exp = cos(x) exp $\\displaystyle \\cos{\\left(x \\right)}$ integrate(exp, x) $\\displaystyle \\sin{\\left(x \\right)}$ 定积分 定积分一般会有上下限,可以用元组(integration_variable, lower_limit, upper_limit)替换var,sympy.注意oo表示无穷大 from sympy import oo,exp $ \\int _0^\\infty e^{-x} dx$ integrate(exp(-x), (x, 0, oo)) $\\displaystyle 1$ 重积分 比如我们想求如下这个二重积分 $\\int {-\\infty}^\\infty \\int {-\\infty}^\\infty e^{-x^2-y^2} dx dy$ integrate(exp(-x**2 - y**2), (x, -oo, oo), (y, -oo, oo)) $\\displaystyle \\pi$ 如果积分无法求得,那么它会返回原来的样子 expr = integrate(x**x, x) expr $\\displaystyle \\int x^{x}\\, dx$ 就像微分一样,积分也有对应的类型Integral,定义方式也相似 from sympy import Integral,log expr = Integral(log(x)**2, x) expr $\\displaystyle \\int \\log{\\left(x \\right)}^{2}\\, dx$ expr.doit() $\\displaystyle x \\log{\\left(x \\right)}^{2} - 2 x \\log{\\left(x \\right)} + 2 x$ 极限 SymPy可以使用limit函数计算极限。 from sympy import limit,sin limit(sin(x)/x, x, 0) $\\displaystyle 1$ expr = x**2/exp(x) expr.subs(x, oo) $\\displaystyle \\text{NaN}$ limit(expr, x, oo) $\\displaystyle 0$ 极限也有一个类,也和上面的微分积分差不多 from sympy import Limit expr = Limit((cos(x) - 1)/x, x, 0) expr $\\displaystyle \\lim_{x \\to 0^+}\\left(\\frac{\\cos{\\left(x \\right)} - 1}{x}\\right)$ expr.doit() $\\displaystyle 0$ 泰勒展开 泰勒公式是一个用函数在某点的信息描述其附近取值的公式.如果函数足够平滑的话,在已知函数在某一点的各阶导数值的情况之下,泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值.泰勒公式还给出了这个多项式和实际的函数值之间的偏差. SymPy可以计算围绕一个点的函数做泰勒级数展开.它使用算式的.series(x, x0, n)方法,就像之前在验证欧拉公式时我们做的那样 expr = exp(sin(x)) expr $\\displaystyle e^{\\sin{\\left(x \\right)}}$ expr.series(x, 0, 4) $\\displaystyle 1 + x + \\frac{x^{2}}{2} + O\\left(x^{4}\\right)$ from sympy import symbols x0 = symbols(\"x0\") expr.series(x, x0, 4) $\\displaystyle e^{\\sin{\\left(x{0} \\right)}} + \\left(x - x{0}\\right) e^{\\sin{\\left(x{0} \\right)}} \\cos{\\left(x{0} \\right)} + \\left(x - x{0}\\right)^{2} \\left(- \\frac{e^{\\sin{\\left(x{0} \\right)}} \\sin{\\left(x{0} \\right)}}{2} + \\frac{e^{\\sin{\\left(x{0} \\right)}} \\cos^{2}{\\left(x{0} \\right)}}{2}\\right) + \\left(x - x{0}\\right)^{3} \\left(- \\frac{e^{\\sin{\\left(x{0} \\right)}} \\sin{\\left(x{0} \\right)} \\cos{\\left(x{0} \\right)}}{2} + \\frac{e^{\\sin{\\left(x{0} \\right)}} \\cos^{3}{\\left(x{0} \\right)}}{6} - \\frac{e^{\\sin{\\left(x{0} \\right)}} \\cos{\\left(x{0} \\right)}}{6}\\right) + O\\left(\\left(x - x{0}\\right)^{4}; x\\rightarrow x_{0}\\right)$ 有限差分 到目前为止我们分别用分析导数和原始函数来研究表达式. 但是如果我们想要一个表达式来估计曲线的导数,如果我们缺少一个闭合形式表示,或者我们还不知道函数值那又怎么办呢? 一种方法是使用有限差分法. SymPy中提供了接口as_finite_difference()可以在任何微分实例上来生成任意阶导数的近似: from sympy import Function,finite_diff_weights,apply_finite_diff f = Function('f') dfdx = Derivative(f(x))#.diff(x) dfdx $\\displaystyle \\frac{d}{d x} f{\\left(x \\right)}$ dfdx.as_finite_difference() $\\displaystyle - f{\\left(x - \\frac{1}{2} \\right)} + f{\\left(x + \\frac{1}{2} \\right)}$ 这里我们使用步长为1,等距离估计的最小点数来近似函数对x的一阶导数.我们可以使用任意的步长 f = Function('f') d2fdx2 = Derivative(f(x),x, 2) d2fdx2 $\\displaystyle \\frac{d^{2}}{d x^{2}} f{\\left(x \\right)}$ h = symbols('h') d2fdx2.as_finite_difference([-3*h,-h,2*h]) $\\displaystyle \\frac{f{\\left(- 3 h \\right)}}{5 h^{2}} - \\frac{f{\\left(- h \\right)}}{3 h^{2}} + \\frac{2 f{\\left(2 h \\right)}}{15 h^{2}}$ 如果要评估权重,你可以手动计算 finite_diff_weights(2, [-3, -1, 2], 0)[-1][-1] $\\displaystyle \\left[ \\frac{1}{5}, \\ - \\frac{1}{3}, \\ \\frac{2}{15}\\right]$ 注意,我们只需要从finite_diff_weights返回的最后一个子列表中取最后一个元素.这样做的原因是finite_diff_weights产生较低阶导数的权重,并使用较少的点. 如果使用finite_diff_weights直接看起来很复杂,并且觉得Derivative实例操作的as_finite_difference函数不够灵活,则可以使用apply_finite_diff,它接受order，x_list，y_list和x0作为参数 x_list = [-3, 1, 2] y_list = symbols('a b c') apply_finite_diff(1, x_list, y_list, 0) $\\displaystyle - \\frac{3 a}{20} - \\frac{b}{4} + \\frac{2 c}{5}$ Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/解方程/解方程.html":{"url":"工具链篇/计算工具/符号计算/符号计算/解方程/解方程.html","title":"解方程","keywords":"","body":"解方程 和计算算式不同,解方程是知道结果求变量,作为符号运算工具,sympy在解方程方面非常直观 from sympy import init_printing init_printing(use_unicode=True) from sympy import symbols x, y, z = symbols('x y z') 相等 python中=是赋值,==是判断相等,而在SymPy中使用函数Eq(exp1,exp2)来表示两个算式相等 from sympy import Eq Eq(x,y) x = y 求解方程 求解方程使用solveset(Eq(expr, result), var)来实现.求解方程本质上是求方程的解集,因此其返回值是一个集合,集合计算会在后面介绍 from sympy import solveset solveset(Eq(x**2, 1), x) \\left\\{-1, 1\\right\\} 因为就一个参数所以x可以省略 solveset(Eq(x**2, 1)) \\left\\{-1, 1\\right\\} 如果第一个参数不是等式,那么默认的solveset将会把它作为等于0处理 solveset(expr, var) solveset(x**2-1, x) \\left\\{-1, 1\\right\\} 其实solveset(equation, variable=None, domain=S.Complexes)->Set才是solveset的完整接口,而domain则表示域(取值范围集合),SymPy支持的域包括: S.Naturals表示自然数(或计数数),即从1开始的正整数($ℕ$) S.Naturals0非负整数($ℕ0$) S.Integers整数($Z$) S.Reals实数($R$) S.Complexes复数($C$) S.EmptySet空域($\\emptyset$) from sympy import S,sin,cos,exp solveset(x - x, x, domain=S.Reals) \\mathbb{R} solveset(sin(x) - 1, x, domain=S.Reals) \\left\\{2 n \\pi + \\frac{\\pi}{2}\\; |\\; n \\in \\mathbb{Z}\\right\\} 如果无解,那么结果就是一个空集 solveset(exp(x), x) # No solution exists \\emptyset solveset(cos(x) - x, x) \\left\\{x \\mid x \\in \\mathbb{C} \\wedge - x + \\cos{\\left (x \\right )} = 0 \\right\\} 求方程组 求方程组可以用linsolve(exps...,(vars...))-> Set函数 from sympy import linsolve linsolve([x + y + z - 1, x + y + 2*z - 3 ], (x, y, z)) \\left\\{\\left ( - y - 1, \\quad y, \\quad 2\\right )\\right\\} 当然也可以使用矩阵的方式,矩阵将在下一部分学习 from sympy import Matrix A=Matrix([[1,1,1],[1,1,2]]) b = Matrix([[1],[3]]) linsolve((A,b,),(x,y,z,)) \\left\\{\\left ( - y - 1, \\quad y, \\quad 2\\right )\\right\\} 求解微分方程 dsolve()是微分方程的求解函数 from sympy import Function,dsolve f, g = symbols('f g', cls=Function) f(x) f{\\left (x \\right )} f(x).diff(x) \\frac{d}{d x} f{\\left (x \\right )} diffeq = Eq(f(x).diff(x, x) - 2*f(x).diff(x) + f(x), sin(x)) diffeq f{\\left (x \\right )} - 2 \\frac{d}{d x} f{\\left (x \\right )} + \\frac{d^{2}}{d x^{2}} f{\\left (x \\right )} = \\sin{\\left (x \\right )} dsolve(diffeq, f(x)) f{\\left (x \\right )} = \\left(C_{1} + C_{2} x\\right) e^{x} + \\frac{1}{2} \\cos{\\left (x \\right )} f(x).diff(x)*(1 - sin(f(x))) \\left(- \\sin{\\left (f{\\left (x \\right )} \\right )} + 1\\right) \\frac{d}{d x} f{\\left (x \\right )} dsolve(f(x).diff(x)*(1 - sin(f(x))), f(x)) f{\\left (x \\right )} + \\cos{\\left (f{\\left (x \\right )} \\right )} = C_{1} Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/线性代数/线性代数.html":{"url":"工具链篇/计算工具/符号计算/符号计算/线性代数/线性代数.html","title":"线性代数","keywords":"","body":"线性代数 线性代数是许多学科的基本工具,而SymPy也提供了对它的支持.主要包括: 矩阵运算 张量运算 from sympy import init_printing init_printing(use_unicode=True) 矩阵运算 矩阵运算已经是现在科学计算的最常见形式了,算法向量化,利用gpu进行高效的矩阵运算也是现今单机最高效的计算方式,sympy中有类型Matrix用来定义矩阵 from sympy import Matrix Matrix([[1, -1], [3, 4], [0, 2]]) $\\displaystyle \\left[\\begin{matrix}1 & -1\\3 & 4\\0 & 2\\end{matrix}\\right]$ 单列矩阵默认是竖置的(注意于numpy不同) Matrix([1, 2, 3]) $\\displaystyle \\left[\\begin{matrix}1\\2\\3\\end{matrix}\\right]$ 内置矩阵构造函数 单位矩阵 eye(n) from sympy import eye eye(4) $\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\\end{matrix}\\right]$ 全零矩阵 zeros(m,n) from sympy import zeros zeros(2,3) $\\displaystyle \\left[\\begin{matrix}0 & 0 & 0\\0 & 0 & 0\\end{matrix}\\right]$ 全1矩阵 ones(m,n) from sympy import ones ones(3, 2) $\\displaystyle \\left[\\begin{matrix}1 & 1\\1 & 1\\1 & 1\\end{matrix}\\right]$ 对角矩阵diag(eles...) from sympy import diag diag(1, 2, 3) $\\displaystyle \\left[\\begin{matrix}1 & 0 & 0\\0 & 2 & 0\\0 & 0 & 3\\end{matrix}\\right]$ 结合上面的构造函数,我们可以构造比较复杂的矩阵 diag(-1, ones(2, 2), Matrix([5, 7, 5])) $\\displaystyle \\left[\\begin{matrix}-1 & 0 & 0 & 0\\0 & 1 & 1 & 0\\0 & 1 & 1 & 0\\0 & 0 & 0 & 5\\0 & 0 & 0 & 7\\0 & 0 & 0 & 5\\end{matrix}\\right]$ 矩阵的基本属性和操作 M = Matrix([[1, 2, 3], [3, 2, 1]]) N = Matrix([0, 1, 1]) 查看矩阵的shape M.shape $\\displaystyle \\left( 2, \\ 3\\right)$ 查看矩阵的秩 M.rank() $\\displaystyle 2$ 获取单独一行(一列) M.row(0) $\\displaystyle \\left[\\begin{matrix}1 & 2 & 3\\end{matrix}\\right]$ M.col(-1) $\\displaystyle \\left[\\begin{matrix}3\\1\\end{matrix}\\right]$ 矩阵切片 矩阵切片规则为[纵轴,横轴] M[:,:2] $\\displaystyle \\left[\\begin{matrix}1 & 2\\3 & 2\\end{matrix}\\right]$ 需要注意的是matrix是可以修改的,可以用row_del或col_del方法删行删列, row_insert或col_insert添加行或者列,不过最好别这么用. 矩阵运算 矩阵加法 对应项相加 M = Matrix([[1, 3], [-2, 3]]) N = Matrix([[0, 3], [0, 7]]) M+N $\\displaystyle \\left[\\begin{matrix}1 & 6\\-2 & 10\\end{matrix}\\right]$ 矩阵乘常数 3*M $\\displaystyle \\left[\\begin{matrix}3 & 9\\-6 & 9\\end{matrix}\\right]$ 矩阵乘向量 向量使用只有一列的矩阵表示 Matrix([[1, 2, 3], [3, 2, 1]]) * Matrix([0, 1, 1]) $\\displaystyle \\left[\\begin{matrix}5\\3\\end{matrix}\\right]$ 矩阵乘法 M*N $\\displaystyle \\left[\\begin{matrix}0 & 24\\0 & 15\\end{matrix}\\right]$ 矩阵的幂 M**2 $\\displaystyle \\left[\\begin{matrix}-5 & 12\\-8 & 3\\end{matrix}\\right]$ M**-1 $\\displaystyle \\left[\\begin{matrix}\\frac{1}{3} & - \\frac{1}{3}\\\\frac{2}{9} & \\frac{1}{9}\\end{matrix}\\right]$ 矩阵转置 M = Matrix([[1, 2, 3], [4, 5, 6]]) M $\\displaystyle \\left[\\begin{matrix}1 & 2 & 3\\4 & 5 & 6\\end{matrix}\\right]$ M.T $\\displaystyle \\left[\\begin{matrix}1 & 4\\2 & 5\\3 & 6\\end{matrix}\\right]$ 求解行列式 M = Matrix([[1, 0, 1], [2, -1, 3], [4, 3, 2]]) M $\\displaystyle \\left[\\begin{matrix}1 & 0 & 1\\2 & -1 & 3\\4 & 3 & 2\\end{matrix}\\right]$ M.det() $\\displaystyle -1$ rref()方法找出向量组的最大无关组 通过初等行变换,找出向量组的最大无关组,对矩阵操作,转化为最简形矩阵. M = Matrix([[1, 0, 1, 3], [2, 3, 4, 7], [-1, -3, -3, -4]]) M $\\displaystyle \\left[\\begin{matrix}1 & 0 & 1 & 3\\2 & 3 & 4 & 7\\-1 & -3 & -3 & -4\\end{matrix}\\right]$ M.rref() $\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 1 & 3\\0 & 1 & \\frac{2}{3} & \\frac{1}{3}\\0 & 0 & 0 & 0\\end{matrix}\\right], \\ \\left( 0, \\ 1\\right)\\right)$ 计算零空间 定义：已知$A$为一个$m\\times n$矩阵.的零空间(nullspace),又称核(kernel),是一组由下列公式定义的$n$维向量： ker(A) = {x \\in C^n : Ax = 0} 即线性方程组$AX=0$的所有解x的集合。 M = Matrix([[1, 2, 3, 0, 0], [4, 10, 0, 0, 1]]) M $\\displaystyle \\left[\\begin{matrix}1 & 2 & 3 & 0 & 0\\4 & 10 & 0 & 0 & 1\\end{matrix}\\right]$ M.nullspace() $\\displaystyle \\left[ \\left[\\begin{matrix}-15\\6\\1\\0\\0\\end{matrix}\\right], \\ \\left[\\begin{matrix}0\\0\\0\\1\\0\\end{matrix}\\right], \\ \\left[\\begin{matrix}1\\- \\frac{1}{2}\\0\\0\\1\\end{matrix}\\right]\\right]$ Columnspace列空间 列空间是指一个由矩阵的所有的列进行线性组合而形成的空间.sympy中使用columnspace方法计算,结果返回构成该空间的基向量 M = Matrix([[1, 1, 2], [2 ,1 , 3], [3 , 1, 4]]) M $\\displaystyle \\left[\\begin{matrix}1 & 1 & 2\\2 & 1 & 3\\3 & 1 & 4\\end{matrix}\\right]$ M.columnspace() $\\displaystyle \\left[ \\left[\\begin{matrix}1\\2\\3\\end{matrix}\\right], \\ \\left[\\begin{matrix}1\\1\\1\\end{matrix}\\right]\\right]$ 特征值eigenvals,特征向量eigenvects和对角化diagonalize M = Matrix([[3, -2, 4, -2], [5, 3, -3, -2], [5, -2, 2, -2], [5, -2, -3, 3]]) M $\\displaystyle \\left[\\begin{matrix}3 & -2 & 4 & -2\\5 & 3 & -3 & -2\\5 & -2 & 2 & -2\\5 & -2 & -3 & 3\\end{matrix}\\right]$ M.eigenvals() $\\displaystyle \\left{ -2 : 1, \\ 3 : 1, \\ 5 : 2\\right}$ M.eigenvects() $\\displaystyle \\left[ \\left( -2, \\ 1, \\ \\left[ \\left[\\begin{matrix}0\\1\\1\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 3, \\ 1, \\ \\left[ \\left[\\begin{matrix}1\\1\\1\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 5, \\ 2, \\ \\left[ \\left[\\begin{matrix}1\\1\\1\\0\\end{matrix}\\right], \\ \\left[\\begin{matrix}0\\-1\\0\\1\\end{matrix}\\right]\\right]\\right)\\right]$ P, D = M.diagonalize() P $\\displaystyle \\left[\\begin{matrix}0 & 1 & 1 & 0\\1 & 1 & 1 & -1\\1 & 1 & 1 & 0\\1 & 1 & 0 & 1\\end{matrix}\\right]$ D $\\displaystyle \\left[\\begin{matrix}-2 & 0 & 0 & 0\\0 & 3 & 0 & 0\\0 & 0 & 5 & 0\\0 & 0 & 0 & 5\\end{matrix}\\right]$ P*D*P**-1 $\\displaystyle \\left[\\begin{matrix}3 & -2 & 4 & -2\\5 & 3 & -3 & -2\\5 & -2 & 2 & -2\\5 & -2 & -3 & 3\\end{matrix}\\right]$ P*D*P**-1 == M True 张量计算 张量是向量的扩展,在工程上由很广泛的应用,在机器学习领域,随着深度学习的流行,张量计算也被大家广泛使用.通常标量,矢量,矩阵都是特殊的张量,0维的张量是标量,一维的张量是向量,二维的张量是矩阵. SymPy中支持张量计算,它的最基本结构是N维数组(ARRAY) 数组操作 from sympy import Array a1 = Array([[1, 2], [3, 4], [5, 6]]) a1 $\\displaystyle \\left[\\begin{matrix}1 & 2\\3 & 4\\5 & 6\\end{matrix}\\right]$ 数组和矩阵一样,有属性shape有rank a1.shape $\\displaystyle \\left( 3, \\ 2\\right)$ a1.rank() $\\displaystyle 2$ 同时,数组允许使用reshape操作改变其形状,其原理是按行拉平然后在重新切分 a1.reshape(2,3) $\\displaystyle \\left[\\begin{matrix}1 & 2 & 3\\4 & 5 & 6\\end{matrix}\\right]$ 广播操作 数组中的元素可以是数字,也可以是符号或者表达式,如果是表达式,那么对整个数组的操作会广播到所有元素上 from sympy.abc import x, y, z m3 = Array([x**3, x*y, z]) m3 $\\displaystyle \\left[\\begin{matrix}x^{3} & x y & z\\end{matrix}\\right]$ m3.diff(x) $\\displaystyle \\left[\\begin{matrix}3 x^{2} & y & 0\\end{matrix}\\right]$ 我们也可以使用接口applyfunc()来自己定义要广播的函数 m3.applyfunc(lambda x: x/2) $\\displaystyle \\left[\\begin{matrix}\\frac{x^{3}}{2} & \\frac{x y}{2} & \\frac{z}{2}\\end{matrix}\\right]$ 当rank为2时我们可以使用接口.tomatrix()将数组对象转化为矩阵 a1.tomatrix() $\\displaystyle \\left[\\begin{matrix}1 & 2\\3 & 4\\5 & 6\\end{matrix}\\right]$ 张量乘法 张量乘法$ P=A⊗B $定义为: P_{i_1,\\ldots,i_n,j_1,\\ldots,j_m} := A_{i_1,\\ldots,i_n}\\cdot B_{j_1,\\ldots,j_m} 它使用接口tensorproduct(*arrays)来实现, 两个rank为1的张量相乘构造出一个rank为2的张量 from sympy import Array, tensorproduct from sympy.abc import x,y,z,t A = Array([x, y, z, t]) B = Array([1, 2, 3, 4]) C = tensorproduct(A, B) C $\\displaystyle \\left[\\begin{matrix}x & 2 x & 3 x & 4 x\\y & 2 y & 3 y & 4 y\\z & 2 z & 3 z & 4 z\\t & 2 t & 3 t & 4 t\\end{matrix}\\right]$ C.rank() $\\displaystyle 2$ rank为1的张量和矩阵之间的乘积产生rank为3的张量 p1 = tensorproduct(A, eye(4)) p1 $\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}x & 0 & 0 & 0\\0 & x & 0 & 0\\0 & 0 & x & 0\\0 & 0 & 0 & x\\end{matrix}\\right] & \\left[\\begin{matrix}y & 0 & 0 & 0\\0 & y & 0 & 0\\0 & 0 & y & 0\\0 & 0 & 0 & y\\end{matrix}\\right] & \\left[\\begin{matrix}z & 0 & 0 & 0\\0 & z & 0 & 0\\0 & 0 & z & 0\\0 & 0 & 0 & z\\end{matrix}\\right] & \\left[\\begin{matrix}t & 0 & 0 & 0\\0 & t & 0 & 0\\0 & 0 & t & 0\\0 & 0 & 0 & t\\end{matrix}\\right]\\end{matrix}\\right]$ 张量的切片 p1[0,:,:] $\\displaystyle \\left[\\begin{matrix}x & 0 & 0 & 0\\0 & x & 0 & 0\\0 & 0 & x & 0\\0 & 0 & 0 & x\\end{matrix}\\right]$ 张量收缩 张量收缩指在指定轴上求和,可以使用接口tensorcontraction,比如矩阵(2维张量)做收缩 A_{m,n} \\implies \\sum_k A_{k,k} from sympy import tensorcontraction C = Array([[x, y], [z, t]]) C $\\displaystyle \\left[\\begin{matrix}x & y\\z & t\\end{matrix}\\right]$ tensorcontraction(C, (0, 1)) $\\displaystyle t + x$ 矩阵的积相当于两个rank为2的张量求张量积后在第二和第三轴做收缩 A_{m,n}\\cdot B_{i,j} \\implies \\sum_k A_{m, k}\\cdot B_{k, j} C = Array([[x, y], [z, t]]) C $\\displaystyle \\left[\\begin{matrix}x & y\\z & t\\end{matrix}\\right]$ D = Array([[2, 1], [0, -1]]) D $\\displaystyle \\left[\\begin{matrix}2 & 1\\0 & -1\\end{matrix}\\right]$ M_1 = tensorproduct(C, D) M_1 $\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}2 x & x\\0 & - x\\end{matrix}\\right] & \\left[\\begin{matrix}2 y & y\\0 & - y\\end{matrix}\\right]\\\\left[\\begin{matrix}2 z & z\\0 & - z\\end{matrix}\\right] & \\left[\\begin{matrix}2 t & t\\0 & - t\\end{matrix}\\right]\\end{matrix}\\right]$ tensorcontraction(M_1,(1, 2)) $\\displaystyle \\left[\\begin{matrix}2 x & x - y\\2 z & - t + z\\end{matrix}\\right]$ C.tomatrix()*D.tomatrix() $\\displaystyle \\left[\\begin{matrix}2 x & x - y\\2 z & - t + z\\end{matrix}\\right]$ 在数组上求导 如果数组上的元素都可导,那么我们也可以用derive_by_array(func,array)把求导操作广播到数组上 from sympy import derive_by_array from sympy.abc import x, y, z, t from sympy import sin, exp derive_by_array(sin(x*y), x) $\\displaystyle y \\cos{\\left(x y \\right)}$ derive_by_array(sin(x*y), [x, y, z]) $\\displaystyle \\left[\\begin{matrix}y \\cos{\\left(x y \\right)} & x \\cos{\\left(x y \\right)} & 0\\end{matrix}\\right]$ 以函数组成的数组为基础求导: B^{nm} := \\frac{\\partial A^m}{\\partial x^n} basis = [x, y, z] ax = derive_by_array([exp(x), sin(y*z), t], basis) ax $\\displaystyle \\left[\\begin{matrix}e^{x} & 0 & 0\\0 & z \\cos{\\left(y z \\right)} & 0\\0 & y \\cos{\\left(y z \\right)} & 0\\end{matrix}\\right]$ 再对结果做收缩 tensorcontraction(ax, (0, 1)) $\\displaystyle z \\cos{\\left(y z \\right)} + e^{x}$ Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/概率统计/概率统计.html":{"url":"工具链篇/计算工具/符号计算/符号计算/概率统计/概率统计.html","title":"概率统计","keywords":"","body":"概率统计 SymPy也支持概率统计的符号运算.它需要导入子模块sympy.stats from sympy import init_printing init_printing(use_unicode=True) 分布的类型 统计模块建立在分布之上,SymPy支持的分布有: 有限分布 分布 对应类 均匀分布 DiscreteUniform 正态分布 Normal 指数分布 Exponential 投硬币概率分布 Coin 二项分布 Binomial 等概分布 Die 伯努利分布 Bernoulli 超几何分布 Hypergeometric Rademacher分布 Rademacher 离散分布 分布 对应类 几何分布 Geometric 泊松分布 Poisson 对数分布 Logarithmic 负二项分布 NegativeBinomial Yule-Simon分布 YuleSimon Zeta分布 Zeta 连续分布 分布 对应类 反正弦分布 Arcsin Benini分布 Benini Beta分布 Beta Beta素数分布 BetaPrime Cauchy分布 Cauchy 卡分布 Chi 去中心卡分布 ChiNoncentral 卡方分布 ChiSquared Dagum分布 Dagum Erlang分布 Erlang 指数分布 Exponential F分布 FDistribution 费舍尔Z分布 FisherZ Frechet分布 Frechet Gamma分布 Gamma 逆Gamma分布 GammaInverse Kumaraswamy分布 Kumaraswamy 拉普拉斯分布 Laplace Logistic分布 Logistic 对数正态分布 LogNormal Maxwell分布 Maxwell Nakagami分布 Nakagami 正态分布 Normal Pareto分布 Pareto 二次U分布 QuadraticU 升余弦分布 RaisedCosine Rayleigh分布 Rayleigh T分布 StudentT 三角分布 Triangular 均匀分布 Uniform Irwin-Hall分布 UniformSum VonMises分布 VonMises Weibull分布 Weibull 维格纳半圆分布 WignerSemicircle 使用这些这些分布类可以实例化出对应分布的随机变量 from sympy.stats import Die,Normal X, Y = Die('X', 6), Die('Y', 6) # 等概分布 Z = Normal('Z', 0, 1) # 正态分布 X $\\displaystyle X$ Y $\\displaystyle Y$ Z $\\displaystyle Z$ 自定义分布 有限分布中有FiniteRV(name, density)类可以自定义概率;连续分布中有ContinuousRV(symbol, density, set=Interval(-oo, oo))类可以自定义随机变量的分布规则.他们都需要一个名字和一个分布密度作为参数. from sympy.stats import FiniteRV density = {0: .1, 1: .2, 2: .3, 3: .4}# 分布概率密度 X_finite = FiniteRV('X_finite', density) X_finite $\\displaystyle X_{finite}$ from sympy.stats import ContinuousRV from sympy import sqrt, exp, pi,Symbol x = Symbol(\"x\") X_continuous = Symbol(\"X_continuous\") pdf = sqrt(2)*exp(-x**2/2)/(2*sqrt(pi)) # 标准正态分布的概率密度函数 pdf $\\displaystyle \\frac{\\sqrt{2} e^{- \\frac{x^{2}}{2}}}{2 \\sqrt{\\pi}}$ X_continuous = ContinuousRV(X_continuous, pdf) X_continuous $\\displaystyle X_{continuous}$ 概率运算 概率统计部分有如下特有的运算表达式可以用于计算不同分布的随机变量组成的表达式或者条件: 表达式 含义 P(condition, given_condition=None, numsamples=None, evaluate=True) 条件为真的概率 E(expr, condition=None, numsamples=None, evaluate=True) 表达式满足条件的期望值 variance(expr, condition=None) 表达式满足条件的方差 covariance(expr_x, expr_y, condition=None) 两个表达式满足条件的协方差 std(expr, condition=None) 表达式满足条件的标准差 density(expr, condition=None, evaluate=True, numsamples=None) 表达式满足条件的概率密度函数 given(expr, condition=None) 表达式满足条件的该条件概率空间上的相同表达式 sample(expr, condition=None) 从表达式生成一个满足条件的样本 sample_iter(expr, condition=None, numsamples=oo) 从表达式生成一个满足条件的样本生成器,默认无限长度 where(condition, given_condition=None) 条件成立的值域 同时随机变量组成的运算也支持使用接口simplify化简 from sympy.stats import P,variance,E,density,sample,where,Probability from sympy import Eq, simplify, Integral P(X>3) $\\displaystyle \\frac{1}{2}$ E(X+Y) $\\displaystyle 7$ variance(X+Y) $\\displaystyle \\frac{35}{6}$ density(X+Y) $\\displaystyle \\left{ 2 : \\frac{1}{36}, \\ 3 : \\frac{1}{18}, \\ 4 : \\frac{1}{12}, \\ 5 : \\frac{1}{9}, \\ 6 : \\frac{5}{36}, \\ 7 : \\frac{1}{6}, \\ 8 : \\frac{5}{36}, \\ 9 : \\frac{1}{9}, \\ 10 : \\frac{1}{12}, \\ 11 : \\frac{1}{18}, \\ 12 : \\frac{1}{36}\\right}$ sample(X+Y) $\\displaystyle 11$ simplify(P(Z>1)) $\\displaystyle \\frac{1}{2} - \\frac{\\operatorname{erf}{\\left(\\frac{\\sqrt{2}}{2} \\right)}}{2}$ Probability类用于实例化随机变量满足条件的概率表达式. P可以看作是Probability类用实例化后求解积分的值 from sympy.stats import P,Probability from sympy import Eq, simplify, Integral Nor = Normal(\"Nor\", 0, 1) P(Nor>1) # X>3的概率 $\\displaystyle \\frac{\\sqrt{2} \\left(- \\sqrt{2} \\sqrt{\\pi} \\operatorname{erf}{\\left(\\frac{\\sqrt{2}}{2} \\right)} + \\sqrt{2} \\sqrt{\\pi}\\right)}{4 \\sqrt{\\pi}}$ prob = Probability(Nor > 1)# X>3的概率表达式 prob $\\displaystyle Probability\\left(Nor > 1\\right)$ prob.rewrite(Integral) # 改写为积分表达式 $\\displaystyle \\int\\limits_{1}^{\\infty} \\frac{\\sqrt{2} e^{- \\frac{z^{2}}{2}}}{2 \\sqrt{\\pi}}\\, dz$ prob.evaluate_integral() # 求解积分 $\\displaystyle \\frac{\\sqrt{2} \\left(- \\sqrt{2} \\sqrt{\\pi} \\operatorname{erf}{\\left(\\frac{\\sqrt{2}}{2} \\right)} + \\sqrt{2} \\sqrt{\\pi}\\right)}{4 \\sqrt{\\pi}}$ Expectation类用于实例化随机变量满足条件的期望表达式. E可以看作是Expectation类实例化期望后求解积分的结果 from sympy.stats import Expectation, Normal, Probability from sympy import symbols, Integral mu = symbols(\"mu\", positive=True) sigma = symbols(\"sigma\", positive=True) Nor_x = Normal(\"Nor_x\", mu, sigma) Expectation(Nor_x) $\\displaystyle Expectation\\left(Nor_{x}\\right)$ Expectation(Nor_x).evaluate_integral().simplify() $\\displaystyle \\mu$ 我们也可以用Probability表达期望 Expectation(Nor_x).rewrite(Probability) $\\displaystyle \\int\\limits{-\\infty}^{\\infty} nor{x} Probability\\left(Nor{x} = nor{x}\\right)\\, dnor_{x}$ 我们也可以使用doit()接口将期望表达式展开 Nor_y = Normal(\"Nor_y\", 0, 1) Expectation(Nor_x+Nor_y).doit() $\\displaystyle Expectation\\left(Nor{x}\\right) + Expectation\\left(Nor{y}\\right)$ Variance类用于实例化随机变量满足条件的方差表达式 variance可以看作是Variance类实例化期望后求解积分的结果 from sympy import symbols, Integral from sympy.stats import Normal, Expectation, Variance, Probability Variance(Nor_x) $\\displaystyle Variance\\left(Nor_{x}\\right)$ Variance(Nor_x).evaluate_integral() $\\displaystyle \\sigma^{2}$ 我们也可以用Probability表达方差 Variance(Nor_x).rewrite(Probability) $\\displaystyle - \\left(\\int\\limits{-\\infty}^{\\infty} nor{x} Probability\\left(Nor{x} = nor{x}\\right)\\, dnor{x}\\right)^{2} + \\int\\limits{-\\infty}^{\\infty} nor{x}^{2} Probability\\left(Nor{x} = nor{x}\\right)\\, dnor{x}$ 我们也可以用𝐸𝑥𝑝𝑒𝑐𝑡𝑎𝑡𝑖𝑜𝑛表达方差 Variance(Nor_x).rewrite(𝐸𝑥𝑝𝑒𝑐𝑡𝑎𝑡𝑖𝑜𝑛) $\\displaystyle - Expectation\\left(Nor{x}\\right)^{2} + Expectation\\left(Nor{x}^{2}\\right)$ 我们也可以使用doit()接口将方差表达式展开 Variance(Nor_x+Nor_y).doit() $\\displaystyle 2 Covariance\\left(Nor{x}, Nor{y}\\right) + Variance\\left(Nor{x}\\right) + Variance\\left(Nor{y}\\right)$ Covariance类用于实例化两个随机变量间的协方差表达式 covariance可以看作是Covariance类实例化期望后求解积分的结果 from sympy.stats import Covariance,covariance,𝐸𝑥𝑝𝑒𝑐𝑡𝑎𝑡𝑖𝑜𝑛 from sympy.stats import Normal X = Normal(\"X\", 3, 2) Y = Normal(\"Y\", 0, 1) Z = Normal(\"Z\", 0, 1) W = Normal(\"W\", 0, 1) cexpr = Covariance(X, Y) cexpr $\\displaystyle Covariance\\left(X, Y\\right)$ cexpr.evaluate_integral() $\\displaystyle 0$ covariance(X,Y) $\\displaystyle 0$ 我们可以用𝐸𝑥𝑝𝑒𝑐𝑡𝑎𝑡𝑖𝑜𝑛表达协方差 cexpr.rewrite(𝐸𝑥𝑝𝑒𝑐𝑡𝑎𝑡𝑖𝑜𝑛) $\\displaystyle Expectation\\left(X Y\\right) - Expectation\\left(X\\right) Expectation\\left(Y\\right)$ 我们也可以使用doit()接口将协方差表达式展开 from sympy.abc import a, b,c,d cexpr.doit() $\\displaystyle Covariance\\left(X, Y\\right)$ Covariance(X, X).doit() $\\displaystyle Variance\\left(X\\right)$ Covariance(a*X, b*X).doit() $\\displaystyle a b Covariance\\left(X, X\\right)$ Covariance(a*X + b*Y, c*Z + d*W).doit() $\\displaystyle a c Covariance\\left(X, Z\\right) + a d Covariance\\left(W, X\\right) + b c Covariance\\left(Y, Z\\right) + b d Covariance\\left(W, Y\\right)$ 概率空间 SymPy中通常是用来验证某个表达式是否与另一表达式在同一个概率空间 from sympy.stats import pspace, Normal from sympy.stats.rv import IndependentProductPSpace X = Normal('X', 0, 1) pspace(2*X + 1) $\\displaystyle SingleContinuousPSpace\\left(X, NormalDistribution\\left(0, 1\\right)\\right)$ X.pspace $\\displaystyle SingleContinuousPSpace\\left(X, NormalDistribution\\left(0, 1\\right)\\right)$ pspace(2*X + 1) == X.pspace True Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/符号计算/布尔代数/布尔代数.html":{"url":"工具链篇/计算工具/符号计算/符号计算/布尔代数/布尔代数.html","title":"布尔代数","keywords":"","body":"布尔代数 布尔代数又叫逻辑演算,是使用数学方法研究逻辑问题的学科,他用等式表示判断,把推理看作等式的变换.这种变换的有效性不依赖人们对符号的解释,只依赖于符号的组合规律.布尔代数究是非常重要的数学分支,电子,计算机等学科都是建立在其之上. SymPy对逻辑演算也有支持,使用的是子模块sympy.logic下. from sympy import init_printing init_printing(use_unicode=True) 基本的布尔运算 布尔代数研究的对象取值范围就是真值true和假值false,基本操作也只有4个:与&(And),或|(Or),非~(Not),symbol对象可以直接使用对应运算符构造表达式 from sympy import symbols x, y = symbols('x,y') y | (x & y) $\\displaystyle y \\vee \\left(x \\wedge y\\right)$ x | y $\\displaystyle x \\vee y$ ~x $\\displaystyle \\neg x$ 要代入值计算布尔代数表达式只要使用.subs接口即可. from sympy.logic import true,false (y & x).subs({x: true, y: true}) $\\displaystyle \\text{True}$ 注意Sympy下的true,false不等同于python中的True,False,要做判断需要使用if.千万不要使用==或is True if true else False True True if false else False False True == true True False == false True True is true False False is false False 扩展的布尔运算 在实际使用中布尔运算常用的运算符往往是上面三个基本运算符的组合,SymPy还支持的运算符包括: 运算符接口 运算符 含义 ^(Xor) 异或 同则false,异则true Nand 与非门 当输入均为true ,则输出为false;若输入中至少有一个为false,则输出为true Nor 或非门 当输入均为false,则输出为true;若输入中至少有一个为true,则输出为false >> (Implies) 蕴含 相当于`！A \\ B, Equivalent 等价 相当于当一个为true时另一个也为true;当一个为false时另一个也为false x ^ y $\\displaystyle x \\veebar y$ from sympy.logic import Nand,Nor,Equivalent Nand(x, y) $\\displaystyle \\neg (x \\wedge y)$ Nor(x, y) $\\displaystyle \\neg (x \\vee y)$ x >> y $\\displaystyle x \\Rightarrow y$ Equivalent(x,y) $\\displaystyle x \\Leftrightarrow y$ ITE 这个方法在sympy.logic.boolalg下,ITE是一个三元运算符,含义就和python中的xxx if xxx else ...一样. from sympy.logic.boolalg import ITE ITE(true | false, true & true, true ^ true) $\\displaystyle \\text{True}$ 合取范式和析取范式 SymPy中可以使用to_cnf(expr, simplify=False)将逻辑表达式转换为合取范式,使用is_cnf(expr)判断表达式是否符合合取范式;使用to_dnf(expr, simplify=False)将逻辑表达式转换为析取范式,使用is_dnf(expr)判断表达式是否符合析取范式. from sympy.abc import A, B, D from sympy.logic.boolalg import to_cnf,is_cnf expr = ~(A | B) | D expr $\\displaystyle D \\vee \\neg (A \\vee B)$ expr_cnf = to_cnf(expr) expr_cnf $\\displaystyle \\left(D \\vee \\neg A\\right) \\wedge \\left(D \\vee \\neg B\\right)$ is_cnf(expr) False is_cnf(expr_cnf) True todo SOP Form和POS Form 形式化简 逻辑推断 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/输出/求值.html":{"url":"工具链篇/计算工具/符号计算/输出/求值.html","title":"求值","keywords":"","body":"求值 符号运算一种输出自然是带入符号求值了,我们推导了半天,很多时候是为了最终计算出数值解的. 求值有3种方式: 求符号的值 带入符号求算式的值 将算式转化为python函数 符号求值 符号求值使用evalf方法,默认的精度是小数点后15位 import sympy as sp sp.pi.evalf() 3.14159265358979 也可以带上精度 sp.pi.evalf(5) 3.1416 sp.sin(2).evalf(3) 0.909 带入符号求值 符号运算一种输出自然是带入符号求值了,我们推导了半天,很多时候是为了最终计算出数值解的. 我们可以使用算式对象的subs方法为自变量符号代入具体数值以求解 x,y,z = sp.symbols(\"x,y,z\") f = x+2*y+z f.subs([(x,1),(y,2),(z,3)]) $\\displaystyle 8$ 将算式转换为python函数 另一种用处是使用函数lambdify(params:Tuple[symbols],exp,package:str)=>Callable把算式转换成python函数,末位参数表示一些用到的运算使用哪个包,支持的有: sympy,自带的运算库 math标准库math numnpy scipy numexpr mpmath,sympy依赖的运算库 tensorflow 默认是: 如果装了scipy就是[\"scipy\", \"numpy\"] 如果装了numpy就是[\"numpy\"] 如果都没装就是[\"math\", \"mpmath\", \"sympy\"] 构造好之后再直接调用这个函数就可以求出数值解. f_python = sp.lambdify((x,y,z),f,'numpy') f_python(x=1,y=2,z=3) 8 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/输出/更加优雅的输出打印结果/更加优雅的输出打印结果.html":{"url":"工具链篇/计算工具/符号计算/输出/更加优雅的输出打印结果/更加优雅的输出打印结果.html","title":"更加优雅的输出打印结果","keywords":"","body":"更加优雅的输出打印结果 sympy支持unicode,可以使用pprint()函数输出优雅的数学公式,通过初始化时传入参数use_latex=True,使用latex作为引擎美化输出,我们以上一个例子中欧拉公式左边的泰勒展开作为例子 import sympy as sp from sympy import init_printing init_printing(use_latex=True) from sympy import pprint from sympy import exp,I#e的幂 from sympy import series#泰勒展开函数 from sympy import symbols x = symbols(\"x\") tmp = series(exp(I*x), x, 0, 10)#公式,变量, from sympy import re,im #获取实部虚部 tmp 1 + i x - \\frac{x^{2}}{2} - \\frac{i x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{i x^{5}}{120} - \\frac{x^{6}}{720} - \\frac{i x^{7}}{5040} + \\frac{x^{8}}{40320} + \\frac{i x^{9}}{362880} + O\\left(x^{10}\\right) re(tmp) - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{8} \\Im{x}}{40320} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{8}}{40320} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{6} \\left(\\Im{x}\\right)^{3}}{4320} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{6} \\left(\\Im{x}\\right)^{2}}{1440} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{6} \\Im{x}}{720} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{6}}{720} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4} \\left(\\Im{x}\\right)^{5}}{2880} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4} \\left(\\Im{x}\\right)^{4}}{576} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4} \\left(\\Im{x}\\right)^{3}}{144} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4} \\left(\\Im{x}\\right)^{2}}{48} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4} \\Im{x}}{24} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{4}}{24} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{7}}{10080} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{6}}{1440} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{5}}{240} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{4}}{48} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{3}}{12} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\left(\\Im{x}\\right)^{2}}{4} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2} \\Im{x}}{2} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{2}}{2} + \\Re{\\left(O\\left(x^{10}\\right)\\right)} - \\frac{\\left(\\Im{x}\\right)^{9}}{362880} + \\frac{\\left(\\Im{x}\\right)^{8}}{40320} - \\frac{\\left(\\Im{x}\\right)^{7}}{5040} + \\frac{\\left(\\Im{x}\\right)^{6}}{720} - \\frac{\\left(\\Im{x}\\right)^{5}}{120} + \\frac{\\left(\\Im{x}\\right)^{4}}{24} - \\frac{\\left(\\Im{x}\\right)^{3}}{6} + \\frac{\\left(\\Im{x}\\right)^{2}}{2} - \\Im{x} + 1 im(tmp) \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{9}}{362880} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{7} \\left(\\Im{x}\\right)^{2}}{10080} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{7} \\Im{x}}{5040} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{7}}{5040} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{5} \\left(\\Im{x}\\right)^{4}}{2880} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{5} \\left(\\Im{x}\\right)^{3}}{720} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{5} \\left(\\Im{x}\\right)^{2}}{240} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{5} \\Im{x}}{120} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{5}}{120} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\left(\\Im{x}\\right)^{6}}{4320} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\left(\\Im{x}\\right)^{5}}{720} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\left(\\Im{x}\\right)^{4}}{144} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\left(\\Im{x}\\right)^{3}}{36} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\left(\\Im{x}\\right)^{2}}{12} + \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3} \\Im{x}}{6} - \\frac{\\left(\\Re{\\left(x\\right)}\\right)^{3}}{6} + \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{8}}{40320} - \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{7}}{5040} + \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{6}}{720} - \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{5}}{120} + \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{4}}{24} - \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{3}}{6} + \\frac{\\Re{\\left(x\\right)} \\left(\\Im{x}\\right)^{2}}{2} - \\Re{\\left(x\\right)} \\Im{x} + \\Re{\\left(x\\right)} + \\Im{O\\left(x^{10}\\right)} pprint(tmp) 2 3 4 5 6 7 8 9 x ⅈ⋅x x ⅈ⋅x x ⅈ⋅x x ⅈ⋅x ⎛ 10⎞ 1 + ⅈ⋅x - ── - ──── + ── + ──── - ─── - ──── + ───── + ────── + O⎝x ⎠ 2 6 24 120 720 5040 40320 362880 输出latex代码 我们也可以将公式用sp.latex(}输出为latex代码 sp.latex(tmp) '1 + i x - \\\\frac{x^{2}}{2} - \\\\frac{i x^{3}}{6} + \\\\frac{x^{4}}{24} + \\\\frac{i x^{5}}{120} - \\\\frac{x^{6}}{720} - \\\\frac{i x^{7}}{5040} + \\\\frac{x^{8}}{40320} + \\\\frac{i x^{9}}{362880} + O\\\\left(x^{10}\\\\right)' Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/符号计算/输出/画图/画图.html":{"url":"工具链篇/计算工具/符号计算/输出/画图/画图.html","title":"画图","keywords":"","body":"画图 sympy可以方便的绘制图形,如果有安装matplotlib,那么它将会使用matplotlib绘制图形 可以通过 plot()函数来实例化一个绘图类,实现绘图他可以带如下参数: adaptive: Boolean. 默认值设置为True。将自适应设置为False，如果需要均匀采样，请指定nb_of_points。 depth: int 自适应算法的递归深度。值n的深度最多取样$2^n$个点。 nb_of_points: int. 当自适应设置为False时使用。 该函数在nb_of_points点数处统一采样。 line_color: float. 指定绘图的颜色。 每个plot对象又有如下参数可以设置 title : str 标题 xlabel : str x轴标签 ylabel : str y轴标签 legend : bool 是否使用图例 xscale : {‘linear’, ‘log’} x轴坐标按线性还是指数扩展 yscale : {‘linear’, ‘log’} y轴坐标按线性还是指数扩展 axis : bool 是否画出坐标轴 axis_center : tuple of two floats or {‘center’, ‘auto’} 坐标轴中心位置 xlim : tuple of two floats ylim : tuple of two floats aspect_ratio : tuple of two floats or {‘auto’} autoscale : bool 是否自动扩展 margin : float in [0, 1] 边的比例 %matplotlib inline 2维图形 画单一函数 plot(expr, range, **kwargs) from sympy import symbols from sympy.plotting import plot x,y = symbols('x y') p1 = plot(x*x) p2 = plot(x) 多图合并 plot对象可以将多张图合并到一起,单独一张合并使用append(),多个可以使用extend p1.append(p2[0]) p1.axis=True p1.show() 参数范围 plot也可以指定参数范围 plot(x**2,(x,-5,5,)) 同时画多个 plot(expr1, expr2, ..., range, **kwargs) plot(x,x**2,x**3,(x,-5,5,)) 多个但不同取值范围 plot((expr1, range), (expr2, range), ..., **kwargs) plot((x**2, (x, -6, 6)), (x, (x, -5, 5))) 自适应采样 plot(x**2, adaptive=False, nb_of_points=400) 设定颜色 sympy的颜色设定有点类似matplotlib的,可以指定特定字符作为标记,比如红色就是\"r\" y = symbols(\"y\") fz = x**2-y**2 plot(x**2,line_color='r') 绘制3d图形 同时,sympy也可以绘制3d图形,与2d图形形式差不多 plot3d(expr, range_x, range_y, **kwargs) 绘制一个算式的图形 plot3d(expr1, expr2, range_x, range_y, **kwargs) 绘制一个取值范围中的多个图形 plot3d((expr1, range_x, range_y), (expr2, range_x, range_y), ..., **kwargs)绘制多取值范围内的多图形 from sympy.plotting import plot3d p3d = plot3d(x**2+y**2) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/":{"url":"工具链篇/计算工具/数值计算/","title":"数值计算","keywords":"","body":"Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用标准库处理基本数学问题/使用标准库处理基本数学问题.html":{"url":"工具链篇/计算工具/数值计算/使用标准库处理基本数学问题/使用标准库处理基本数学问题.html","title":"使用标准库处理基本数学问题","keywords":"","body":"使用标准库处理基本数学问题 python内置了多数情况下足够用的数学工具用于做一些简单计算,主要包括几个部分: 专用的数据类型 包括有标准库中的理数,高精度数类型以及默认支持的复数类型 常用计算算法 包括针对自然数和复数的log,三角函数等计算 统计学工具 包括常用的统计计算算法和随机数模块 默认数的形式 python中的数分为 整数 0 浮点数 1.1 复数,而且大数计算是内置的被优化的相当好. 1+2j 数的进制 默认的数是10进制的,但python中也支持其他几种进制 16进制 0x10 16 8进制 0O10 8 2进制 0b10 2 而十进制转换成别的进制就会麻烦一些,需要使用bin(dec),oct(dec),hex(dec)这样的内置函数 bin(2) '0b10' oct(8) '0o10' hex(16) '0x10' 可以看到转换后得到的其实是字符串,这个需要注意. 大数的表示[3.6] 从python3.6起,大数可以使用每三位加一格下划线的表述方式输入了 12_234 12234 内置的计算操作 python内置的合法计算符号包括: 符号 操作名 + 加 - 减 * 乘 / 除 // 整除 ** 求幂 abs 取绝对值 另外还有一个符号@表示矩阵乘法,这个官方的数据结构没有实现但numpy和pandas倒是实现了 高精度数(decimal) 高精度数模块(decimal)提供了一种可用于代替float的数据类型,这种数据类型并不适合常用计算,因为高精度意味着更耗时;但在需要高精度浮点运算时比较好用,适合用在财务上. 这种数据类型可以由整数,浮点数,数字字符串转化得来 获得当前精度环境 from decimal import getcontext getcontext() Context(prec=28, rounding=ROUND_HALF_EVEN, Emin=-999999, Emax=999999, capitals=1, clamp=0, flags=[], traps=[InvalidOperation, DivisionByZero, Overflow]) 设定精度 getcontext().prec = 10 转化为decimal数据类型 from decimal import Decimal Decimal(1) / Decimal(7) Decimal('0.1428571429') 有理数(fractions) 有理数(fractions)模块提供了一种用来表示有理数的数据类型,它可以用整数,浮点数,高精度数或者数字和除号字符串创建 from fractions import Fraction Fraction(16, -10) Fraction(-8, 5) Fraction(123) Fraction(123, 1) Fraction() Fraction(0, 1) Fraction('3/7') Fraction(3, 7) Fraction('1.414213 \\t\\n') Fraction(1414213, 1000000) Fraction('-.125') Fraction(-1, 8) Fraction('7e-6') Fraction(7, 1000000) Fraction(2.25) Fraction(9, 4) Fraction(1.1) Fraction(2476979795053773, 2251799813685248) from decimal import Decimal Fraction(Decimal('1.1')) Fraction(11, 10) 数学运算模块math python自带的math模块提供了一些常用的数学运算和常数 常数 常数 说明 math.e 自然常数e math.pi 圆周率pi math.tau[Python3.6] 数学常数$τ = 6.283185...$,精确到可用精度.Tau 是一个圆周常数,等于 2π,圆的周长与半径之比 math.inf[Python3.5] 浮点正无穷大,相当于float('inf') math.nan[Python3.5] 浮点\"非数字\"(NaN)值,相当于float('nan') 数值计算 函数 说明 例子 math.ceil(x) 返回大于x的整数上限的浮点数,x为整数则返回自己的浮点形式 math.ceil(1)->1.0,math.ceil(1.1)->2.0,math.ceil(-1.5)->-1.0 `math.copysign(x, y) 返回绝对值为x,符号为y的符号的数 math.copysign(1.0, -0.0)->-1.0 math.fabs(x) 相当于abs(x),返回绝对值 math.fabs(-3.4)->3.4 math.factorial(x) 数学上的x!,阶乘 math.factorial(3)->6 math.floor(x) 与ceil相反,得到上限 math.floor(-0.5)->-1.0 math.fmod(x, y) 求模运算,适合用在浮点数,注意和%的不同 math.fmod(3.5, -2)->1.53.5%-2->-0.5 math.frexp(x) 将x拆成分(m,e),$ x = m \\cdot 2^e $ math.frexp(2.43)->(0.6075, 2) math.fsum(iterable) 求序列中所有数的和的精确值 fsum([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1])->1.0 math.isinf(x) 判断x是不是float(\"inf\") --- math.isfinite(x) 如果 x 既不是无穷大也不是NaN,则返回True,否则返回False math.isclose(a, b, rel_tol=1e-09, abs_tol=0.0)[Python3.5] 用于测试近似相等的函数,rel_tol为相对容差,abs_tol为最小绝对容差 --- math.isnan(x) 判断x是不是float(\"NaN\") --- math.ldexp(m,e) 求$m \\cdot 2^e$ math.ldexp(3, 1)->6.0 math.modf(x) 拆分整数小数部分 math.modf(-3.5)->(-0.5, -3.0) math.trunc(x) 返回整数部分 math.trunc(3.5)->3 math.gcd(a, b)[Python3.5] 返回两个数的最大公约数 math.gcd(12,15)->3 math.remainder(x, y)[Python3.7] 返回IEEE 754风格的x相对于y的余数 --- 平方和对数 函数 说明 例子 math.exp(x) 自然数的幂$e^x$ math.exp(2)->7.38905609893065 math.expm1(x) 返回e的x次方减1 math.expm1(2)->6.38905609893065 math.log(x[, base]) 返回x的以base为底的对数,base默认为e math.log(math.e)->1.0math.log(10,2)->3.3219280948873626 math.log10(x) 返回x的以10为底的对数 math.log10(2)->0.30102999566398114 math.log1p(x) 返回1+x的自然对数（以e为底) math.log1p(math.e-1)->1.0 math.pow(x, y) 返回x的y次方 math.pow(5,3)->125.0 math.sqrt(x) 返回x的平方根 math.sqrt(3)->1.7320508075688772 三角函数 弧度 函数 说明 math.acos(x) acos(x) math.asin(x) asin(x) math.atan(x) atan(x) math.atan2(y, x) atan(y / x) math.cos(x) cos(x) math.hypot(x, y) sqrt(x*x + y*y) math.sin(x) sin(x) math.tan(x) tan(x) 角度,弧度转换 函数 说明 math.degrees(x) 弧度转度 math.radians(x) 度转弧度 双曲函数 函数 说明 math.sinh(x) 双曲正弦 $ \\sinh x = {\\frac {e^x - e^{ - x} } 2} $ math.cosh(x) 双曲余弦 $ \\cosh x = {\\frac {e^x + e^{ - x} } 2} $ math.tanh(x) 双曲正切 $ \\tanh x = {\\frac {\\sinh x} {\\cosh x}} $ math.acosh(x) 反双曲余弦 math.asinh(x) 反双曲正弦 math.atanh(x) 反双曲正切 特殊函数: 函数 说明 math.erf(x) 误差函数: $\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2}\\,\\mathrm dt.$ math.erfc(x) 互补误差函数:$\\operatorname{erfc}(x) = 1-\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_x^{\\infty} e^{-t^2}\\,\\mathrm dt\\,.$ math.gamma(x) 伽玛函数 $\\Gamma(z) = \\int_{0}^{\\infty} \\frac{t^{z-1}}{\\mathrm{e}^t} \\,{\\rm{d}}t$ math.lgamma(x) 伽马函数绝对值的自然对数 import math math.gcd(12,15) 3 复数运算(cmath) 这个模块和math很像,只是面向的操作对象是复数.它独有的接口有 from cmath import phase,polar,rect 极坐标转换 phase()求相(相当于求atan2(x.imag, x.real)) phase(-1.0+0.0j) 3.141592653589793 phase(complex(-1.0,-0.0)) -3.141592653589793 polar(x)转换为极坐标 polar(x) 相当于 (abs(x), phase(x)). polar(complex(-1.0,-0.0)) (1.0, -3.141592653589793) rect(r,phi)已知半径和度数求以两边长为值的复数 r * (math.cos(phi) + math.sin(phi)*1j) from math import pi rect(1,pi/4) (0.7071067811865476+0.7071067811865475j) 统计模块(statistics) 该模块是3.4后新增的模块,这个模块提供一些统计学方法 均值中心性 函数 说明 mean() 均值 harmonic_mean() 计算调和平均,比如求a,b,c三个数的调和平均就是计算$ \\frac3 {(\\frac 1 a + \\frac 1 b + \\frac 1 c)}$ median() 中位数 median_low() Low median of data. median_high() High median of data. median_grouped() Median, or 50th percentile, of grouped data. mode() 众数 Mode (most common value) of discrete data. L = range(10000) from statistics import mean,median,median_low,median_high,median_grouped,mode mean(L) 4999.5 median(L) 4999.5 median_low(L) 4999 median_low(L) 4999 median_high(L) 5000 median_high(L) 5000 median_grouped(L) 4999.5 median_grouped(L, interval=2) 4999.0 from random import randint XL = [randint(1,10) for i in range(10000)] mode(XL) 3 分布统计 函数 说明 pstdev() 总体标准差 pvariance() 总体方差 stdev() 样本标准差 variance() 样本方差 from statistics import pstdev,pvariance,stdev,variance pstdev(L) 2886.751331514372 pvariance(L) 8333333.25 stdev(L) 2886.8956799071675 variance(L) 8334166.666666667 随机模块random 常规用法 无论在做测试中还是在做模拟中,随机都是必须的模块,具体这样用: import random random.random() # [0,1)内随机浮点数 0.25415523691401276 random.uniform(1, 10) # [1,10)内随机浮点数 5.470078932858412 random.randint(1, 10) # [1,10]范围内的随机整数 4 random.randrange(0, 101, 2) # 从等差数列中随机挑一个数 54 random.choice('abcdefghij') # 随机选一个 'e' items = [1, 2, 3, 4, 5, 6, 7] random.shuffle(items)#随机排序 random.sample([1, 2, 3, 4, 5], 3) # 随机选3个元素 [5, 2, 1] 随机种子 学过C的都知道伪随机,python也是伪随机,所以可以通过设定seed值来改变随机状态 from matplotlib import pyplot as plt %matplotlib inline 数学上的一些特殊随机 三角分布 三角分布式是连续概率分布,可以看做是在一个范围中有一个数(众数)它附近有最高的概率密度即最有可能出现在该众数上 random.triangular(low, high, mode)#三角形分布,默认众数(mode)是中值 hight,low,mode = 0,2,0.5 random.triangular(hight,low,mode) 1.1251317155398355 from collections import Counter c = Counter() for nbr in [round(random.triangular(0,2,0.5),2) for i in range(10000)]: c[nbr] = c[nbr] + 1 plt.plot([float(i) for i in c.keys()],[float(i) for i in c.values()],\".\") plt.show() β分布 β分布是在0到1上的特殊分布, 做硬币试验的假定分布，即做伯努利试验的假定分布在确定伯努利试验的分布之前，我们利用试验的少量数据来估计试验的概率（作为先验概率）beta分布涉及两个参数：1、试验成功的次数2、试验失败的次数. 概率密度函数为: \\begin{align} f(x)=& x^{\\alpha-1}(1-x)^{\\beta-1} \\over \\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1}du\\\\ =&{\\frac {\\Gamma(\\alpha+\\beta)} {\\Gamma(\\alpha)\\Gamma(\\beta)}}x^{\\alpha-1}(1-x)^{\\beta-1}\\\\ =&{\\frac {1} {B(\\alpha-\\beta)}}x^{\\alpha-1}(1-x)^{\\beta-1} \\end{align} 累积分布函数: \\begin{align} F(x;\\alpha,\\beta)=& \\frac {B_x(\\alpha,\\beta)} {B(\\alpha,\\beta)}&=I_x(\\alpha,\\beta) \\end{align} 其中{B_x(\\alpha,\\beta)}是不完全Β函数,{I_x(\\alpha,\\beta)} 是正则不完全贝塔函数 是不完全Β函数，I_x (\\alpha,\\beta) 是正则不完全贝塔函数 random.betavariate(alpha, beta)#beta分布 l0=[random.betavariate(0.5,1) for i in range(100000)] l1=[random.betavariate(2,3) for i in range(100000)] l2=[random.betavariate(3,4) for i in range(100000)] l3=[random.betavariate(2,5) for i in range(100000)] r0=[len([1 for j in l0 if i+0.005>j>i-0.005]) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r1=[len([1 for j in l1 if i+0.005>j>i-0.005]) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r2=[len([1 for j in l2 if i+0.005>j>i-0.005]) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r3=[len([1 for j in l3 if i+0.005>j>i-0.005]) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] plt.plot(list(map(lambda x:round(x*0.01,3), range(0,100,1))),r0,color=\"red\") plt.plot(list(map(lambda x:round(x*0.01,3), range(0,100,1))),r1,color=\"blue\") plt.plot(list(map(lambda x:round(x*0.01,3), range(0,100,1))),r2,color=\"green\") plt.plot(list(map(lambda x:round(x*0.01,3), range(0,100,1))),r3,color=\"yellow\") plt.show() 指数分布 概率密度函数: 指数分布可以用来表示独立随机事件发生的时间间隔，比如旅客进机场的时间间隔、中文维基百科新条目出现的时间间隔等等。 许多电子产品的寿命分布一般服从指数分布。有的系统的寿命分布也可用指数分布来近似。它在可靠性研究中是最常用的一种分布形式。指数分布是伽玛分布和威布尔分布的特殊情况，产品的失效是偶然失效时，其寿命服从指数分布。 指数分布可以看作当威布尔分布中的形状系数等于1的特殊分布，指数分布的失效率是与时间t无关的常数，所以分布函数简单。 概率密度函数: f(x)=\\begin{cases} \\lambda e^{-\\lambda x}&x>0,\\\\ 0&x\\le0. \\end{cases} 累积分布函数: F(x;\\lambda)=\\begin{cases} 1- e^{-\\lambda x}&x\\ge0,\\\\ 0&x 期望值: EX=\\lambda^{-1} 方差: D(X)=Var(X)=\\lambda ^{-2} random.expovariate(lambd) random.expovariate(3) 0.00044025063564512545 c_e3 = Counter() for nbr in [round(random.expovariate(3),2 ) for i in range(10000)]: c_e3[nbr] = c_e3[nbr] + 1 plt.plot([float(i) for i in c_e3.keys()], [float(i) for i in c_e3.values()],\".\") plt.show() 伽玛分布 概率密度函数: 令 X\\sim \\Gamma(\\alpha,\\beta) ; 则有: f(x)={\\frac {x^{(\\alpha-1)} e^{(-\\lambda x)}} {\\Gamma(\\alpha)\\beta^\\alpha}} ,x>0 random.gammavariate(alpha, beta) c_gamma = Counter() for nbr in [round(random.gammavariate(5,0.5),2 ) for i in range(10000)]: c_gamma [nbr] = c_gamma [nbr] + 1 plt.plot([float(i) for i in c_gamma.keys()], [float(i) for i in c_gamma.values()],\".\") plt.show() 高斯分布(正态分布) 概率密度函数: f(x)={\\frac 1 {\\sqrt {2\\pi}\\sigma}} e^{-{\\frac {(x-\\mu)^2} {2\\sigma^2}}} 其中 \\mu 与 \\sigma 分别是变量对数的平均值与标准差 random.gauss(mu, sigma)#略快于下面的方法 random.normalvariate(mu, sigma) c_gauss = Counter() for nbr in [round(random.gauss(0,1),2 ) for i in range(10000)]: c_gauss[nbr] = c_gauss[nbr] + 1 plt.plot([float(i) for i in c_gauss.keys()], [float(i) for i in c_gauss.values()],\".\") plt.show() 对数正态分布 如果 X 是正态分布的随机变量，则 exp(X) 为对数正态分布；同样，如果 Y 是对数正态分布，则 ln(Y) 为正态分布。 如果一个变量可以看作是许多很小独立因子的乘积，则这个变量可以看作是对数正态分布。 概率密度函数: f(x;\\mu,\\sigma) = \\frac{1}{ \\sigma \\sqrt{2 \\pi}} e^{-(\\ln x - \\mu)^2/2\\sigma^2} 其中 \\mu 与 \\sigma 分别是变量对数的平均值与标准差 random.lognormvariate(mu, sigma) c_log = Counter() for nbr in [round(random.lognormvariate(0.5,0.5),2 ) for i in range(10000)]: c_log[nbr] = c_log[nbr] + 1 plt.plot([float(i) for i in c_log.keys()],[float(i) for i in c_log.values()],\".\") plt.show() 冯·米塞斯分布 冯·米塞斯分布（von Mises distribution）指一种圆上连续概率分布模型，它也被称作循环正态分布 概率密度函数: f(x|\\mu,\\kappa)=\\frac{e^{\\kappa\\cos(x-\\mu)}}{2\\pi I_0(\\kappa)} 参数μ和1/κ是μ和σ^2（对应正态分布中的均值和方差）的模拟量 μ是位置的度量（分布将围绕μ成簇） κ是集中度的度量（分散度的倒数，所以1/κ是σ^2的模拟量） 如果κ为0，分布是均匀分布，对于κ很小的情形，分布近似均匀分布 如果κ很大，分布紧紧围绕μ集中分布。实际上，随着κ增加，分布将趋于x以μ为均值1/κ为方差的正态分布 random.vonmisesvariate(mu, kappa) c_von = Counter() for nbr in [round(random.vonmisesvariate(10,1),2 ) for i in range(10000)]: c_von[nbr] = c_von[nbr] + 1 plt.plot([float(i) for i in c_von.keys()], [float(i) for i in c_von.values()],\".\") plt.show() 帕累托分布 帕累托法则(2,8定律) 帕累托分布是以意大利经济学家维弗雷多·帕雷托命名的。 是从大量真实世界的现象中发现的幂定律分布。这个分布在经济学以外，也被称为布拉德福分布。 概率密度函数: p(x) = \\left \\{ \\begin{matrix} 0, & x x_{\\min}. \\end{matrix} \\right. k是形状参数(shape parameter) random.paretovariate(k) c_par = Counter() for nbr in [round(random.paretovariate(10),2 ) for i in range(10000)]: c_par[nbr] = c_par[nbr] + 1 plt.plot([float(i) for i in c_par.keys()], [float(i) for i in c_par.values()],\".\") plt.show() 韦伯分布 韦伯分布（Weibull distribution），又称韦氏分布或威布尔分布，是可靠性分析和寿命检验的理论基础。 概率密度函数: f(x;\\lambda,k) = \\begin{cases} \\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{k-1}e^{-(x/\\lambda)^{k}} & x\\geq0\\\\ 0 & x λ＞0是比例参数（scale parameter），k＞0是形状参数（shape parameter） random.weibullvariate(lambda, k) c_wei = Counter() for nbr in [round(random.weibullvariate(2,1),2 ) for i in range(10000)]: c_wei[nbr] = c_wei[nbr] + 1 plt.plot([float(i) for i in c_wei.keys()], [float(i) for i in c_wei.values()],\".\") plt.show() Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/","title":"使用numpy_scipy处理复杂的数值计算问题","keywords":"","body":"使用numpy和scipy处理复杂的数值计算问题 numpy虽然没在标准库但其地位恐怕高于标准库.它是事实上python做科学计算的根基,许多科学计算工具要么依赖它要么接口定义向其靠拢.而scipy则是numpy的一个补充算法包. 科学计算主要需要的是完成代数方程,矩阵,微分,积分,微分方程,统计,方程求解等方面计算的能力.Python语言本身并不满足所有需求,但是NumPy和SciPy作为python语言很好的补充可以完成这些需求,同时numpy和scipy都是用C和Fortran实现的,因此性能很强,这让python在科学计算领域可以和matlab有抗衡的能力. NumPy NumPy是Python科学计算的基础程序包,它的主要贡献是 提供了多维数组这一高效的数据结构 提供了高性能的universal function可以将函数广播到数组的每个元素上 提供了矩阵运算,张量运算等高性能线性代数计算接口 Scipy SciPy是对NumPy的功能扩展,它提供了许多高级数学函数,例如微分,积分,微分方程,优化方法,数值分析,高级统计函数,方程式求解等功能,SciPy是在NumPy数组框架的基础上实现的,它对NumPy数组和基本的数组运算进行扩展,满足科学家和工程师解决问题时需要用到的大部分数学计算功能. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/numpy的高性能同构定长多维数组/numpy的高性能同构定长多维数组.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/numpy的高性能同构定长多维数组/numpy的高性能同构定长多维数组.html","title":"numpy的高性能同构定长多维数组","keywords":"","body":"numpy的高性能同构定长数组 numpy最大的意义在于它构建了这个高性能同构定长多维数组数组类型numpy.ndarray它是由C写的数据类型因此具有非常强的性能.同时 ndarray实现了Collection,Container和Iterable协议.因此python的序列操作也都可以使用. numpy.ndarray是几乎所有python科学计算工具的基石,包括pandas也是在其上构建的. import numpy as np ndarray数组的基本操作 ndarray本质上是一段连续内存.它支持基本的构造,切片,访问,和修改操作,并且也支持合并操作. 构建ndarray数组 numpy的数组需要由序列构建而来.通常我们使用构造函数numpy.array(obj:sequence,dtype:str,copy:bool=True,order:str,subok:bool=False,ndmin:int) 在数学上我们定义一个如下的矩阵: \\left [ \\begin{matrix} 0 & 1 &2\\\\ 3 & 4 &5 \\end{matrix} \\right ] 我们以它为例子介绍数组的构建 obj是用于转化的序列对象,要求必须是同一种数据类型 X = np.array([[0,1,2],[1,0,3]]) print(X) [[0 1 2] [1 0 3]] dtype可以用于指定数据类型 可用的数据类型下面\"元素的数据类型\"部分会列出,如果不指定则会自动使用可以存放数据的最小内存的数据类型. numpy可以特化其中元素的类型来获得更高的效率 类型 类型代码 说明 int8/uint8 i1/u1 有符号/无符号8位整型 int16/uint16 i2/u2 有符号/无符号16位整型 int32/uint32 i4/u4 有符号/无符号32位整型 int64/uint64 i8/u8 有符号/无符号64位整型 float16 f2 半精度浮点数 float32 f4或f 标准单精度浮点数 float64 f8或d 标准双精度浮点数 float128 f16或g 扩展精度浮点数 complex64 c8 32为浮点数表示的复数 complex128 c16 64为浮点数表示的复数 complex256 c32 128为浮点数表示的复数 bool ? 布尔值 object O python对象类型 string_ SX 固定长度字符串,比如长度为10,则S10 unicode_ UX 固定长度unicode,比如长度为10,则U10 X = np.array([[0,1,2],[1,0,3]],dtype=\"f2\") print(X) [[0. 1. 2.] [1. 0. 3.]] X.dtype dtype('float16') 结构体数组 在numpy中可以定义结构数组来表现结构化数据,虽然这个功能现在有pandas这个更好的实现,但如果只是轻量级的使用,numpy的结构数组或许更有效率 构造结构体使用np.dtype(obj,align:bool=False,copy:bool=True)类. 要自己构建结构体,其中obj需为一个字典,包含两个字段: names 用于指明字段名 formats 用于指明字段的类型 同时指明align=True用于指明数据要对齐(通过补齐让每个字段自动与最长的字段长度一致). copy则意味着类型是新对象还是只是原来标准类型的映射. persontype=np.dtype({'names':['name','age','weight'],'formats':['S32','i','f']},align=True)#先创建一个人物类型 People = np.array([ [(\"Huang\",27,75),(\"Hao\",25,55),(\"Li\",26,80)], [(\"Hu\",28,65),(\"Hua\",23,75),(\"Liu\",26,85)] ],dtype=persontype) print(People) [[(b'Huang', 27, 75.) (b'Hao', 25, 55.) (b'Li', 26, 80.)] [(b'Hu', 28, 65.) (b'Hua', 23, 75.) (b'Liu', 26, 85.)]] 访问某个数据的字段可以使用[]运算符 People[0,0]['name'] b'Huang' 也可以直接以字段为索引直接获取切片或全局的字段数组 People[0,:]['name']# 取行 array([b'Huang', b'Hao', b'Li'], dtype='|S32') People[:,0]['name']# 取列 array([b'Huang', b'Hu'], dtype='|S32') People['name'] # 全局 array([[b'Huang', b'Hao', b'Li'], [b'Hu', b'Hua', b'Liu']], dtype='|S32') copy,默认值为True,如果为True,构造出的数组为新建的,否则如果参数obj也是ndarray数组,则只是修改这个数组. X = np.array([[0,1,2],[1,0,3]],dtype=\"f2\") X_copy = np.array(X,copy=True) X is X_copy False X_self = np.array(X,copy=False) X is X_self True order用于标明下标的表现方式,可选的参数有'K', 'A', 'C', 'F',默认为\"K\" 在数学上我们定义一个如下的矩阵: \\left [ \\begin{matrix} 0 & 1 &2\\\\ 3 & 4 &5 \\end{matrix} \\right ] 在C语言中,中其存储方式是以行为主轴的,也就是会这么存. 0,1,2,3,4,5 而在Fortran中会以列为主轴存储,也就是这样 0,3,1,4,2,5 内部存储的样子比较难以观察,我们可以通过reshape来查看 X = np.array([[0,1,2],[3,4,5]],order=\"C\") print(X) X.reshape(1,6,order=\"C\") [[0 1 2] [3 4 5]] array([[0, 1, 2, 3, 4, 5]]) Y = np.array([[0,1,2],[3,4,5]],order=\"F\") print(Y) Y.reshape(1,6,order='F') [[0 1 2] [3 4 5]] array([[0, 3, 1, 4, 2, 5]]) 如果参数指明为\"C\",则表示使用C语言的规范存储,如果使用参数\"F\"则表示使用Fortran的规范存储. \"K\"和\"A\"在参数copy为False时表示继承obj数组的存储顺序,但copy为True时则表现不同,\"K\"在obj为数组时继承其顺序,非数组则会按C的格式存储.而\"A\"会在obj为数组时如果它的order为\"F\",\"A\",\"K\"时都是Fortran的存储顺序,否则为C的存储方式. subok如果为True,则子类将被传递;否则返回的数组将被强制为基类数组,False为默认. ndmin指定维度 这个参数更多的是用于将向量,矩阵等转化为更高维的张量,同样的功能也可以用reshape实现 X = np.array([[0,1,2],[3,4,5]],ndmin=2) print(X) [[0 1 2] [3 4 5]] Y = np.array([[0,1,2],[3,4,5]],ndmin=3) print(Y) [[[0 1 2] [3 4 5]]] Z = np.array([[0,1,2],[3,4,5]],ndmin=3) print(Z) [[[0 1 2] [3 4 5]]] 快速构建数组 numpy提供了很多方法快速构建一些特殊形式的数组 固定形状固定值的数组 全0数组np.zeros(shape) np.zeros((2,2)) array([[0., 0.], [0., 0.]]) 全1数组np.ones(shape) np.ones((2,2)) array([[1., 1.], [1., 1.]]) 空数组(只分配内存空间不赋值,也就是说里面可能是无意义数据)np.empty(shape) np.empty((2,2)) array([[1., 1.], [1., 1.]]) 特殊一维数组 一维等差数列数组np.arange(dow,up,step) np.arange(1,11,2) array([1, 3, 5, 7, 9]) 一维均分数组(可以看做等差数列的一种)np.linspace(start, stop, num=50, endpoint=True, retstep=False)->sample 其中endpoint如果为True表示stop为最后一个数. 如果retstep为True,则返回值为(sample,step),其中step就是均分的间隔 np.linspace(-np.pi, np.pi, 6,endpoint=True) array([-3.14159265, -1.88495559, -0.62831853, 0.62831853, 1.88495559, 3.14159265]) np.linspace(-np.pi, np.pi, 6,endpoint=True,retstep=True) (array([-3.14159265, -1.88495559, -0.62831853, 0.62831853, 1.88495559, 3.14159265]), 1.2566370614359172) 一维等比数列数组np.logspace(start, stop, num=num, endpoint=endpoint,base = base) 其中base参数可以用来固定底,这样就相当于做乘方了 np.logspace(0.1, 1, 10) array([ 1.25892541, 1.58489319, 1.99526231, 2.51188643, 3.16227766, 3.98107171, 5.01187234, 6.30957344, 7.94328235, 10. ]) np.logspace(0.1, 1, 10,base = 2) array([1.07177346, 1.14869835, 1.23114441, 1.31950791, 1.41421356, 1.51571657, 1.62450479, 1.74110113, 1.86606598, 2. ]) 特殊二维数组 单位矩阵数组np.eye(n) np.eye(3) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 对角矩阵数组np.diag(obj) 这个obj为一维序列时将会产生一个以其值为对角线其他都是0的对角矩阵;如果如一个方正则会取其对角线组成一个数组 np.diag((1,2,3)) array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) np.diag( ( (1,2,3), (4,5,6), (7,8,9) ) ) array([1, 5, 9]) 构造随机数组 numpy有自己的随机数生成器,它可以作为标准库的补充,其接口基本和标准库的一致,用它可以方便的构造固定shape的随机数组. 像标准库random一样,numpy也可以设置随机种子np.random.seed(number) 洗牌 我们可以以一个一维序列作为输入,生成一个元素随机顺序的数组有两种方式 使用np.random.permutation生成一个新的一维序列 就地洗牌使用np.random.shuffle,改变原对象的排序 import matplotlib.pyplot as plt %matplotlib inline np.random.permutation([1,2,3,4,5,6]) array([1, 6, 5, 4, 2, 3]) a = np.array([1,2,3,4,5,6]) np.random.shuffle(a) a array([6, 2, 4, 1, 3, 5]) 均匀分布 numpy有3个均匀分布的函数 np.random.rand(d0,d1,...,dn)0~1返回内的均匀分布 np.random.uniform(low=0.0, high=1.0, size=None)low到high范围内的均匀分布 np.random.randint(low, high=None, size=None, dtype='l')low到high范围内整数的均匀分布 np.random.rand(2,3) array([[0.69799255, 0.7888051 , 0.66893797], [0.01018505, 0.93295118, 0.57403557]]) np.random.uniform(2,3,size=(3,4)) array([[2.27883779, 2.10539064, 2.6946143 , 2.99850912], [2.381377 , 2.33687032, 2.31932192, 2.96783378], [2.46179302, 2.72258605, 2.4427342 , 2.20615523]]) np.random.randint(1,9,size=(3,3)) array([[6, 7, 4], [4, 8, 1], [4, 3, 3]]) l=np.random.rand(1000) r1=[len(list(filter(lambda x:i+0.005>x>i-0.005,l))) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] plt.plot(list(map(lambda x:round(x*0.01,3),range(0,100,1))),r1) plt.show() 正态分布 正态分布又叫高斯分布,numpy中有两个方法实现: np.random.normal(loc=0.0, scale=1.0,size=None)loc为均值,scale为标准差size则为形状 np.random.randn(d0, d1, ..., dn)标准正态分布 np.random.normal(1,0.5,size=(3,3)) array([[1.14928514, 0.98874561, 1.08238708], [1.16838246, 1.08098035, 0.83320946], [2.2213291 , 1.10766759, 0.61062045]]) np.random.randn(3,3) array([[-0.74651304, -0.52009124, 0.10955316], [ 0.32689031, -1.44277601, 0.53329921], [ 1.21113617, -0.01221963, 0.88827929]]) l=np.random.randn(100000) r1=[len(list(filter(lambda x:i+0.05>x>i-0.05,l))) for i in map(lambda x:round(x*0.1,2),range(-40,40,1))] plt.plot(list(map(lambda x:round(x*0.1,2),range(-40,40,1))),r1) plt.show() 二项分布 二项分布是n个独立的是/非试验中成功的次数的离散概率分布,其中每次试验的成功概率为p.这样的单次成功/失败试验又称为伯努利试验.实际上当n = 1时,二项分布就是伯努利分布.二项分布是显著性差异的二项试验的基础. Poisson分布是二项分布n很大而P很小时的特殊形式,是两分类资料在n次实验中发生x次某种结果的概率分布.其概率密度函数为: P(x)=e-\\mu*\\frac {\\mu x} {x!} x=0,1,2...n 其中e为自然对数的底,$\\mu$为总体均数,x为事件发生的阳性数. 二项分布在numpy中使用np.random.binomial(n, p, size=None),n为重复次数,p为True的概率,size为形状 sum(np.random.binomial(10,0.1,1000)==0)/1000.0# 10个样本成功率为0.1,验证1000次全部都失败的概率 0.363 Beta分布 使用函数np.random.beta(a, b, size=None)a为分布的参数$\\alpha$,b为参数$\\beta$ np.random.beta(0.5,0.3,[2,3]) array([[0.96751372, 0.89294397, 0.01424346], [0.32563002, 0.99996309, 0.69713176]]) l0=np.random.beta(0.5,1,100000) l1=np.random.beta(2,3,100000) l2=np.random.beta(3,4,100000) l3=np.random.beta(2,5,100000) r0=[len(list(filter(lambda x:i+0.005>x>i-0.005,l0))) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r1=[len(list(filter(lambda x:i+0.005>x>i-0.005,l1))) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r2=[len(list(filter(lambda x:i+0.005>x>i-0.005,l2))) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] r3=[len(list(filter(lambda x:i+0.005>x>i-0.005,l3))) for i in map(lambda x:round(x*0.01,3),range(0,100,1))] plt.plot(list(map(lambda x:round(x*0.01,3),range(0,100,1))),r0,color=\"red\") plt.plot(list(map(lambda x:round(x*0.01,3),range(0,100,1))),r1,color=\"blue\") plt.plot(list(map(lambda x:round(x*0.01,3),range(0,100,1))),r2,color=\"green\") plt.plot(list(map(lambda x:round(x*0.01,3),range(0,100,1))),r3,color=\"yellow\") plt.show() 卡方分布 若k个随机变量$ Z_1、……、Z_k $是相互独立,符合标准正态分布的随机变量(数学期望为0,方差为1),则随机变量Z的平方和 X=\\sum_{i=1}^k Z_i^2 被称为服从自由度为k的卡方分布,记作 X\\sim\\chi^2(k) X\\sim\\chi^2_k numpy中使用接口np.random.chisquare(df, size=None)其中df为分布的参数,自由度$df$ np.random.chisquare(2,(2,3)) array([[1.2339406 , 2.84964228, 3.60035737], [0.33187465, 3.02932834, 4.39024623]]) 伽马分布 伽马分布使用接口np.random.gamma(shape, scale=1.0, size=None)shape表示形状参数$\\alpha$;scale为尺度参数$\\beta$ np.random.gamma(1,2,(2,3)) array([[3.17820824, 2.53817032, 1.49755265], [0.24098586, 1.61370319, 0.48898025]]) l0=np.random.gamma(0.5,1,100000) l1=np.random.gamma(9,0.5,100000) l2=np.random.gamma(7,1,100000) r0=[len(list(filter(lambda x:i+0.05>x>i-0.05,l0))) for i in map(lambda x:round(x*0.1,3),range(0,200,1))] r1=[len(list(filter(lambda x:i+0.05>x>i-0.05,l1))) for i in map(lambda x:round(x*0.1,3),range(0,200,1))] r2=[len(list(filter(lambda x:i+0.05>x>i-0.05,l2))) for i in map(lambda x:round(x*0.1,3),range(0,200,1))] plt.plot(list(map(lambda x:round(x*0.1,3),range(0,200,1))),r0,color=\"red\") plt.plot(list(map(lambda x:round(x*0.1,3),range(0,200,1))),r1,color=\"blue\") plt.plot(list(map(lambda x:round(x*0.1,3),range(0,200,1))),r2,color=\"green\") plt.show() 根据下标自定义数组构造过程 有的时候我们希望可以自定义的生成一个矩阵,比如我们希望生成一个各项值等于i+10j的矩阵(i为行号,j为列号)这时候可以使用np.fromfunction(func,shape,dtype) 其中的参数func的参数个数为shape的位数,而参数的值则是下标 np.fromfunction(lambda i,j: i+10*j,(3,4)) array([[ 0., 10., 20., 30.], [ 1., 11., 21., 31.], [ 2., 12., 22., 32.]]) 网格数组 np.ogrid可以用于快速构建用于构建网格的数组.他的参数是切片表达式,每有一个切片就会生成一个网格的一根轴,因此他的返回数据个数与参数个数一致. x,y = np.ogrid[:5,:10:2] print(x) print(y) [[0] [1] [2] [3] [4]] [[0 2 4 6 8]] 可以看到这两个数组一个行一个是列,且每位对称,这样做出来的数组就可以直接进行矩阵乘法了 x@y array([[ 0, 0, 0, 0, 0], [ 0, 2, 4, 6, 8], [ 0, 4, 8, 12, 16], [ 0, 6, 12, 18, 24], [ 0, 8, 16, 24, 32]]) 利用ogrid的返回值可以很容易的计算出二元函数在等间距网格上的值 例:画出 f(x,y)=xe^{x^2-y^2} x,y = np.ogrid[-2:2:20j,-2:2:20j] z = x*np.exp(-x**2-y**2) from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = Axes3D(fig) ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap='hot') plt.show() ndarray对象的常用属性 属性 含义 T 转置,与self.transpose()相同,如果维度小于2返回self size 数组中元素个数 itemsize 数组中单个元素的字节长度 dtype 数组元素的数据类型对象 ndim 数组的维度 shape 数组的形状 data 指向存放数组数据的memoryview对象 flat 返回数组的一维迭代器 imag 返回数组的虚部 real 返回数组的实部 nbytes 数组中所有元素的字节长度 ndarray也支持一些方法,这些方法与numpy对应函数重名.这边就不做详细介绍了 访问ndarray中的元素 访问某个元素可以像访问多维list对象中元素一样操作 X array([[0, 1, 2], [3, 4, 5]]) X[0][1] 1 也可以使用切片的方式,以,分隔每维来指定位置 X[0,1] 1 ndarray数组的切片 数字切片的规则和python中list一致, 以,分隔每维来指定位置. 以:代表范围,如果:之前或者之后缺省,则分别代表最初一位和最后一位.正整数数代表从头数到这位,复数代表从末位倒着数到这位 ,分隔的每一位都可以以一个list指定取某几位 X[:,1] array([1, 4]) X[:,:-1] array([[0, 1], [3, 4]]) X[:,[0,-1]] array([[0, 2], [3, 5]]) 按下表提取元素 numpy中是用np.take(a, indices, axis=None, out=None, mode='raise')从数组总提取对应的下标指示的元素构建一个新数组 x = np.arange(10) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np.take(x,[1,2,3]) array([1, 2, 3]) 按条件查找下标对应的元素 numpy支持按条件查找下标对应的元素,使用的接口是np.choose(a, choices, out=None, mode='raise') 其中a为指定的下标组成的序列,choices为可被选择的内容, choices = [ [0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33] ] np.choose([2, 3, 1, 0], choices) array([20, 31, 12, 3]) 可以看到最后的结果是每列选择对应下标的值 按条件查找元素下标 numpy支持按条件查找元素所在下标,使用的接口是np.where,它有两种用法: np.where(condition, [x, y])来按条件查找元素并执行赋值. 其逻辑是:满足条件(condition),输出x,不满足输出y.condition是判断条件. aa = np.arange(10) np.where(aa,1,-1) # 0值也是False array([-1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) np.where(aa > 5,1,-1) # condition为一个bool值序列 array([-1, -1, -1, -1, -1, -1, 1, 1, 1, 1]) np.where( [ [True,False], [True,True] ], # 官网上的例子 [ [1,2], [3,4] ], [ [9,8], [7,6] ] ) ## 多维数组则是看对应下标位置判断取值 array([[1, 8], [3, 4]]) np.where(condition)返回满足条件的下标,即等价于condition.nonzero() np.where(aa>5) (array([6, 7, 8, 9]),) np.where(np.array([[0,1], [1,0]]) > 0) (array([0, 1]), array([1, 0])) 使用where实现三角波 T=1#定义周期T为1 x = np.linspace(0, 2,201,endpoint=True) C = 0.7#定义为0的部分 up = 0.5#定义上升的持续时间 top = 1.0#定义最大y值 #y=Kx+B K_up = lambda : top/up K_down = lambda : top/(up-C) B_down = top-K_down()*up y = np.where(np.modf(x)[0] >= C ,0,np.where(np.modf(x)[0] 按条件条件查找元素 numpy支持按条件查找元素,使用的是接口np.select(condlist, choicelist, default=0). 当满足多个条件时，将使用condlist中遇到的第一个条件.位置m处的输出是choicelist中数组的第m个元素，其中condlist中相应数组的第m个元素为True。 x = np.arange(10) x array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) condlist = [x5] condlist [array([ True, True, True, False, False, False, False, False, False, False]), array([False, False, False, False, False, False, True, True, True, True])] choicelist = [x, x**2] choicelist [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81])] np.select(condlist, choicelist) array([ 0, 1, 2, 0, 0, 0, 36, 49, 64, 81]) 使用select实现三角波 T=1#定义周期T为1 x = np.linspace(0, 2,201,endpoint=True) C = 0.7#定义为0的部分 up = 0.5#定义上升的持续时间 top = 1.0#定义最大y值 #y=Kx+B K_up = lambda : top/up K_down = lambda : top/(up-C) B_down = top-K_down()*up y1 = np.select([np.modf(x)[0] >= C,np.modf(x)[0] 数组分段执行函数 np.piecewise(x, condlist, funclist, *args, **kw)可以按condlist将x拆成几段,每段分别执行funclist中对应的函数 使用piecewise实现三角波 y2 = np.piecewise( x, [np.modf(x)[0] >= C,np.modf(x)[0] 数组拼接 二维数组拼接分为横向拼接和纵向拼接 np.hstack(tup)横向拼接 np.vstack(tup)纵向拼接 其中tup为要拼接的对象组成的序列 更加通用的则是指定要拼接的轴np.concatenate(tup,axis=0,out=None)其中用axis指定以第几维为轴,out如果设置则必须为符合拼接后形状,元素类型的数组对象,这样拼接的结果会放入这个数组. A=np.arange(1,13).reshape(3,4) print(A) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] B=np.arange(2,14).reshape(3,4) print(B) [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] np.hstack((A,B)) array([[ 1, 2, 3, 4, 2, 3, 4, 5], [ 5, 6, 7, 8, 6, 7, 8, 9], [ 9, 10, 11, 12, 10, 11, 12, 13]]) np.vstack((A,B)) array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [ 2, 3, 4, 5], [ 6, 7, 8, 9], [10, 11, 12, 13]]) np.concatenate((A,B),axis=0) array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [ 2, 3, 4, 5], [ 6, 7, 8, 9], [10, 11, 12, 13]]) np.concatenate((A,B),axis=1) array([[ 1, 2, 3, 4, 2, 3, 4, 5], [ 5, 6, 7, 8, 6, 7, 8, 9], [ 9, 10, 11, 12, 10, 11, 12, 13]]) 序列操作 作为一个序列,numpy也提供了很多序列专有操作,这些操作看起来有点函数式编程的意思.习惯这种思维的可以很方便的写出简洁的代码 真值判断 numpy使用any(a,axis=None)和all(a,axis=None)进行真值判断.,同时可以指定轴. x = np.array([0,1,2,3,4,5,6]) np.all(x) False np.any(x) True 迭代器 numpy使用np.nditer将数组转化为迭代器.它支持python的迭代器协议,可以使用next获取下一个元素.order参数可以直接影响它的迭代顺序. x = np.array([1,2,3,4,5,6]).reshape(2,3) print(x) x_i = np.nditer(x,order='C') [[1 2 3] [4 5 6]] for i in x_i: print(i) 1 2 3 4 5 6 x_j = np.nditer(x,order='F') for i in x_j: print(i) 1 4 2 5 3 6 排序 np.lexsort(keys, axis=-1) 使用一系列键执行间接排序,keys为要排序的复数序列.axis为以谁为轴,返回的是排序后的下标位置 X = np.array([ [9,4,0,4,0,2,1],# b [1,5,1,4,3,4,4] # a ]) # b在前，a在后，即是先按照a的元素进行比较 # 如a中的最小值为两个1，其索引分别为0,2，再计较b中相应索引上的值，即9,0 # 对应的最小应是：1,0，而其对应的索引为2，所以排序后返回的结果第一个值为索引2 # 下一个最小应是：1,9，而其对应的索引为0，所以排序后返回的结果第一个值为索引0 # 以此类推... ind = np.lexsort(X) # Sort by a, then by b print(ind) [2 0 4 6 5 3 1] np.argsort(a, axis=-1, kind='quicksort', order=None) 排序并返回下标 a ：所需排序的数组 axis：数组排序时的基准，axis=0，按行排列；axis=1，按列排列 kind：数组排序时使用的方法，其中:kind=′quicksort′为快排；kind=′mergesort′为混排；kind=′heapsort′为堆排； order：一个字符串或列表，可以设置按照某个属性进行排序 a=np.array([4,2,5,7,3]) b=np.argsort(a) print(b) # 列表b的元素表示的是原列表a中的元素的索引，5各元素的索引分别为0-4 # 返回的结果可以这样解读： # b[0]=1，表示原列表a的最小元素的索引为1，即原列表a中的第2个元素为最小值 # b[1]=4，表示原列表a的第二小元素的索引为4，即原列表a中的第5个元素为第二小元素 # ... # b[4]=3，表示原列表a中的最大元素的索引为3，即原列表a中的第4个元素为最大值 [1 4 0 2 3] c=np.array([ [3, 2], [5, 7] ]) d = np.argsort(c, axis=1) print(d) # axis=1，表明按照列进行排序，即是对[3, 2]进行排序，所以得到索引为[1, 0],其他同理 [[1 0] [0 1]] e = np.argsort(c, axis=0) print(e) # axis=0，表明按照行进行排序，即是对[3, 5]进行排序，所以得到索引为[0, 1],其他同理 [[0 0] [1 1]] np.sort(a, axis=-1, kind='quicksort', order=None) 这个接口与标准排序接口sorted类似功能 a ：所需排序的数组 axis：数组排序时的基准，axis=0，按行排列；axis=1，按列排列 kind：数组排序时使用的方法，其中:kind=′quicksort′为快排；kind=′mergesort′为混排；kind=′heapsort′为堆排； order：一个字符串或列表，可以设置按照某个属性进行排序,有点类似标准接口的key dtype = [('name', 'S10'), ('height', float), ('age', int)] values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),('Galahad', 1.7, 38)] a = np.array(values, dtype=dtype) print(a) np.sort(a, order='height') [(b'Arthur', 1.8, 41) (b'Lancelot', 1.9, 38) (b'Galahad', 1.7, 38)] array([(b'Galahad', 1.7, 38), (b'Arthur', 1.8, 41), (b'Lancelot', 1.9, 38)], dtype=[('name', 'S10'), ('height', ' np.searchsorted(a, v, side='left', sorter=None) 排序后插入v时插入的索引位置 a：所需排序的数组 v：待查询索引的元素值 side：查询索引时的方向，其中:kind=′left′为从左至右；kind=′right′为从右至左 sorder：一个字符串或列表，可以设置按照某个属性进行排序 list3=[1,2,3,4,5] np.searchsorted(list3,2) # 如若要在list3中插入元素2，则应当将其插在原列表索引为1的地方，即是插在元素1的后面 1 np.searchsorted(list3,[-5,7,4,9]) # 如若要在list3中插入元素-5，则应当将其插在原列表索引为0的地方，即是插在元素1的前面 # 其他以此类推... array([0, 5, 3, 5]) np.partition(a, kth, axis=-1, kind='introselect', order=None) a:所需排序的数组 kth:以排序后第几个数为分界 axis: 以哪一维为轴 side:查询索引时的方向，其中:kind=′left′为从左至右；kind=′right′为从右至左 sorder:一个字符串或列表，可以设置按照某个属性进行排序 list4=[3,4,5,2,1] np.partition(list4,3) # 以排序后的第3个数，即3进行分区，分区后的结果即是： # 小于3的元素2,1位于3的前面，大于等于3的元素4,5位于3的后面 array([2, 1, 3, 4, 5]) 累积 其基本模式是延一条轴用一个函数计算到当前位的累积 求沿给定轴的元素的累积积np.cumprod(a, axis=None, dtype=None, out=None) a = np.array([1,2,3,4]) np.cumprod(a) # 结果每位是到这一位前各个的积 1, 1*2, 1*2*3 ,1*2*3*4 array([ 1, 2, 6, 24]) a = np.array([ [1, 2, 3], [4, 5, 6] ]) np.cumprod(a, axis=0) # 求各个列的累积积 # [ 1, 2, 3], # [ 1*4, 2*5, 3*6]] array([[ 1, 2, 3], [ 4, 10, 18]]) np.cumprod(a,axis=1) # 求各行的累积积 # [ 1, 1*2, 1*2*3], # [ 4, 4*5, 4*5*6] array([[ 1, 2, 6], [ 4, 20, 120]]) 沿给定轴的元素的累积和np.cumsum(a, axis=None, dtype=None, out=None) a = np.array([ [1, 2, 3], [4, 5, 6] ]) np.cumsum(a, axis=0) # 求各个列的累积和 # [ 1, 2, 3], # [ 1+4, 2+5, 3+6]] array([[1, 2, 3], [5, 7, 9]]) np.cumsum(a,axis=1) # 求各行的累积积 # [ 1, 1+2, 1+2+3], # [ 4, 4+5, 4+5+6] array([[ 1, 3, 6], [ 4, 9, 15]]) 差分diff 求沿给定轴axis的第n个离散差分diff(a, n=1, axis=-1)(常用在时间序列) x = np.array([1, 2, 4, 7, 0]) np.diff(x) # [2-1,4-2,7-4,0-7] array([ 1, 2, 3, -7]) x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]]) np.diff(x) # [[3-1,6-3,10-6], # [5-0,6-5,8-6]] array([[2, 3, 4], [5, 1, 2]]) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/universal_function/universal_function.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/universal_function/universal_function.html","title":"universal_function","keywords":"","body":"universal function 除了通用的序列操作外,还支持ufunc ufunc是universal function的简写,它是一种对数组中每个元素做相同操作的函数,概念上类似原生python的map,但在实际的运算中又不同. 原生map实际上是运行迭代器一个一个操作,而universal function则是向量化的执行函数,即一个函数不同的数据一起运行,这样就大大提高了效率. 我们可以实际测试下python自带的迭代和universal function的性能差距 import numpy as np import matplotlib.pyplot as plt %matplotlib inline test = np.arange(int(1e5)) print(test) [ 0 1 2 ... 99997 99998 99999] %timeit -n 3 map(lambda x:x**2,test) 517 ns ± 303 ns per loop (mean ± std. dev. of 7 runs, 3 loops each) %timeit -n 3 test**2 The slowest run took 29.67 times longer than the fastest. This could mean that an intermediate result is being cached. 548 µs ± 967 µs per loop (mean ± std. dev. of 7 runs, 3 loops each) 广播 可见universal function的高效.universal function的另一个特性是两个ndarray对象可以对应项计算,这一特性被称作广播 当俩数组形状不同的时候,那就会进行广播处理 让所有数组向其中维数最多的数组看齐,shape不足的部分通过在前面加1补齐 输出数组的shape属性是输入数组shape属性在各轴上的最大值 如果输入数组的某个轴长度是1或输出数组对应数组对应轴的长度相同,这个数组就够用来计算,否则出错 当输入数组的某个轴长度为1时,沿着该轴运算时都用此轴上的额第一组值 看例子: 相同shape a = np.arange(12).reshape(3,4) a array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) b = np.arange(3,15).reshape(3,4) b array([[ 3, 4, 5, 6], [ 7, 8, 9, 10], [11, 12, 13, 14]]) a+b array([[ 3, 5, 7, 9], [11, 13, 15, 17], [19, 21, 23, 25]]) 不同shape在对应轴上广播 a = np.arange(0,60,10).reshape(-1,1) print(a) a.shape [[ 0] [10] [20] [30] [40] [50]] (6, 1) b = np.arange(0,5) print(b) b.shape [0 1 2 3 4] (5,) c = a+b print(c) c.shape [[ 0 1 2 3 4] [10 11 12 13 14] [20 21 22 23 24] [30 31 32 33 34] [40 41 42 43 44] [50 51 52 53 54]] (6, 5) 与常量做四则运算 ndarray对象支持python支持的四则运算接口,包括幂运算,而且都是universal function X = np.arange(10).reshape(2,5) print(X) [[0 1 2 3 4] [5 6 7 8 9]] X+2 array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11]]) X-2 array([[-2, -1, 0, 1, 2], [ 3, 4, 5, 6, 7]]) X*2 array([[ 0, 2, 4, 6, 8], [10, 12, 14, 16, 18]]) X/2 array([[0. , 0.5, 1. , 1.5, 2. ], [2.5, 3. , 3.5, 4. , 4.5]]) X**2 array([[ 0, 1, 4, 9, 16], [25, 36, 49, 64, 81]]) Y = np.arange(2,12).reshape(2,5) Y array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11]]) X+Y array([[ 2, 4, 6, 8, 10], [12, 14, 16, 18, 20]]) X-Y array([[-2, -2, -2, -2, -2], [-2, -2, -2, -2, -2]]) X*Y array([[ 0, 3, 8, 15, 24], [35, 48, 63, 80, 99]]) X/Y array([[0. , 0.33333333, 0.5 , 0.6 , 0.66666667], [0.71428571, 0.75 , 0.77777778, 0.8 , 0.81818182]]) X**Y array([[ 0, 1, 16, 243, 4096], [ 78125, 1679616, 40353607, 1073741824, 31381059609]]) 比较运算 python的比较操作也都是universal function,只是返回的结果是bool值 X>5 array([[False, False, False, False, False], [False, True, True, True, True]]) X array([[ True, True, True, True, True], [False, False, False, False, False]]) X==5 array([[False, False, False, False, False], [ True, False, False, False, False]]) X>=5 array([[False, False, False, False, False], [ True, True, True, True, True]]) X array([[ True, True, True, True, True], [ True, False, False, False, False]]) 2 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) in ----> 1 2其他的函数 numpy中还定义了其他的常用universal function具体可以看官方https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs 自定义ufunc 有的时候自带的ufunc不能满足需要,numpy允许自定义ufunc 使用接口np.frompyfunc(func, nin, nout) 例:用一个分段函数描述三角波 def triangle_wave(x,c,c0,hc): \"\"\"三角波\"\"\" x = x - int(x) if x >= c: r = 0.0 elif x x = np.linspace(0,2,1000) y1 = np.array([triangle_wave(t,0.6,0.4,1.0) for t in x]) triangl_ufunc1 = np.frompyfunc(triangle_wave,4,1) y2 = triangl_ufunc1(x,0.6,0.4,1.0) plt.plot(range(len(y2)),y2) plt.show() 使用接口np.vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False, signature=None)直接将python函数转化为ufunc triangl_ufunc2 = np.vectorize(triangle_wave) triangl_ufunc2.__doc__ '三角波' y3 = triangl_ufunc2(x,0.6,0.4,1.0) plt.plot(range(len(y3)),y3) plt.show() ufunc的泛函 简单的可以理解为函数的函数就是泛函.numpy支持针对ufunc的泛函操作,具体的有如下几种: at(a, indices, b=None) at用于指定下标执行函数,未被指定的就不会执行函数,注意其返回值为None,但它会改变第一个参数数组中对应元素的值. a = np.array([1, 2, 3, 4]) np.negative.at(a, [0, 1]) print(a) [-1 -2 3 4] a = np.array([1, 2, 3, 4]) np.add.at(a,[0,1],1) print(a) [2 3 3 4] reduce reduce和原生python中的reduce差不多,就是rfold,折叠操作的特化 np.add.reduce([1,2,3]) # 相当于sum 6 np.add.reduce([[1,2,3],[6,7,8]],axis=1) #可以指定延哪条轴折叠 array([ 6, 21]) accumulate accumulate和python3中functiontools新增的累积函数accumulate一样 np.add.accumulate([1,2,3]) array([1, 3, 6]) outer outer,会对俩数组中每两对元素组合进行运算. np.add.outer([1,2,3,4,5],[2,3,4]) array([[3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9]]) reduceat(a, indices, axis=0, dtype=None, out=None) 指定范围执行reduce操作.相当于 ufunc.reduce(a[indices[i]:indices[i+1]]). 特殊情况是-- 1. 如果i+1x=np.arange(8) print(x) [0 1 2 3 4 5 6 7] x.shape (8,) np.add.reduceat(x,[0,4, 1,5, 2,6, 3,7]) array([ 6, 4, 10, 5, 14, 6, 18, 7]) 在多维的情况下,可以用axis指定延哪条轴计算 x = np.linspace(0, 15, 16).reshape(4,4) print(x) x.shape [[ 0. 1. 2. 3.] [ 4. 5. 6. 7.] [ 8. 9. 10. 11.] [12. 13. 14. 15.]] (4, 4) np.add.reduceat(x, [0, 3, 1, 2, 0]) array([[12., 15., 18., 21.], [12., 13., 14., 15.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [24., 28., 32., 36.]]) 指定轴执行函数 ufunc会在所有的元素上进行执行,但很多时候我们只希望指定轴执行函数.这时候就可以使用如下接口 np.apply_along_axis(func1d, axis, arr, *args, **kwargs) 沿给定轴对一维切片应用函数,函数的第一个参数必须为一维数组.其他参数可以在后面放入 def my_func(a,n): return (a[0] + a[-1]) * n b = np.array([[1,2,3], [4,5,6], [7,8,9]]) np.apply_along_axis(my_func, 0, b,n=0.25) array([2. , 2.5, 3. ]) np.apply_over_axes(func, a, axes) 在多个轴上重复应用一个函数,函数的值为res = func(a, axis) a = np.arange(24).reshape(2,3,4) print(a) print(a.shape) [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] (2, 3, 4) np.apply_over_axes(np.sum, a, [0,2]) array([[[ 60], [ 92], [124]]]) np.sum(a,axis=0) array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/精度与基本数学运算.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/精度与基本数学运算.html","title":"精度与基本数学运算","keywords":"","body":"精度与基本数学运算 精度 科学计算的一个核心点在于精度,numpy的默认精度是64位的浮点数.最高的精度可以用numpy.longdouble(f16)表示，复数用numpy.clongdouble(c32)，但是这个具体能精确到多少是和你的电脑硬件相关的. 通常我们并不需要最高精度,这时可以使用numpy.around(a, decimals=0, out=None)来控制精度,其中a为要固定精度的数组,decimals则是精确到小数点后多少位(四舍五入). import numpy as np x = np.linspace(-np.pi, np.pi, 6,endpoint=True) print(x) [-3.14159265 -1.88495559 -0.62831853 0.62831853 1.88495559 3.14159265] np.round(x) array([-3., -2., -1., 1., 2., 3.]) np.round(x,1) array([-3.1, -1.9, -0.6, 0.6, 1.9, 3.1]) np.round(x,2) array([-3.14, -1.88, -0.63, 0.63, 1.88, 3.14]) 基本数学运算 numpy中也有常用数学运算的实现,由于做了足够的优化性能比python标准库高很多 一元运算 函数 说明 abs/fabs 绝对值 sqrt 平方根 square 平方 exp 指数 log/log10/log2/log1p 分别为自然对数(e为底数)/底数为10的log/底数为2的log/log(1+x) sign 求符号 ceil 大于等于该值的最小整数 floor 小于等于该值的最大整数 rint 四舍五入到最近的整数,dtype不变 modf 小数整数部分分离 isnan --- isfinite/isinf --- sin/sinh/cos/cosh/tan/tanh 三角函数,双曲三角函数 arcsin/arcsinh/arccos/arccosh/arctan/arctanh 反三角函数,反双曲三角函数 logical_not 计算各元素not x的真值 二元运算 函数 说明 add 加 subtract 减 multiply 乘 divide/floor_divide 除 power 乘方 maximun/fmax 最大值 minimum/fmin 最小值 mod 求模 copysign 将后面的符号付给前面 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/多项式计算/多项式计算.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/多项式计算/多项式计算.html","title":"多项式计算","keywords":"","body":"多项式计算 多项式函数是变量的整数次幂与系数的乘积之和 f(x)=a_nx^n+a_{n-1}x^{n-1}+...+a_2x^2+a_1x+a_0 在numpy中可以用一个一维数组表示x各项的系数 numpy提供了polynomial模块专门处理多项式的 基本用法 多项式求值 可以用polynomia.polynomia()将系数转化为一元多项式对象,之后就可以像用函数一样用它了,比如下面的函数: f(x)=1+2x+3x^2 import matplotlib.pyplot as plt %matplotlib inline import numpy as np from numpy.polynomial import Polynomial as P v1 = np.array([1,2,3]) p = P(v1) p $x \\mapsto \\text{1.0} + \\text{2.0}\\,x + \\text{3.0}\\,x^{2}$ 注意,长版本的打印输出有三个部分. 第一个是系数,第二个是域,第三个是窗口:他们分别可以通过访问属性p.coef,p.domain和p.window获得 要求x在某个值时f(x)的值,只要简单的代入就行 p(0) 1.0 p(1) 6.0 多项式是天生的universal function,他的参数可以是一个序列 p(np.array([1,2,3,4])) array([ 6., 17., 34., 57.]) 多项式运算 初等变换 一个多项式可以通过与一个非字符串的数值序列相加或者乘以一个标量来获得一个新的多项式 p*2 #与标量相乘,多项式系数全部与标量相乘,类似向量与标量乘法 $x \\mapsto \\text{2.0} + \\text{4.0}\\,x + \\text{6.0}\\,x^{2}$ p+p # 与多项式相加,对应系数相加 $x \\mapsto \\text{2.0} + \\text{4.0}\\,x + \\text{6.0}\\,x^{2}$ p+(2,5) # 与另一个序列相加,相当于把序列作为多项式,对应系数相加 $x \\mapsto \\text{3.0} + \\text{7.0}\\,x + \\text{3.0}\\,x^{2}$ p*p # 与多项式相乘各项一一相乘,然后相同次数的系数相加 $x \\mapsto \\text{1.0} + \\text{4.0}\\,x + \\text{10.0}\\,x^{2} + \\text{12.0}\\,x^{3} + \\text{9.0}\\,x^{4}$ p**2 # 幂,与乘法规则相同 $x \\mapsto \\text{1.0} + \\text{4.0}\\,x + \\text{10.0}\\,x^{2} + \\text{12.0}\\,x^{3} + \\text{9.0}\\,x^{4}$ 除法: //是多项式类的除法运算符，在这方面，多项式被视为整数.与之对应的是求余%,表示除后余下的项 p//P([-1,1]) #相当于多项式分解 $x \\mapsto \\text{5.0} + \\text{3.0}\\,x$ p%P([-1,1]) $x \\mapsto \\text{6.0}$ P([ 5., 3.])*P([-1,1])+[6] $x \\mapsto \\text{1.0} + \\text{2.0}\\,x + \\text{3.0}\\,x^{2}$ 如果要一次求出,可以使用divmod方法 quo, rem = divmod(p, P([-1, 1])) quo $x \\mapsto \\text{5.0} + \\text{3.0}\\,x$ rem $x \\mapsto \\text{6.0}$ 多项式因式分解 多项式的根可以使用np.roots()方法获得 其意义是令该多项式等于0,则当变量为这些根时满足该等式 r = p.roots() r array([-0.33333333-0.47140452j, -0.33333333+0.47140452j]) p(r) array([0.-2.77555756e-16j, 0.+2.77555756e-16j]) np.poly(r) array([1. , 0.66666667, 0.33333333]) 多项式拟合 多项式的拟合使用Chebyshev模块 的fit(x,y,deg),来做 其中deg为最高次数 我们用1000个在${-\\pi\\over 2} \\sim{\\pi \\over 2}$间的值拟合sin(x) from numpy.polynomial import Chebyshev as T x = np.linspace(-np.pi/2,np.pi/2,20) y = np.sin(x) a = T.fit(x,y,5) xx,yy=a.linspace() plt.plot(x,y,'o',color=\"red\") plt.plot(xx,yy,\"--\",lw=2,color = \"blue\") plt.show() error = np.abs(a(x)-y)#polyval计算多项式的值 plt.plot(x,error) plt.show() 做3,5,7次多项式的拟合,比较结果误差 x = np.linspace(-np.pi/2,np.pi/2,1000) y = np.sin(x) for i in (3,5,7): a = T.fit(x,y,i) if i == 3: color = \"red\" elif i == 5: color = \"blue\" else : color = \"yellow\" error = np.abs(a(x)-y) plt.plot(x,error,color = color) plt.show() 微积分 用deriv()和integ()可以分别计算多项式的微分和积分 f(x)=3x^2+2x+1 做微分是 f^{'}(x)=6x+2 v1 = np.array([1,2,3]) p = P(v1) p $x \\mapsto \\text{1.0} + \\text{2.0}\\,x + \\text{3.0}\\,x^{2}$ p.deriv() $x \\mapsto \\text{2.0} + \\text{6.0}\\,x$ p = P(v1) f(x)=3x^2+2x+1 做积分是 F(x)=x^3+x^2+x+N N是无法预测的所以置0 p.integ() $x \\mapsto \\color{LightGray}{\\text{0.0}} + \\text{1.0}\\,x + \\text{1.0}\\,x^{2} + \\text{1.0}\\,x^{3}$ Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/线性代数.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/线性代数.html","title":"线性代数","keywords":"","body":"线性代数 numpy中的多维数组最常见的用法就是解决线性代数问题,尤其是矩阵运算. 向量间计算 向量间计算多数都是对应项操作,这个ufunc可以直接支持.比较特殊的操作是点乘和求夹角 import numpy as np 求模 向量中模的定义为各个项的平方和开方 |v| = \\sqrt {\\sum_{i=0}^{N} {v_i^2}} numpy中使用np.linalg.norm(v)来计算 v_1 = np.array([3,5]) l_v1 = np.linalg.norm(v_1) print(l_v1) 5.830951894845301 点积(内积) 向量的点积为各项相乘的和,为一个标量.numpy中使用np.linalg.dot来计算其值,或者使用python3.6定义的矩阵乘法符号@来计算 a \\cdot b = \\sum_{i=0}^{n} a_i\\cdot b_i v_1 = np.array([3,5]) v_2 = np.array([4,2]) v_1@v_2 22 叉乘(外积) |C| = | v_1 \\times v_2 |= |v_1| |v_2|sin 或者在二维情况下可以看做: (x_1,y_1) \\times (x_2,y_2)= x_1*y_2 -x_2*y_1 C的方向用右手定则,右手4指从v_1不超过180度转向v_2时大拇指的方向即为C的方向.其值为新向量的模 numpy中没有直接计算的方法,这边需要迂回下,通过计算余弦,再转换为角度计算正弦再求出 v_1 = np.array([3,5]) v_2 = np.array([4,2]) v_1_norm = np.linalg.norm(v_1) v_2_norm = np.linalg.norm(v_2) cos_theta = (v_1@v_2)/(v_1_norm * v_2_norm) # 计算余弦 theta = np.arccos(cos_theta) cross_product = np.sin(theta)* (v_1_norm * v_2_norm) cross_product 14.000000000000002 矩阵计算 通常我们认为向量是一维的矩阵,numpy中通常使用二维数组代表矩阵,概括下矩阵计算的工具包括 向量,矩阵运算: 函数 说明 dot 向量乘法 vdot 向量点乘 inner 向量内积 outer 向量外积 matmul 矩阵乘法 trnsordot 张量乘法 einsum 评估操作数上的爱因斯坦求和约定 linalg.matrix_power 矩阵幂 kron 克罗内克积 矩阵分解 函数 说明 linalg.cholesky(a) Cholesky 分解 linalg.qr(a[, mode]) QR分解 linalg.svd(a[, full_matrices, compute_uv]) 奇异值分解 矩阵征值操作 函数 说明 linalg.norm(x[, ord, axis, keepdims]) 矩阵或向量范数 linalg.cond(x[, p]) 计算矩阵的条件数 linalg.det(a) 计算矩阵行列式 linalg.matrix_rank(M[, tol]) 使用SVD方法返回阵列的矩阵秩 linalg.slogdet(a) 计算数组行列式的符号和（自然）对数 trace(a[, offset, axis1, axis2, dtype, out]) 计算对角线元素的和 diag 以一维数组的形式返回方阵的对角线(或非对角线)元素,或将一维数组转换为方阵(非对角线元素为0) eig 计算方阵的本征值和本征向量 求解方程和求逆矩阵 函数 说明 linalg.solve(a, b) 解线性方程组Ax=b linalg.tensorsolve(a, b[, axes]) 解张量表达式Ax = b linalg.lstsq(a, b[, rcond]) 计算Ax=b的最小二乘解 linalg.inv(a) 计算方阵的逆 linalg.pinv(a[, rcond]) 计算矩阵的Moore-Penrose伪逆 linalg.tensorinv(a[, ind]) 计算N维数组的“逆”。 A = np.arange(9).reshape(3,3) print(A) [[0 1 2] [3 4 5] [6 7 8]] B = np.arange(13,22).reshape(3,3) print(B) [[13 14 15] [16 17 18] [19 20 21]] 矩阵的秩 线性代数中 \\mathbf {A} ={\\begin{bmatrix}1&2\\\\3&4\\end{bmatrix}} 一个矩阵A的列秩是A的线性独立的纵列的极大数目.类似地,行秩是A的线性独立的横行的极大数目. 矩阵的列秩和行秩总是相等的,因此它们可以简单地称作矩阵A的秩.通常表示为r(A)，rk(A)或rank A. 矩阵的行秩与列秩相等,是线性代数基本定理的重要组成部分.其基本证明思路是,矩阵可以看作线性映射的变换矩阵,列秩为像空间的维度,行秩为非零原像空间的维度,因此列秩与行秩相等,即像空间的维度与非零原像空间的维度相等(这里的非零原像空间是指约去了零空间后的商空间:原像空间).这从矩阵的奇异值分解就可以看出来. 给出这一结果的两种证明.第一个证明是简短的,仅用到向量的线性组合的基本性质.第二个证明利用了正交性.第一个证明利用了列空间的基,第二个证明利用了行向量空间的基.第一个证明适用于定义在标量域上的矩阵,第二个证明适用于内积空间.二者都适用于实或复的欧氏空间,也都易于修改去证明当A是线性变换的情形. M_1 = np.matrix(np.arange(1,5).reshape(2,2)) M_1 matrix([[1, 2], [3, 4]]) np.linalg.matrix_rank(M_1) 2 方阵的迹 迹就是方阵主对角线元素之和 np.trace(M_1) 5 转置矩阵(transpose) 将矩阵延对角线翻转 M_1.T matrix([[1, 3], [2, 4]]) 共轭矩阵(hermitian)(复数为元素) M_2 = np.matrix([[1+1j,2-4j],[3-1j,2+3j]]) M_2 matrix([[1.+1.j, 2.-4.j], [3.-1.j, 2.+3.j]]) M_2.H matrix([[1.-1.j, 3.+1.j], [2.+4.j, 2.-3.j]]) 逆矩阵(inverse) 在线性代数中,给定一个n阶方阵$\\mathbf{A}$，若存在一n阶方阵$ \\mathbf {B}$,使得 $ \\mathbf{AB}=\\mathbf{BA}=\\mathbf{I}_n$,其中 $ \\mathbf{I}_n $为n阶单位矩阵,则称 $\\mathbf{A} $是可逆的,且 $\\mathbf {B} $是$\\mathbf{A}$的逆矩阵,记作$\\mathbf {A} ^{-1}$ 只有正方形（n×n）的矩阵,即方阵,才可能但非必然有逆矩阵.若方阵 $\\mathbf{A}$的逆矩阵存在,则称 $\\mathbf{A}$为非奇异方阵或可逆方阵. 性质有: $ \\left (A^{-1} \\right )^{-1}=A $ $ (\\lambda A)^{-1}=\\frac{1}{\\lambda}\\times A^{-1} $ $ (AB)^{-1}=B^{-1}A^{-1} $ $ \\left (A^\\mathrm{T} \\right )^{-1}=\\left (A^{-1} \\right )^{\\mathrm{T}}$ ($ A^{\\mathrm{T}}$ 为A的转置) $ \\det(A^{-1})=\\frac{1}{\\det(A)} $（det为行列式） M_1.I matrix([[-2. , 1. ], [ 1.5, -0.5]]) 伴随矩阵(adjoint) 在线性代数中,一个方形矩阵的伴随矩阵是一个类似于逆矩阵的概念.如果矩阵可逆,那么它的逆矩阵和它的伴随矩阵之间只差一个系数.然而伴随矩阵对不可逆的矩阵也有定义,并且不需要用到除法. 对n×n的矩阵A和B,有: $\\mathrm{adj}(\\mathbf{I}) = \\mathbf{I}$ $(\\mathbf{AB}) = \\mathrm{adj}(\\mathbf{B})\\,\\mathrm{adj}(\\mathbf{A})$ $\\mathrm{adj}(\\mathbf{A}^T) = \\mathrm{adj}(\\mathbf{A})^T $ $ \\det\\big(\\mathrm{adj}(\\mathbf{A})\\big) = \\det(\\mathbf{A})^{n-1}$ $ \\mathrm{adj}(k \\mathbf{A}) = k^{n-1} \\ \\mathrm{adj}(\\mathbf{A}) $ 当n>2时,$\\mathrm{adj}(\\mathrm{adj}(\\mathbf{A})) =(\\det \\mathbf{A})^{n-2} \\mathbf{A} $ 如果A可逆,那么 $ \\mathrm{adj}(\\mathbf{A}^{-1}) = \\mathrm{adj}(\\mathbf{A})^{-1} = \\frac{A}{\\det A} $ 如果A是对称矩阵,那么其伴随矩阵也是对称矩阵;如果A是反对称矩阵,那么当n为偶数时,A的伴随矩阵也是反对称矩阵,n为奇数时则是对称矩阵. 如果A是(半)正定矩阵,那么其伴随矩阵也是(半)正定矩阵. 如果矩阵A和B相似,那么$\\mathrm{adj}(\\mathbf{A})$和 $\\mathrm{adj}(\\mathbf{B})$也相似. 如果n>2,那么非零矩阵A是正交矩阵当且仅当 $ \\mathrm{adj}(\\mathbf{A}) = \\pm A^T $ 伴随矩阵的秩 当矩阵A可逆时,它的伴随矩阵也可逆,因此两者的秩一样,都是n.当矩阵A不可逆时,A的伴随矩阵的秩通常并不与A相同.当A的秩为n-1时,其伴随矩阵的秩为1,当A的秩小于n-1时,其伴随矩阵为零矩阵. 伴随矩阵的特征值 设矩阵A在复域中的特征值为 $\\lambda1, \\lambda_2 \\cdots \\lambda_n$ (即为特征多项式的n个根),则A的伴随矩阵的特征值为$\\lambda_2 \\lambda_3 \\cdots \\lambda{n}, \\ \\lambda1 \\lambda_3 \\cdots \\lambda{n}, \\cdots , \\lambda1 \\lambda_2 \\cdots \\lambda{n-1} $ np.dot(np.linalg.det(M_1),M_1.I) matrix([[ 4., -2.], [-3., 1.]]) 矩阵的范数(matrix norms) 范数(norm),是具有\"长度\"概念的函数.在线性代数中其为向量空间内的所有向量赋予非零的正长度或大小.半范数反而可以为非零的向量赋予零长度. 举一个简单的例子,一个二维度的欧氏几何空间 $\\mathbb {R} ^{2}$就有欧氏范数.在这个向量空间的元素(譬如：(3,7))常常在笛卡儿坐标系统被画成一个从原点出发的箭号.每一个向量的欧氏范数就是箭号的长度. 拥有范数的向量空间就是赋范向量空间.同样,拥有半范数的向量空间就是赋半范向量空间. np.linalg.norm(M_1) 5.477225575051661 和和差 矩阵计算和和差要求两个矩阵形状一致,就是对应下标的各项计算的结果,numpy的ufunc刚好可以满足 A+B array([[13, 15, 17], [19, 21, 23], [25, 27, 29]]) A-B array([[-13, -13, -13], [-13, -13, -13], [-13, -13, -13]]) 矩阵与标量的积 矩阵与标量的积就是标量在各项上的积.这也刚好ufunc就可以直接支持. A*3 array([[ 0, 3, 6], [ 9, 12, 15], [18, 21, 24]]) 3*A array([[ 0, 3, 6], [ 9, 12, 15], [18, 21, 24]]) 矩阵的内积(点积) 矩阵的乘法必须前一个矩阵的行数与后一个举证的列数相同.返回的是一个以前一个矩阵的行数为行数,后一个矩阵的列数为列数的新矩阵. 即形状上 $ (x,n)@ (n,y) -> (x,y) $,n是一致的 C = np.arange(12).reshape(4,3) C array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) D = np.arange(1,7).reshape(3,2) D array([[1, 2], [3, 4], [5, 6]]) C@D array([[ 13, 16], [ 40, 52], [ 67, 88], [ 94, 124]]) 求特征值特征向量 M_lamida=np.matrix([[3,0,-1],[2,4,2],[-1,0,3]]) np.linalg.eig(M_lamida) (array([4., 4., 2.]), matrix([[ 0. , 0.70710678, 0.40824829], [ 1. , 0. , -0.81649658], [ 0. , -0.70710678, 0.40824829]])) 第一项是特征值,第二项是特征向量 判断正定矩阵 正定矩阵的定义是:设M是n阶方阵,如果对任何非零向量z,都有$ z^TMz > 0$,其中$z^T$ 表示z的转置,就称M正定矩阵. M_4=np.arange(16).reshape(4,4) M_4 array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) M_4 = M_4+M_4.T#将方阵转换成对称阵 M_4 array([[ 0, 5, 10, 15], [ 5, 10, 15, 20], [10, 15, 20, 25], [15, 20, 25, 30]]) lambdas,_ = np.linalg.eig(M_4) lambdas array([ 6.74165739e+01, -7.41657387e+00, 1.82694656e-15, -1.72637110e-15]) #判断是否所有特征值都大于0 True if np.all(lambdas > 0) else False False 因此矩阵不是正定矩阵 还有一种方式是使用cholesky分解的方法: Cholesky 分解是把一个对称正定的矩阵表示成一个下三角矩阵L和其转置的乘积的分解.它要求矩阵的所有特征值必须大于零,故分解的下三角的对角元也是大于零的. np.linalg.cholesky(np.arange(16).reshape(4,4)) --------------------------------------------------------------------------- LinAlgError Traceback (most recent call last) in ----> 1 np.linalg.cholesky(np.arange(16).reshape(4,4)) ~/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py in cholesky(a) 757 t, result_t = _commonType(a) 758 signature = 'D->D' if isComplexType(t) else 'd->d' --> 759 r = gufunc(a, signature=signature, extobj=extobj) 760 return wrap(r.astype(result_t, copy=False)) 761 ~/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_nonposdef(err, flag) 98 99 def _raise_linalgerror_nonposdef(err, flag): --> 100 raise LinAlgError(\"Matrix is not positive definite\") 101 102 def _raise_linalgerror_eigenvalues_nonconvergence(err, flag): LinAlgError: Matrix is not positive definite np.linalg.cholesky(M_4) --------------------------------------------------------------------------- LinAlgError Traceback (most recent call last) in ----> 1 np.linalg.cholesky(M_4) ~/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py in cholesky(a) 757 t, result_t = _commonType(a) 758 signature = 'D->D' if isComplexType(t) else 'd->d' --> 759 r = gufunc(a, signature=signature, extobj=extobj) 760 return wrap(r.astype(result_t, copy=False)) 761 ~/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_nonposdef(err, flag) 98 99 def _raise_linalgerror_nonposdef(err, flag): --> 100 raise LinAlgError(\"Matrix is not positive definite\") 101 102 def _raise_linalgerror_eigenvalues_nonconvergence(err, flag): LinAlgError: Matrix is not positive definite 报错了,因此可以看出不是正定的 我们试试测试一个单位矩阵 i=np.eye(4) np.linalg.cholesky(i) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) 奇异值分解 arr = np.arange(9).reshape((3, 3)) + np.diag([1, 0, 1]) arr array([[1, 1, 2], [3, 4, 5], [6, 7, 9]]) uarr, spec, vharr = np.linalg.svd(arr) uarr array([[-0.1617463 , -0.98659196, 0.02178164], [-0.47456365, 0.09711667, 0.87484724], [-0.86523261, 0.13116653, -0.48390895]]) spec array([14.88982544, 0.45294236, 0.29654967]) vharr array([[-0.45513179, -0.54511245, -0.70406496], [ 0.20258033, 0.70658087, -0.67801525], [-0.86707339, 0.45121601, 0.21115836]]) 矩阵QR分解 X = np.random.randn(5,5) X array([[-0.18551814, -0.20917874, -0.40211628, -1.19894807, -1.46630053], [ 1.29711693, 0.20234821, 0.32854939, -1.8502084 , -1.30213228], [-0.89666881, -1.69630947, -0.0259825 , -1.45345265, 0.06733376], [-0.43573678, -1.83826832, 1.7405289 , 0.49307392, 1.21645164], [-0.34511311, -0.48444181, -1.52753147, 0.20559079, -1.36148696]]) mat = X.T.dot(X) mat array([[ 2.82991386, 2.79049186, 0.29282322, -1.16004689, -1.53755427], [ 2.79049186, 6.57608063, -2.26488911, 1.33591464, -1.64758744], [ 0.29282322, -2.26488911, 5.63311054, 0.45615902, 4.35704246], [-1.16004689, 1.33591464, 0.45615902, 7.25866167, 4.38925923], [-1.53755427, -1.64758744, 4.35704246, 4.38925923, 7.18352088]]) q,r=np.linalg.qr(mat) q array([[-0.63936271, 0.21100986, -0.18461235, -0.49914373, 0.51328742], [-0.63045609, -0.56252424, -0.23227077, 0.4278655 , -0.22152427], [-0.06615758, 0.53636822, -0.63382613, -0.02263921, -0.55288716], [ 0.2620895 , -0.58615552, -0.32811236, -0.66437978, -0.19665346], [ 0.34737978, -0.08815989, -0.63448821, 0.35478672, 0.5857527 ]]) r array([[-4.42614784, -6.0024363 , 2.50112288, 3.29643578, 5.37932372], [ 0. , -4.96300058, 3.70576854, -5.3932577 , -0.26673908], [ 0. , 0. , -6.01256737, -5.55185025, -8.09309891], [ 0. , 0. , 0. , -2.12496231, -0.40364251], [ 0. , 0. , 0. , 0. , 0.51142418]]) 张量计算 将数据继续扩展,那么矩阵,向量,常数就都是张量的特殊形式了. 张量(tensor)是一个可用来表示在一些矢量,标量和其他张量之间的线性关系的多线性函数,这些线性关系的基本例子有内积,外积,线性映射以及笛卡儿积.其坐标在 n 维空间内,有$ n^r $ 个分量的一种量,其中每个分量都是坐标的函数,而在坐标变换时,这些分量也依照某些规则作线性变换.r称为该张量的秩或阶(与矩阵的秩和阶均无关系) 在同构的意义下,第零阶张量(r=0)即为标量,第一阶张量(r=1)即为矢量或者说向量,第二阶张量(r=2)则成为矩阵.由于变换方式的不同,张量分成\"协变张量\"(指标在下者),\"逆变张量\"(指标在上者),\"混合张量\"(指标在上和指标在下两者都有)三类. 一个张量的大小是一个向量,向量的第 n 个元素描述了张量在第 n 维上的大小,在numpy中,张量的大小即为shape 张量加减 张量加减依然是ufunc,这边不做复述. Hadamard Product Hadamard Product 要求形状一致,本质上也还是ufunc A = np.array([ [[1,2,3], [4,5,6], [7,8,9]], [[11,12,13], [14,15,16], [17,18,19]], [[21,22,23], [24,25,26], [27,28,29]], ]) B = np.array([ [[1,2,3], [4,5,6], [7,8,9]], [[11,12,13], [14,15,16], [17,18,19]], [[21,22,23], [24,25,26], [27,28,29]], ]) C = A * B C array([[[ 1, 4, 9], [ 16, 25, 36], [ 49, 64, 81]], [[121, 144, 169], [196, 225, 256], [289, 324, 361]], [[441, 484, 529], [576, 625, 676], [729, 784, 841]]]) 张量的积算子 张量积算子通常表示为中间带有小x的圆$\\otimes $.给定具有q维度的张量A和具有r维度的张量B，这些张量的乘积将是具有q + r的量级的新张量,或者换句话说q + r维度. 需要注意张量的积算子在r为1,2时是区别于向量点乘,矩阵点乘的.它要求形状一致 张量积算子不限于张量,也可以在矩阵和向量上执行，这可以是练习的好地方，以便发展更高维度的直觉. 在向量上: a = (a_1, a_2) b = (b_1, b_2) c = a \\otimes b c = {\\begin{bmatrix} a_1 * b\\\\ a_2 * b \\end{bmatrix} } 在矩阵上: A = {\\begin{bmatrix} a_{1,1}&a_{1,2}\\\\ a_{2,1}&a_{2,2} \\end{bmatrix}} B = {\\begin{bmatrix} b_{1,1}& b_{1,2}\\\\ b_{2,1} & b_{2,2} \\end{bmatrix}} C = A \\otimes B C ={\\begin{bmatrix} a_{1,1} * {\\begin{bmatrix}b_{1,1}&b_{1,2}\\\\b_{2,1}&b_{2,2}\\end{bmatrix}}&a_{1,2}*{\\begin{bmatrix}b_{1,1}&b_{1,2}\\\\b_{2,1}&b_{2,2}\\end{bmatrix}}\\\\ a_{2,1} * {\\begin{bmatrix}b_{1,1}&b_{1,2}\\\\b_{2,1}&b_{2,2}\\end{bmatrix}}&a_{2,2}*{\\begin{bmatrix}b_{1,1}&b_{1,2}\\\\b_{2,1}&b_{2,2}\\end{bmatrix}} \\end{bmatrix}} C = {\\begin{bmatrix} a_{1,1} * b_{1,1}&a_{1,1} * b_{1,2}& a_{1,2} * b_{1,1}& a_{1,2} * b_{1,2}\\\\ a_{1,1} * b_{2,1}&a_{1,1} * b_{2,2}& a_{1,2} * b_{2,1}& a_{1,2} * b_{2,2}\\\\ a_{2,1} * b_{1,1}&a_{2,1} * b_{1,2}& a_{2,2} * b_{1,1}& a_{2,2} * b_{1,2}\\\\ a_{2,1} * b_{2,1}&a_{2,1} * b_{2,2}& a_{2,2} * b_{2,1}& a_{2,2} * b_{2,2} \\end{bmatrix}} 注意上式中的值为压扁为矩阵后的形态,在拆解时每次拆解都是一层 在numpy中张量的积算子需要指定axes为0 np.tensordot(np.arange(4).reshape(2,2),np.arange(4).reshape(2,2),axes=0).shape (2, 2, 2, 2) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/统计计算/统计计算.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/统计计算/统计计算.html","title":"统计计算","keywords":"","body":"统计计算 常见的统计函数有 统计运算 说明 sum 统计求和 mean 均值 average 加权求均值 median 中值 std 标准差 var 方差 min 最小值 argmin 最小值下标 max 最大值 argmax 最大值下标 ptp 最大值和最小值之差 sort 排序 argsort 排序下标 unique 找出所有整数(不会出现重复),并排序,可选参数return_index=True,会额外返回一个记录下标的数组可选return_inverse=True,会额外返回一个下标数组,数组长度为原始数组,表示原始数组中对应的下标 bincount 对整数数组个元素出现次数统计,可选参数weight,可以对各个元素加权 其中很多运算还有个nanxxx版本,用来求相应函数去掉nan值得结果 import numpy as np import matplotlib.pyplot as plt %matplotlib inline 统计频数bincount a = np.random.randint(0,5,10) a array([4, 4, 3, 3, 0, 0, 4, 0, 0, 0]) np.bincount(a) array([5, 0, 0, 2, 3]) np.bincount(a,np.random.rand(10)) array([3.57475312, 0. , 0. , 0.42478611, 1.84216822]) 相关性 numpy提供了3种算相关性的操作 orrcoef(x, y=None, rowvar=1) Pearson乘积矩相关系数。 其中rowvar=1表示向量是横置的,即每一列为向量的一个属性,每行是一个向量.为0则说明向量为竖置 np.corrcoef([[1,1,0,1,1],[0,1,0,1,1]]) array([[1. , 0.61237244], [0.61237244, 1. ]]) np.corrcoef([[1,1,0,1,1],[0,1,0,1,1]],rowvar=0) /Users/huangsizhe/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide c /= stddev[:, None] /Users/huangsizhe/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide c /= stddev[None, :] array([[ 1., nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan]]) correlate(a, v, mode='valid') 两个1维序列的互相关.该函数计算信号处理文本中通常定义的相关性 c_{av}[k] = sum_n a[n+k] * conj(v[n]) 其中a和v序列在必要时被填零,conj是共轭.mode可选{‘valid’, ‘same’, ‘full’} np.correlate([1, 2, 3], [0, 1, 0.5]) array([3.5]) np.correlate([1, 2, 3], [0, 1, 0.5], \"same\") array([2. , 3.5, 3. ]) np.correlate([1, 2, 3], [0, 1, 0.5], \"full\") array([0.5, 2. , 3.5, 3. , 0. ]) cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None) 估计协方差矩阵,给定数据和权重.协方差表示两个变量一起变化的水平.如果我们检查N维样本,则协方差矩阵元素$C{ij}$是$x_i$和$x_j$的协方差.$C{ii}$元素是$x_i$的方差.其中rowvar一样是代表向量的是横置还是竖置 在概率论和统计学中,协方差Cov(X,Y)用于衡量两个变量的总体误差.而方差是协方差的一种特殊情况,即当两个变量是相同的情况. 期望值分别为$E(X)=\\mu$与$E(Y)=\\nu$的两个实数随机变量X 与Y 之间的协方差定义为: Cov(X, Y) = E((X - \\mu) (Y - \\nu)) Cov(X, Y) = E(X \\cdot Y) - \\mu \\nu 协方差矩阵是一个矩阵,其每个元素是各个向量元素之间的协方差.是从标量随机变量到高维度随机向量的自然推广. persontype=np.dtype({'names':['name','height','weight'],'formats':['S32','f','f']},align=True) a = np.array([(\"Huang\",175,70),(\"Hao\",170,60),(\"Li\",180,75)],dtype=persontype) data = np.array([a[\"height\"],a[\"weight\"]]) data array([[175., 170., 180.], [ 70., 60., 75.]], dtype=float32) data_cov = np.cov(data) data_cov #协方差矩阵 array([[25. , 37.5 ], [37.5 , 58.33333333]]) #相关系数矩阵 data_corr = np.corrcoef(data) data_corr array([[1. , 0.98198051], [0.98198051, 1. ]]) 也就是说身高与体重相关系数高达98.19% 直方图 通常在统计分析中最常见的是使用直方图观察数据的分布,numpy中有函数直接支持这个操作 直方图统计histogram 最基础的直方图,计算一组数据各个区间中的数据统计 用法: histogram(a,bins=10,range=None,normed=False,weights=None) 其中 bin指定统计区间个数, range是一个长为2的元组,分别表示统计范围的最小值和最大值(None表示由数据决定) normed=False表示返回在每个区间的个数,为True则表示返回一个在各个区间的概率密度 weight表示权值和前面一样用法 c = np.random.rand(100) y,x = np.histogram(c,bins=5,range=(0,1)) xticks = [str(x[i])+'~'+str(x[i+1]) for i in range(len(y))] plt.axes([0.025,0.025,0.95,0.95]) plt.bar(range(len(y)), y, facecolor='#9999ff', edgecolor='white') for i,j in zip(range(len(y)), y): plt.text(i,j,str(float(y[i])/sum(y)*100)+\"%\") plt.xlim(-0.4,5) plt.xticks(np.arange(5)+0.4,xticks) plt.show() histogram2d(x, y[, bins, range, normed, weights]) 计算两个数据样本的二维直方图。 xedges = [0, 1, 1.5, 3, 5] yedges = [0, 2, 3, 4, 6] x = np.random.normal(3, 1, 100) y = np.random.normal(1, 1, 100) H, xedges, yedges = np.histogram2d(y, x, bins=(xedges, yedges)) H = np.ones((4, 4)).cumsum().reshape(4, 4) print(H[::-1]) fig = plt.figure(figsize=(7, 3)) ax = fig.add_subplot(131) ax.set_title('imshow: equidistant') im = plt.imshow(H, interpolation='nearest', origin='low',extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]]) [[13. 14. 15. 16.] [ 9. 10. 11. 12.] [ 5. 6. 7. 8.] [ 1. 2. 3. 4.]] histogramdd(sample, bins=10, range=None, normed=False, weights=None) 计算多维直方图 r = np.random.randn(100,3) H, edges = np.histogramdd(r, bins = (5, 8, 4)) H.shape, edges[0].size, edges[1].size, edges[2].size ((5, 8, 4), 6, 9, 5) digitize(x, bins, right=False) 返回输入数组中每个值所属的bin的索引 x = np.array([0.2, 6.4, 3.0, 1.6]) bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0]) inds = np.digitize(x, bins) inds array([1, 4, 3, 2]) for n in range(x.size): print(bins[inds[n]-1], \" 0.0 x = np.array([1.2, 10.0, 12.4, 15.5, 20.]) bins = np.array([0, 5, 10, 15, 20]) np.digitize(x,bins,right=True) array([1, 2, 3, 4, 4]) np.digitize(x,bins,right=False) array([1, 3, 3, 4, 5]) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/傅里叶变换/傅里叶变换.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/傅里叶变换/傅里叶变换.html","title":"傅里叶变换","keywords":"","body":"傅里叶变换 将时域信号变换至频域加以分析的方法称为频谱分析.频谱分析的目的是把复杂的时间历程波形,经过傅里叶变换分解为若干单一的谐波分量来研究,以获得信号的频率结构以及各谐波和相位信息. 对信号进行频谱分析可以获得更多有用信息,如求得动态信号中的各个频率成分和频率分布范围,求出各个频率成分的幅值分布和能量分布,从而得到主要幅度和能量分布的频率值. 由时间函数求频谱函数的傅里叶变换公式就是将该时间函数乘以以频率为系数的指数函数之后,在负无限大到正无限大的整个区间内,对时间进行积分.这样就得到了与这个时间函数对应的.以频率为自变量的频谱函数.频谱函数是信号的频域表示方式.根据上述傅里叶变换公式,可以求出常数的频谱函数为频域中位于零频率处的一个冲激函数,表示直流信号就是一个频率等于零的信号.与此相反冲激函数的频谱函数等于常数,表示冲激函数含有无限多个、频率无限密集的正弦成分.同样的单个正弦波的频谱函数就是频域中位于该正弦波频率处的一对冲激函数. np中用np.fft模块可以做傅里叶变换 傅里叶级数 假设:关于一个变量的任意一个周期函数 $f(x)$,不论连续或不连续,且函数 $f(x)$ 必须平方可积的,则 $f(x)$ 都可以近似的展开为正弦函数的级数,而正弦函数的参数为变量的倍数,大约就是: f(x)=\\sum_k c_k sin kx 最常见的表达形式 周期为T的函数f(x)有: f(x)=a_0+\\sum_{k=1}^\\infty{[a_k sin (k{2\\pi\\over T}x)+b_k cos(k{2\\pi\\over T}x)]} f(x)=a_0+\\sum_{k=1}^\\infty{\\sqrt{a_k^2+b_k^2} [sin(k{2\\pi\\over T}x+\\theta)]} 然后因为有欧拉公式: cos x={ \\frac {e^{ix}+e^{-ix}} 2} sin x={\\frac {e^{ix}-e^{-ix}} 2i} 所以有: f(x) = \\sum_{k=-\\infty}^{+\\infty}a_k e^{ik({2\\pi\\over T})t} 傅里叶变换: 假设一个函数（信号）是周期的，但是它的周期是无穷大,可以得出: 傅里叶正变换: F(\\omega)=\\int_{-\\infty}^\\infty f(t) e^{-i\\omega t}dt 傅里叶反变换: f(t)={1\\over {2\\pi}}\\int_{-\\infty}^\\infty F(\\omega) e^{-i\\omega t}d\\omega 于是,我们可以利用傅里叶正变换变到分解状态(或者说频域),然后把它的分量分别处理,再利用反变换回去了. numpy中的接口 实域变频域后,函数表现形式有了变化,各个分量有了频率,周期. numpy中获取和处理频率的工具有 fftfreq(n[, d]) 获取连续傅里叶变换分量频率 rfftfreq(n[, d]) 获取离散傅里叶变换分量频率 fftshift(x[, axes]) 将0频分量移动到中心 ifftshift(x[, axes]) The inverse of fftshift. 上一操作的反操作 离散时间信号 在时间上依次出现的数值序列,例如,{…，0.5，1，2，-1，0，5，…}.相邻两个数之间的时间间隔可以是相等的,也可以是不等的.在前一情况下,设时间间隔为T秒,则离散信号可用符号x(nT)来表示(图1).在间隔T归一化为1的条件下,T可以省略,即将x(nT)表示为x(n).x(n)既可表示整个序列,也可表示离散信号在nT瞬间的值. 离散傅里叶变换 简称DFT, python的numpy里的fft模块就是用的这种变换 对于N点序列 $\\left{x[n]\\right}_{0\\le n \\hat{x}[k]=\\sum_{n=0}^{N-1} e^{-i\\frac{2\\pi}{N}nk}x[n] \\qquad k = 0,1,\\ldots,N-1. 其中i是虚数单位 其逆变换为: x\\left[n\\right]={1 \\over N}\\sum_{k=0}^{N-1} e^{ i\\frac{2\\pi}{N}nk}\\hat{x}[k] \\qquad n = 0,1,\\ldots,N-1. numpy中实现的标准快速傅里叶变换 方法 说明 fft(a[, n, axis, norm]) 一维空间离散傅里叶变换 ifft(a[, n, axis, norm]) 一维空间离散傅里叶反变换 fft2(a[, s, axes, norm]) 二维空间离散傅里叶变换 ifft2(a[, s, axes, norm]) 二维空间离散傅里叶反变换 fftn(a[, s, axes, norm]) N维空间离散傅里叶变换 ifftn(a[, s, axes, norm]) N维空间离散傅里叶反变换 fft的参数是一个数组,这个数组可以理解为一个连续函数按一定周期采样的结果.而这个数组一般都是2的整数次幂.比如256,128,64这样 以下是标准傅里叶变换的例子 一维空间 我们以一个矩形波为例子 import numpy as np import matplotlib.pyplot as plt %matplotlib inline t = np.linspace(-1, 1, 128,endpoint=True) tz = list(map(lambda x: 1 if (x > -0.5 and x sp = np.fft.fft(tz) re = np.fft.ifft(sp)#傅里叶逆变换 plt.plot(t,re) plt.show() /Users/huangsizhe/Lib/conda/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py:538: ComplexWarning: Casting complex values to real discards the imaginary part return array(a, dtype, copy=False, order=order) freq = np.fft.fftfreq(t.shape[-1])#获取频率 plt.plot(freq, sp.real) plt.show() plt.plot(freq, sp.imag) plt.show() 二维空间(常用图像处理) a = np.mgrid[:5, :5][0] a array([[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]]) np.fft.fft2(a) array([[ 5.00000000e+01+0.00000000e+00j, 2.22044605e-15+0.00000000e+00j, 2.22044605e-15+0.00000000e+00j, 2.22044605e-15+0.00000000e+00j, 2.22044605e-15+0.00000000e+00j], [-1.25000000e+01+1.72047740e+01j, -5.55111512e-16+7.64045449e-16j, -5.55111512e-16+7.64045449e-16j, -5.55111512e-16+7.64045449e-16j, -5.55111512e-16+7.64045449e-16j], [-1.25000000e+01+4.06149620e+00j, -5.55111512e-16+1.80366664e-16j, -5.55111512e-16+1.80366664e-16j, -5.55111512e-16+1.80366664e-16j, -5.55111512e-16+1.80366664e-16j], [-1.25000000e+01-4.06149620e+00j, -5.55111512e-16-1.80366664e-16j, -5.55111512e-16-1.80366664e-16j, -5.55111512e-16-1.80366664e-16j, -5.55111512e-16-1.80366664e-16j], [-1.25000000e+01-1.72047740e+01j, -5.55111512e-16-7.64045449e-16j, -5.55111512e-16-7.64045449e-16j, -5.55111512e-16-7.64045449e-16j, -5.55111512e-16-7.64045449e-16j]]) n维空间 b = np.mgrid[:3, :3, :3][0] b array([[[0, 0, 0], [0, 0, 0], [0, 0, 0]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[2, 2, 2], [2, 2, 2], [2, 2, 2]]]) np.fft.fftn(b, axes=(1, 2)) array([[[ 0.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j]], [[ 9.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j]], [[18.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j]]]) np.fft.fftn(b, (2, 2), axes=(0, 1)) array([[[ 2.+0.j, 2.+0.j, 2.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j]], [[-2.+0.j, -2.+0.j, -2.+0.j], [ 0.+0.j, 0.+0.j, 0.+0.j]]]) [X, Y] = np.meshgrid(2 * np.pi * np.arange(200) / 12, 2 * np.pi * np.arange(200) / 34) S = np.sin(X) + np.cos(Y) + np.random.uniform(0, 1, X.shape) FS = np.fft.fftn(S) plt.imshow(np.log(np.abs(np.fft.fftshift(FS))**2)) plt.show() numpy中其他的傅里叶变换还有 实数快速傅里叶变换: 方法 说明 rfft(a[, n, axis, norm]) 实数输入的一维空间离散傅里叶变换 irfft(a[, n, axis, norm]) 实数输入的一维空间离散傅里叶反变换 rfft2(a[, s, axes, norm]) 实数输入的二维空间离散傅里叶变换 irfft2(a[, s, axes, norm]) 实数输入的二维空间离散傅里叶反变换 rfftn(a[, s, axes, norm]) 实数输入的N维空间离散傅里叶变换 irfftn(a[, s, axes, norm]) 实数输入的N维空间离散傅里叶反变换 Hermitian傅里叶变换: 方法 说明 hfft(a[, n, axis, norm]) 计算实域中埃尔米特对称的信号的快速傅里叶变换 ihfft(a[, n, axis, norm]) 计算实域中埃尔米特对称的信号的快速傅里叶反变换 他们用法与上面的相同就不一一描述了 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/窗口函数与卷积/窗口函数与卷积.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/窗口函数与卷积/窗口函数与卷积.html","title":"窗口函数与卷积","keywords":"","body":"窗口函数与卷积 import numpy as np import matplotlib.pyplot as plt %matplotlib inline 窗口函数 在信号处理中,窗函数(window function)是一种除在给定区间之外取值均为0的实函数.譬如:在给定区间内为常数而在区间外为0的窗函数被形象地称为矩形窗.任何函数与窗函数之积仍为窗函数,所以相乘的结果就像透过窗口\"看\"其他函数一样.窗函数在频谱分析,滤波器设计,波束形成,以及音频数据压缩(如在Ogg Vorbis音频格式中)等方面有广泛的应用. numpy中提供了几种常见的窗函数 函数 说明 bartlett(M) Bartlett窗口函数 blackman(M) Blackman 窗口函数 hamming(M) Hamming窗口函数 hanning(M) Hanning窗口函数 kaiser(M, beta) Kaiser窗口函数 bartlett窗 w(n)=\\frac{2}{N-1}\\cdot\\left(\\frac{N-1}{2}-\\left |n-\\frac{N-1}{2}\\right |\\right)\\, window = np.bartlett(51) plt.plot(window) plt.title(\"Bartlett window\") plt.ylabel(\"Amplitude\") plt.xlabel(\"Sample\") plt.show() Blackman窗 w(n)=a_0 - a_1 \\cos \\left ( \\frac{2 \\pi n}{N-1} \\right) + a_2 \\cos \\left ( \\frac{4 \\pi n}{N-1} \\right) {\\displaystyle a_{0}=0.42;\\quad a_{1}=0.5;\\quad a_{2}=0.08\\,} a_0=0.42;\\quad a_1=0.5;\\quad a_2=0.08\\, window = np.blackman(51) plt.plot(window) plt.title(\"Blackman window\") plt.ylabel(\"Amplitude\") plt.xlabel(\"Sample\") plt.show() Hamming窗 w(n)=0.53836 - 0.46164\\; \\cos \\left ( \\frac{2 \\pi n}{N-1} \\right) window = np.hamming(51) plt.plot(window) plt.title(\"Hamming window\") plt.ylabel(\"Amplitude\") plt.xlabel(\"Sample\") plt.show() Hanning窗 w(n)= 0.5\\; \\left(1 - \\cos \\left ( \\frac{2 \\pi n}{N-1} \\right) \\right) window = np.hanning(51) plt.plot(window) plt.title(\"Hanning window\") plt.ylabel(\"Amplitude\") plt.xlabel(\"Sample\") plt.show() Kaiser窗 w(n)=\\frac{I_0\\Bigg (\\pi\\alpha \\sqrt{1 - (\\begin{matrix} \\frac{2 n}{N-1} \\end{matrix}-1)^2}\\Bigg )} {I_0(\\pi\\alpha)} window = np.kaiser(51, 14) plt.plot(window) plt.title(\"Kaiser window\") plt.ylabel(\"Amplitude\") plt.xlabel(\"Sample\") plt.show() 卷积 卷积运算符经常出现在信号处理中,其中它模拟线性时不变系统对信号的影响.在概率理论中,两个独立随机变量的和根据它们各自的分布的卷积来分布. 离散卷积运算定义为: (a * v)[n] = \\sum_{m = -\\infty}^{\\infty} a[m] v[n - m] numpy提供了通用的卷积操作convolve(a, v, mode='full') 其中前两个参数都是一维的输入向量,而mode则提供了可选的三种运算规则,它可以有3种选项 full 默认情况下,模式为\"full\".这在每个重叠点处返回卷积,其输出形状为(N M-1,).在卷积的端点,信号不完全重叠,并且可以看到边界效应. same 模式same返回长度max(M,N)的输出.边界效应仍然可见. valid 模式'valid'返回长度为max(M,N)-min(M,N)+1.卷积产物仅针对信号完全重叠的点给出.信号边界外的值没有效果. np.convolve([1, 2, 3], [0, 1, 0.5]) # a相当于[...0,0,1,2, 3 ,0,0,...] # v相当于[...0,0,0,1,0.5,0,0,...] # [0*0.5+0*1+1*0+2*0+3*0, # 0*0.5+1*1+2*0+3*0, # 1*0.5+2*1+3*0, # 1*0+2*0.5+3*1+0*0, # 1*0+2*0+3*0.5+0*1+0*0] array([0. , 1. , 2.5, 4. , 1.5]) np.convolve([1,2,3],[0,1,0.5], 'same') array([1. , 2.5, 4. ]) np.convolve([1,2,3],[0,1,0.5], 'valid') array([2.5]) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/财务分析.html":{"url":"工具链篇/计算工具/数值计算/使用numpy_scipy处理复杂的数值计算问题/财务分析.html","title":"财务分析","keywords":"","body":"财务分析 numpy提供了简单的财务分析函数 import numpy as np fv(rate, nper, pmt, pv[, when])求按比率计算n步后的值 例如:现在存100美元,且每月存100美元,假设利率是5%,6%,7%（每月）复利,求10年后的未来价值是多少 a = np.array((0.05, 0.06, 0.07))/12 np.fv(a, 10*12, -100, -100) array([15692.92889434, 16569.87435405, 17509.44688102]) npv(rate, values)净现值 values为现金流量时间序列的价值, 现金流\"事件\"之间的(固定)时间间隔必须与给出费率的时间间隔相同(即如果费率是每年,则恰好一年被理解为在每个现金流事件之间流逝). 按惯例,投资或\"存款\"是负数,收入或\"提款\"是正数; 值必须以初始投资开始,因此值通常为负值. 净现值是一项投资所产生的未来现金流的折现值与项目投资成本之间的差值. 净现值指标是反映项目投资获利能力的指标. 决策标准: 净现值≥0方案可行; 净现值＜0 方案不可行; 净现值均＞0 净现值最大的方案为最优方案. 优点: 考虑了资金时间价值,增强了投资经济性的评价; 考虑了全过程的净现金流量,体现了流动性与收益性的统一; 考虑了投资风险,风险大则采用高折现率,风险小则采用低折现率. 缺点： 净现值的计算较麻烦,难掌握; 净现金流量的测量和折现率较难确定; 不能从动态角度直接反映投资项目的实际收益水平; 项目投资额不等时,无法准确判断方案的优劣. numpy中的净现值使用这个公式 \\sum_{t=0}^{M-1}{\\frac{values_t}{(1+rate)^{t}}} np.npv(0.281,[-100, 39, 59, 55, 20]) -0.00847859163845488 pmt(rate, nper, pv, fv=0, when='end')计算贷款本金加利息的付款 nper是计算次数 pv是本金 np.pmt(0.075/12, 12*15, 200000) -1854.0247200054619 ppmt(rate, per, nper, pv, fv=0.0, when='end') 计算贷款本金的付款 ipmt(rate, per, nper, pv, fv=0.0, when='end') 计算付款的利息部分 irr(values)返回内部收益率(IRR) numpy使用公式 \\sum_{t=0}^M{\\frac{v_t}{(1+irr)^{t}}} = 0 print(round(np.irr([-100, 39, 59, 55, 20]), 5)) print(round(np.irr([-100, 0, 0, 74]), 5)) print(round(np.irr([-100, 100, 0, -7]), 5)) print(round(np.irr([-100, 100, 0, 7]), 5)) print(round(np.irr([-5, 10.5, 1, -8, 1]), 5)) 0.28095 -0.0955 -0.0833 0.06206 0.0886 mirr(values, finance_rate, reinvest_rate)修改后的内部收益率 nper(rate, pmt, pv, fv=0, when='end')计算定期付款的数量 计算公式为: fv + pv*(1+rate)**nper + pmt*(1+rate*when)/rate*((1+rate)**nper-1) = 0 如果rate = 0,那么: fv + pv + pmt*nper = 0 print(np.nper(0.07/12, -150, 8000)) rate(nper, pmt, pv, fv, when='end', guess=0.1, tol=1e-06, maxiter=100)计算每个周期的利率 通过迭代求解(非线性)方程来计算利息率: fv + pv*(1+rate)**nper + pmt*(1+rate*when)/rate * ((1+rate)**nper - 1) = 0 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/","title":"使用pandas处理结构化数据","keywords":"","body":"pandas pandas是基于NumPy的工具,该工具是为了解决数据分析任务而创建的.Pandas纳入了大量库和一些标准的数据模型,提供了高效地操作大型数据集所需的工具,是使Python成为强大而高效的数据分析环境的重要因素之一. 它最初由AQRCapital Management于2008年4月开发,并于2009年底开源出来,目前由专注于Python数据包开发的PyData开发team继续开发和维护,属于PyData项目的一部分.Pandas最初被作为金融数据分析工具而开发出来,因此pandas为时间序列分析提供了很好的支持,只是其特色之一.Pandas的名称来自于面板数据(panel data)和python数据分析(data analysis). Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的序列对象.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的序列对象.html","title":"pandas的序列对象","keywords":"","body":"pandas的序列对象 pandas的序列对象Series是numpy1维数组的封装,带index 创建Series 创建Series有两种方式 通过序列创建,这个序列可以是迭代器,list,dict,也可以是numpy的一维数组,dict创建后key就是它的index,其他序列index默认则是序列的位数 通过固定值创建,创建后每一位都是这个固定值,但这就必须指定index(第二位)参数了 我们也可以用参数index为序列手动指定index import pandas as pd import numpy as np a = pd.Series([1,2,3]) a 0 1 1 2 2 3 dtype: int64 b = pd.Series(np.array([1,2,3])) b 0 1 1 2 2 3 dtype: int64 c = pd.Series({\"a\":1,\"b\":2,\"c\":3}) c a 1 b 2 c 3 dtype: int64 d = pd.Series(4,range(5)) d 0 4 1 4 2 4 3 4 4 4 dtype: int64 序列命名 Series对象可以使用name参数设置名字 s = pd.Series(np.random.randn(5), name='取名字真难') s 0 -0.360848 1 1.470764 2 0.588654 3 -0.055047 4 0.411094 Name: 取名字真难, dtype: float64 元素类型 和numpy的数组一样,Series是同构定长的可迭代数据结构,它可以使用元素dtype查看 a.dtype dtype('int64') 要设置类型,需要使用接口astype(dtype, copy=True, errors='raise', **kwargs) 参数dtype的取值范围是numpy的数组中的dtype和python对象类型的并集,另外额外多一种category类型,标明是分类数据(有限类别) 参数copy标明是在原来对象上修改还是在原来对象内容基础上创建一个新的对象返回 参数errors标明类型转化过程中遇到错误后的处理方式, raise会抛出错误,ignore则会跳过错误,返回原始数据 e = pd.Series([\"a\",\"1\",\"3\"]) e.astype(\"int64\") --------------------------------------------------------------------------- ValueError Traceback (most recent call last) in ----> 1 e.astype(\"int64\") ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors, **kwargs) 5689 # else, only a single dtype is given 5690 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors, -> 5691 **kwargs) 5692 return self._constructor(new_data).__finalize__(self) 5693 ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in astype(self, dtype, **kwargs) 529 530 def astype(self, dtype, **kwargs): --> 531 return self.apply('astype', dtype=dtype, **kwargs) 532 533 def convert(self, **kwargs): ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs) 393 copy=align_copy) 394 --> 395 applied = getattr(b, f)(**kwargs) 396 result_blocks = _extend_blocks(applied, result_blocks) 397 ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors, values, **kwargs) 532 def astype(self, dtype, copy=False, errors='raise', values=None, **kwargs): 533 return self._astype(dtype, copy=copy, errors=errors, values=values, --> 534 **kwargs) 535 536 def _astype(self, dtype, copy=False, errors='raise', values=None, ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py in _astype(self, dtype, copy, errors, values, **kwargs) 631 632 # _astype_nansafe works fine with 1-d only --> 633 values = astype_nansafe(values.ravel(), dtype, copy=True) 634 635 # TODO(extension) ~/Lib/conda/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 681 # work around NumPy brokenness, #1987 682 if np.issubdtype(dtype.type, np.integer): --> 683 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 684 685 # if we have a datetime/timedelta array of objects pandas/_libs/lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: 'a' e.astype(\"int64\",errors=\"ignore\") 0 a 1 1 2 3 dtype: object 元素类型推断 pandas.api.types.infer_dtype()提供了推断数据类型的能力,其返回值可以有 string unicode bytes floating integer mixed-integer mixed-integer-float decimal complex categorical boolean datetime64 datetime date timedelta64 timedelta time period mixed 另外还有专门针对不同类型的判断函数,包括: pandas.api.types.is_bool_dtype() pandas.api.types.is_categorical_dtype() pandas.api.types.is_complex_dtype() pandas.api.types.is_datetime64_any_dtype() pandas.api.types.is_datetime64_dtype() pandas.api.types.is_datetime64_ns_dtype() pandas.api.types.is_datetime64tz_dtype() pandas.api.types.is_extension_array_dtype() pandas.api.types.is_float_dtype() pandas.api.types.is_int64_dtype() pandas.api.types.is_integer_dtype() pandas.api.types.is_interval_dtype() pandas.api.types.is_numeric_dtype() pandas.api.types.is_object_dtype() pandas.api.types.is_period_dtype() pandas.api.types.is_signed_integer_dtype() pandas.api.types.is_string_dtype() pandas.api.types.is_timedelta64_dtype() pandas.api.types.is_timedelta64_ns_dtype() pandas.api.types.is_unsigned_integer_dtype() 迭代 类似list,Series是可迭代对象,直接迭代时抛出的是每一位的值,使用items()接口则会抛出和字典一样的index,value对 for i in enumerate(s): print(i) (0, -0.3608478521120298) (1, 1.4707644792136825) (2, 0.5886541766129135) (3, -0.05504658364902078) (4, 0.41109401352028685) for k,v in c.items(): print(f\"{k}:{v}\") a:1 b:2 c:3 取值 Series取值可以类似字典一样用index取,也可以类似list一样 c[\"a\"] 1 c[1] 2 矢量化操作和标签对齐Series 使用原始numpy数组时通常不需要循环,在pandas中使用Series时也是如此.Series可以使用多数numpy的Universal Functions. b+2 0 3 1 4 2 5 dtype: int64 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的数据框对象.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的数据框对象.html","title":"pandas的数据框对象","keywords":"","body":"pandas的数据框对象 所谓数据框(Dataframe)是这一种最常见的数据分析用的数据结构,它针对的是二维数据.数据帧有如下特点: 列是不同的类型 大小可变 标记轴(行和列) 可以对行和列执行算术运算 数据框是数据科学中一种非常的重要的数据结构,几乎所有的工作都建立在其上 import pandas as pd 数据框的构造 从python对象构造数据框对象可以有如下途径 从二维列表转换 pandas可以将二维列表以表格的形式组合 names = ['Bob','Jessica','Mary','John','Mel'] births = [1968, 1955, 1977,1978, 1973] weight = [69,89,76,90,78] table_o = list(zip(names,births,weight)) table_o [('Bob', 1968, 69), ('Jessica', 1955, 89), ('Mary', 1977, 76), ('John', 1978, 90), ('Mel', 1973, 78)] pd.DataFrame(table_o,columns =[\"name\",\"births\",\"weight\"])# columns指定列标签 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name births weight 0 Bob 1968 69 1 Jessica 1955 89 2 Mary 1977 76 3 John 1978 90 4 Mel 1973 78 从字典中直接生成 pandas也允许将数据放在字典内,这样key是每列的标题,value就会按顺序填入 table_dict = {\"names\":['Bob','Jessica','Mary','John','Mel'], \"births\":[1968, 1955, 1977,1978, 1973], \"weight\":[69,89,76,90,78] } table_dict {'names': ['Bob', 'Jessica', 'Mary', 'John', 'Mel'], 'births': [1968, 1955, 1977, 1978, 1973], 'weight': [69, 89, 76, 90, 78]} pd.DataFrame(table_dict) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } names births weight 0 Bob 1968 69 1 Jessica 1955 89 2 Mary 1977 76 3 John 1978 90 4 Mel 1973 78 从包裹字典的列表中获取 另一种则是按行获取,每个字典是表格中的一行 table_row = [{\"names\":'Bob', \"births\":1968, \"weight\":69 }, {\"names\":'Jessica', \"births\":1955, \"weight\":89 }, {\"names\":'Mary', \"births\":1977, \"weight\":76 }, {\"names\":'John', \"births\":1978, \"weight\":90 }, {\"names\":'Mel', \"births\": 1973, \"weight\":78 }] table_row [{'names': 'Bob', 'births': 1968, 'weight': 69}, {'names': 'Jessica', 'births': 1955, 'weight': 89}, {'names': 'Mary', 'births': 1977, 'weight': 76}, {'names': 'John', 'births': 1978, 'weight': 90}, {'names': 'Mel', 'births': 1973, 'weight': 78}] pd.DataFrame(table_row) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } births names weight 0 1968 Bob 69 1 1955 Jessica 89 2 1977 Mary 76 3 1978 John 90 4 1973 Mel 78 数据框的基本操作 我们以常见的数据集iris作为例子,如何读入外部数据可以看[数据获取与保存]部分 import pandas as pd iris_data = pd.read_csv(\"source/iris.csv\") iris_data[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 数据过滤 就像透视表一样,我们可以有选择性的查看表格 iris_data[iris_data[\"class\"]==\"Iris-virginica\"][::10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 100 6.3 3.3 6.0 2.5 Iris-virginica 110 6.5 3.2 5.1 2.0 Iris-virginica 120 6.9 3.2 5.7 2.3 Iris-virginica 130 7.4 2.8 6.1 1.9 Iris-virginica 140 6.7 3.1 5.6 2.4 Iris-virginica iris_data[iris_data[\"petal_width\"]>iris_data[\"petal_width\"].mean()][::10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 50 7.0 3.2 4.7 1.4 Iris-versicolor 63 6.1 2.9 4.7 1.4 Iris-versicolor 75 6.6 3.0 4.4 1.4 Iris-versicolor 88 5.6 3.0 4.1 1.3 Iris-versicolor 100 6.3 3.3 6.0 2.5 Iris-virginica 110 6.5 3.2 5.1 2.0 Iris-virginica 120 6.9 3.2 5.7 2.3 Iris-virginica 130 7.4 2.8 6.1 1.9 Iris-virginica 140 6.7 3.1 5.6 2.4 Iris-virginica 排序sort 比如我们根据sepal_length做降序排列 biggest5_sl_iris = iris_data.sort_values('sepal_length',ascending=False)[:5] biggest5_sl_iris .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 131 7.9 3.8 6.4 2.0 Iris-virginica 135 7.7 3.0 6.1 2.3 Iris-virginica 122 7.7 2.8 6.7 2.0 Iris-virginica 117 7.7 3.8 6.7 2.2 Iris-virginica 118 7.7 2.6 6.9 2.3 Iris-virginica 再把序号(index)排排序 biggest5_sl_iris.sort_index(ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 135 7.7 3.0 6.1 2.3 Iris-virginica 131 7.9 3.8 6.4 2.0 Iris-virginica 122 7.7 2.8 6.7 2.0 Iris-virginica 118 7.7 2.6 6.9 2.3 Iris-virginica 117 7.7 3.8 6.7 2.2 Iris-virginica 排名rank biggest5_sl_iris.rank(method=\"min\",numeric_only = True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width 131 5.0 4.0 2.0 1.0 135 1.0 3.0 1.0 4.0 122 1.0 2.0 3.0 1.0 117 1.0 4.0 3.0 3.0 118 1.0 1.0 5.0 4.0 选择,切片操作 切片可以用来准确的提取需要的数据 pandas支持多种切片方式 间隔切片 iris_data[::20]#每20行取一次 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 20 5.4 3.4 1.7 0.2 Iris-setosa 40 5.0 3.5 1.3 0.3 Iris-setosa 60 5.0 2.0 3.5 1.0 Iris-versicolor 80 5.5 2.4 3.8 1.1 Iris-versicolor 100 6.3 3.3 6.0 2.5 Iris-virginica 120 6.9 3.2 5.7 2.3 Iris-virginica 140 6.7 3.1 5.6 2.4 Iris-virginica 连续数据段提取 iris_data[5:10]#取第5到第9行 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 提取某一行 iris_data.loc[5] #取第5行的数据 sepal_length 5.4 sepal_width 3.9 petal_length 1.7 petal_width 0.4 class Iris-setosa Name: 5, dtype: object 投影操作 所谓投影和数据库中差不多,就是取列(取属性),简单的方式就是用[]圈住需要的列号或者列名 iris_data[\"sepal_length\"][:5]#取某列 0 5.1 1 4.9 2 4.7 3 4.6 4 5.0 Name: sepal_length, dtype: float64 iris_data.sepal_length[:5]#同样地取某列 0 5.1 1 4.9 2 4.7 3 4.6 4 5.0 Name: sepal_length, dtype: float64 iris_data[[\"sepal_length\",\"petal_width\"]][:5]#取两列 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length petal_width 0 5.1 0.2 1 4.9 0.2 2 4.7 0.2 3 4.6 0.2 4 5.0 0.2 iloc 位置坐标操作 简单粗暴的直接查看对应坐标,第一位参数是行,第二位是列 iris_data.iloc[5]#取第5行的数据 sepal_length 5.4 sepal_width 3.9 petal_length 1.7 petal_width 0.4 class Iris-setosa Name: 5, dtype: object iris_data.iloc[0,2:4]#取第一行第3个数据和第四个数据 petal_length 1.4 petal_width 0.2 Name: 0, dtype: object 增加一列元素 增加一列只需要在原数据上后面用[]填入要新增的元素即可,注意这个操作是对源数据的修改,如果希望源数据不变,先copy再增加 people_fromExcel = pd.read_excel('./source/people.xlsx', u'工作表1', index_col=None, na_values=['NA']) people_Data = people_fromExcel.append(pd.DataFrame([[\"Hao\",24]],columns = [\"name\",\"age\"])).reset_index(drop=True) people_Data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Michael NaN 1 Andy 30.0 2 Justin 19.0 3 Hao 24.0 people_Data[\"nation\"] = [\"USA\",\"UK\",\"AUS\",\"PRC\"] people_Data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age nation 0 Michael NaN USA 1 Andy 30.0 UK 2 Justin 19.0 AUS 3 Hao 24.0 PRC 也可以只输入一个值,这样就全部都是都是它了 people_Data[u\"星球\"] = u\"地球\" people_Data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age nation 星球 0 Michael NaN USA 地球 1 Andy 30.0 UK 地球 2 Justin 19.0 AUS 地球 3 Hao 24.0 PRC 地球 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/复合索引.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/复合索引.html","title":"复合索引","keywords":"","body":"复合索引 除了常规的索引方式,pandas还可以定义复合索引 import pandas as pd import numpy as np 分层索引 分层/多级索引是功能强大的工具,它为一些非常复杂的数据分析和操作,尤其是对于高维数据的处理提供了便利.实际上它使我们能够在诸如Series和DataFrame这样的的低维数据结构中存储和操作具有任意数量维度的数据. 创建分层索引 创建分层索引可以使用 pd.MultiIndex.from_tuples从元祖创建 pd.MultiIndex.from_product当你想要在两个迭代中的每个元素的配对时可以使用 为了方便,可以将数组列表直接传递到Series或DataFrame,以自动构建MultiIndex： arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']] tuples = list(zip(*arrays)) tuples [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')] index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second']) index MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second']) s = pd.Series(np.random.randn(8), index=index) s first second bar one -0.691989 two -1.073631 baz one -1.269384 two 0.700795 foo one 0.337454 two -1.847011 qux one -1.708133 two 0.131374 dtype: float64 iterables = [['bar', 'baz', 'foo', 'qux'], ['one', 'two']] pd.MultiIndex.from_product(iterables, names=['first', 'second']) MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second']) arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux']), np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])] s = pd.Series(np.random.randn(8), index=arrays) s bar one 0.194750 two 1.519968 baz one 0.998856 two -0.513483 foo one -0.108684 two -0.113097 qux one 0.355886 two 1.293657 dtype: float64 df = pd.DataFrame(np.random.randn(8, 4), index=arrays) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 bar one -0.021652 -1.128505 0.696531 -0.132104 two 0.157299 -1.695966 -0.355066 0.346010 baz one 1.377002 0.974853 1.082361 0.861955 two -1.820746 0.746764 0.532095 0.256986 foo one 1.449398 -1.361580 -0.950292 0.102716 two -1.724920 -0.414413 -1.112252 -0.917028 qux one 2.683856 -0.240259 -2.093356 1.078969 two 0.419431 -0.136242 -0.587698 0.483396 将复合索引应用于列 df = pd.DataFrame(np.random.randn(3, 8), index=['A', 'B', 'C'], columns=index) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } first bar baz foo qux second one two one two one two one two A 0.075922 -1.508019 -1.023941 0.142567 -1.790206 0.360418 1.143857 0.411639 B -2.117579 -1.026132 -1.541469 -0.575999 -0.976532 2.045527 0.157497 0.869661 C -1.702369 -0.171502 -0.748726 -0.622308 1.184927 2.208714 0.473766 -0.679159 MultiIndex的重要性在于它允许进行分组,选择和重塑操作.我们将在下面的后续部分中进行描述.你可以发现自己使用分层索引的数据而不需要自己创建一个MultiIndex.但是从文件加载数据时,我们可能希望在准备数据集时生成自己的MultiIndex.可以通过使用pandas.set_printoptions中的multi_sparse参数进行控制显示索引的形式 pd.set_option('display.multi_sparse', False) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } first bar bar baz baz foo foo qux qux second one two one two one two one two A 0.075922 -1.508019 -1.023941 0.142567 -1.790206 0.360418 1.143857 0.411639 B -2.117579 -1.026132 -1.541469 -0.575999 -0.976532 2.045527 0.157497 0.869661 C -1.702369 -0.171502 -0.748726 -0.622308 1.184927 2.208714 0.473766 -0.679159 pd.set_option('display.multi_sparse', True) 重建分级标签 方法get_level_values将返回特定级别上每个位置的标签的向量 index.get_level_values(0) Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='first') index.get_level_values('second') Index(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'], dtype='object', name='second') 使用MultiIndex在轴上进行基本索引 分层索引的一个重要特征是可以通过标识数据中子组的\"部分\"标签来选择数据.部分选择\"丢弃\"层次索引的水平在结果中以一种完全类似的方式选择常规DataFrame中的列: df['bar'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } second one two A 0.075922 -1.508019 B -2.117579 -1.026132 C -1.702369 -0.171502 df['bar', 'one'] A 0.075922 B -2.117579 C -1.702369 Name: (bar, one), dtype: float64 df['bar']['one'] A 0.075922 B -2.117579 C -1.702369 Name: one, dtype: float64 s['qux'] one 0.355886 two 1.293657 dtype: float64 df.columns MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second']) df[['foo','qux']].columns MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[2, 2, 3, 3], [0, 1, 0, 1]], names=['first', 'second']) 这样做是为了避免重新计算水平以便使切片具有高性能.如果我们想看到实际使用的index层级 df[['foo','qux']].columns.values array([('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], dtype=object) df[['foo','qux']].columns.get_level_values(0) Index(['foo', 'foo', 'qux', 'qux'], dtype='object', name='first') pd.MultiIndex.from_tuples(df[['foo','qux']].columns.values) MultiIndex(levels=[['foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1], [0, 1, 0, 1]]) 数据对齐和使用reindex 在轴上具有多索引的不同索引对象之间的操作将像下面演示的这样,数据对齐将像元组索引一样工作: s + s[:-2] bar one 0.389501 two 3.039936 baz one 1.997712 two -1.026967 foo one -0.217368 two -0.226194 qux one NaN two NaN dtype: float64 s + s[::2] bar one 0.389501 two NaN baz one 1.997712 two NaN foo one -0.217368 two NaN qux one 0.711773 two NaN dtype: float64 reindex可以用另一个MultiIndex或甚至一个元组的列表或数组调用: s.reindex(index[:3]) first second bar one 0.194750 two 1.519968 baz one 0.998856 dtype: float64 s.reindex([('foo', 'two'), ('bar', 'one'), ('qux', 'one'), ('baz', 'one')]) foo two -0.113097 bar one 0.194750 qux one 0.355886 baz one 0.998856 dtype: float64 高级索引与层次索引 使用.loc在高级索引中语法集成MultiIndex有点具有挑战性,但我们已尽一切努力这样做.例如下面： df = df.T df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second bar one 0.075922 -2.117579 -1.702369 two -1.508019 -1.026132 -0.171502 baz one -1.023941 -1.541469 -0.748726 two 0.142567 -0.575999 -0.622308 foo one -1.790206 -0.976532 1.184927 two 0.360418 2.045527 2.208714 qux one 1.143857 0.157497 0.473766 two 0.411639 0.869661 -0.679159 df.loc['bar'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C second one 0.075922 -2.117579 -1.702369 two -1.508019 -1.026132 -0.171502 df.loc['bar', 'two'] A -1.508019 B -1.026132 C -0.171502 Name: (bar, two), dtype: float64 df.loc['baz':'foo'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second baz one -1.023941 -1.541469 -0.748726 two 0.142567 -0.575999 -0.622308 foo one -1.790206 -0.976532 1.184927 two 0.360418 2.045527 2.208714 你可以通过提供一个元组的切片使用一个\"范围\"的值. df.loc[('baz', 'two'):('qux', 'one')] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second baz two 0.142567 -0.575999 -0.622308 foo one -1.790206 -0.976532 1.184927 two 0.360418 2.045527 2.208714 qux one 1.143857 0.157497 0.473766 df.loc[('baz', 'two'):'foo'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second baz two 0.142567 -0.575999 -0.622308 foo one -1.790206 -0.976532 1.184927 two 0.360418 2.045527 2.208714 传递标签或元组的列表与重建索引类似: df.loc[[('bar', 'two'), ('qux', 'one')]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second bar two -1.508019 -1.026132 -0.171502 qux one 1.143857 0.157497 0.473766 使用swaplevel()交换index的层级 df[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C first second bar one 0.075922 -2.117579 -1.702369 two -1.508019 -1.026132 -0.171502 baz one -1.023941 -1.541469 -0.748726 two 0.142567 -0.575999 -0.622308 foo one -1.790206 -0.976532 1.184927 df[:5].swaplevel(0, 1, axis=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C second first one bar 0.075922 -2.117579 -1.702369 two bar -1.508019 -1.026132 -0.171502 one baz -1.023941 -1.541469 -0.748726 two baz 0.142567 -0.575999 -0.622308 one foo -1.790206 -0.976532 1.184927 使用reorder_levels()重新排序层级 df[:5].reorder_levels([1,0], axis=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C second first one bar 0.075922 -2.117579 -1.702369 two bar -1.508019 -1.026132 -0.171502 one baz -1.023941 -1.541469 -0.748726 two baz 0.142567 -0.575999 -0.622308 one foo -1.790206 -0.976532 1.184927 CategoricalIndex 介绍一个层级类型CategoricalIndex,一种新的索引类型,用于支持索引与重复.这是围绕分类类型数据的容器,并且允许对具有大量重复元素的索引进行有效的索引和存储. df = pd.DataFrame({'A': np.arange(6), 'B': list('aabbca')}) df['B'] = df['B'].astype('category', categories=list('cab')) /Users/huangsizhe/Lib/conda/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3325: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead exec(code_obj, self.user_global_ns, self.user_ns) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0 a 1 1 a 2 2 b 3 3 b 4 4 c 5 5 a df.dtypes A int64 B category dtype: object df.B.cat.categories Index(['c', 'a', 'b'], dtype='object') 设置索引将其创建为一个CategoricalIndex df2 = df.set_index('B') df2.index CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category') df2.loc['a'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B a 0 a 1 a 5 这些保留了分类索引 df2.loc['a'].index CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category') df2.sort_index() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B c 4 a 0 a 1 a 5 b 2 b 3 索引上的Groupby操作也将保留索引本质 df2.groupby(level=0).sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B c 4 a 6 b 5 df2.groupby(level=0).sum().index CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category') 重索引操作将根据传递的索引器的类型返回一个结果索引,这意味着传递一个列表将返回一个普通的索引;使用分类索引将返回CategoricalIndex,根据PASSED分类类型的类别索引.这允许任意索引这些甚至与不在类别中的值,类似于您可以重新索引任何pandas索引. df2.reindex(['a','e']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B a 0.0 a 1.0 a 5.0 e NaN df2.reindex(['a','e']).index Index(['a', 'a', 'a', 'e'], dtype='object', name='B') df2.reindex(pd.Categorical(['a','e'],categories=list('abcde'))) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B a 0.0 a 1.0 a 5.0 e NaN Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/分组与集聚.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/分组与集聚.html","title":"分组与集聚","keywords":"","body":"分组与集聚 我们处理数据有时候会有分组的需求,比如统计的是全年龄段的人的,但我们可能会按年龄分组成老中青三组,可能会分成男女两组. 也有时候我们统计一份问卷,需要将数据转置 import pandas as pd import numpy as np 转置 比如我们有这样的一组多选问卷结果统计 d = {'a':[1,0,0,1,0,1,1,0,1,0], 'b':[0,0,1,1,1,0,1,0,1,0], \"c\":[1,0,0,0,0,1,0,1,0,0], \"d\":[1,0,0,1,0,1,1,0,1,1]} i = [\"no.{n}\".format(n=i) for i in range(10)] df = pd.DataFrame(data = d, index = i) df a b c d no.0 1 0 1 1 no.1 0 0 0 0 no.2 0 1 0 0 no.3 1 1 0 1 no.4 0 1 0 0 no.5 1 0 1 1 no.6 1 1 0 1 no.7 0 0 1 0 no.8 1 1 0 1 no.9 0 0 0 1 转置只需要使用T方法 df.T no.0 no.1 no.2 no.3 no.4 no.5 no.6 no.7 no.8 no.9 a 1 0 0 1 0 1 1 0 1 0 b 0 0 1 1 1 0 1 0 1 0 c 1 0 0 0 0 1 0 1 0 0 d 1 0 0 1 0 1 1 0 1 1 堆积操作 还是之前的多选问题,如果我们想把它堆积起来成为一个有多重索引的序列,可以使用stack()方法 stack = df.stack() stack no.0 a 1 b 0 c 1 d 1 no.1 a 0 b 0 c 0 d 0 no.2 a 0 b 1 c 0 d 0 no.3 a 1 b 1 c 0 d 1 no.4 a 0 b 1 c 0 d 0 no.5 a 1 b 0 c 1 d 1 no.6 a 1 b 1 c 0 d 1 no.7 a 0 b 0 c 1 d 0 no.8 a 1 b 1 c 0 d 1 no.9 a 0 b 0 c 0 d 1 dtype: int64 stack[\"no.0\"] a 1 b 0 c 1 d 1 dtype: int64 也可以使用unstack汇总每个选项不同题目的结果 unstack = df.unstack() unstack a no.0 1 no.1 0 no.2 0 no.3 1 no.4 0 no.5 1 no.6 1 no.7 0 no.8 1 no.9 0 b no.0 0 no.1 0 no.2 1 no.3 1 no.4 1 no.5 0 no.6 1 no.7 0 no.8 1 no.9 0 c no.0 1 no.1 0 no.2 0 no.3 0 no.4 0 no.5 1 no.6 0 no.7 1 no.8 0 no.9 0 d no.0 1 no.1 0 no.2 0 no.3 1 no.4 0 no.5 1 no.6 1 no.7 0 no.8 1 no.9 1 dtype: int64 unstack[\"a\"] no.0 1 no.1 0 no.2 0 no.3 1 no.4 0 no.5 1 no.6 1 no.7 0 no.8 1 no.9 0 dtype: int64 groupby groupby的功能类似SQL的group by关键字: Split-Apply-Combine Split,就是按照规则分组 Apply,通过⼀一定的agg函数来获得输⼊入pd.Series返回⼀一个值的效果 Combine,把结果收集起来 Pandas的groupby的灵活性: 分组的关键字可以来⾃自于index,也可以来⾃自于真实的列数据 分组规则可以通过⼀一列或者多列 iris_data = pd.read_csv(\"./source/iris.data\",header = None,encoding = \"utf-8\", names=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\",\"class\"]) iris_data[:5] sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa iris_group = iris_data.groupby(\"class\") iris_group.sum() sepal_length sepal_width petal_length petal_width class Iris-setosa 250.3 170.9 73.2 12.2 Iris-versicolor 296.8 138.5 213.0 66.3 Iris-virginica 329.4 148.7 277.6 101.3 iris_group.mean() sepal_length sepal_width petal_length petal_width class Iris-setosa 5.006 3.418 1.464 0.244 Iris-versicolor 5.936 2.770 4.260 1.326 Iris-virginica 6.588 2.974 5.552 2.026 for level,subset in iris_group: print(level) print(subset[:5]) Iris-setosa sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa Iris-versicolor sepal_length sepal_width petal_length petal_width class 50 7.0 3.2 4.7 1.4 Iris-versicolor 51 6.4 3.2 4.5 1.5 Iris-versicolor 52 6.9 3.1 4.9 1.5 Iris-versicolor 53 5.5 2.3 4.0 1.3 Iris-versicolor 54 6.5 2.8 4.6 1.5 Iris-versicolor Iris-virginica sepal_length sepal_width petal_length petal_width class 100 6.3 3.3 6.0 2.5 Iris-virginica 101 5.8 2.7 5.1 1.9 Iris-virginica 102 7.1 3.0 5.9 2.1 Iris-virginica 103 6.3 2.9 5.6 1.8 Iris-virginica 104 6.5 3.0 5.8 2.2 Iris-virginica 由此可见实际上groupby将表格拆分成了一组(分组名,子表)的键值对 之后的操作可以有: agg()方法 agg方法 是将由子表构成的序列作为参数操作,要求操作可以每个子表返回一个非序列的返回值,操作完成后生成新的表格 iris_group.agg(lambda x :\"好\") sepal_length sepal_width petal_length petal_width class Iris-setosa 好 好 好 好 Iris-versicolor 好 好 好 好 Iris-virginica 好 好 好 好 transform() transform方法对子表序列运算方法,分别运算完后结果放回对应的行,也就是说原来的表什么样算完结构一样但内容不一样了,和map有点像,但运算的时候序列不是1个总序列而是多个分开的子序列 iris_group.transform(lambda x:x - x.mean())[:5] sepal_length sepal_width petal_length petal_width 0 0.094 0.082 -0.064 -0.044 1 -0.106 -0.418 -0.064 -0.044 2 -0.306 -0.218 -0.164 -0.044 3 -0.406 -0.318 0.036 -0.044 4 -0.006 0.182 -0.064 -0.044 Categorical类型 现在pandas可以使用Categorical类型了, iris_data[:5] sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa iris_data[\"class\"] = iris_data[\"class\"].astype(\"category\") iris_data[\"class\"][::20] 0 Iris-setosa 20 Iris-setosa 40 Iris-setosa 60 Iris-versicolor 80 Iris-versicolor 100 Iris-virginica 120 Iris-virginica 140 Iris-virginica Name: class, dtype: category Categories (3, object): [Iris-setosa, Iris-versicolor, Iris-virginica] 聚集 聚集操作一般是将多组数据合并到一张表格中 比如连接表格,我们可以使用函数concat([tables...]) df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3]) df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'], 'C': ['C4', 'C5', 'C6', 'C7'], 'D': ['D4', 'D5', 'D6', 'D7']}, index=[4, 5, 6, 7]) df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'], 'B': ['B8', 'B9', 'B10', 'B11'], 'C': ['C8', 'C9', 'C10', 'C11'], 'D': ['D8', 'D9', 'D10', 'D11']}, index=[8, 9, 10, 11]) df1 A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 df2 A B C D 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 df3 A B C D 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 result = pd.concat([df1,df2,df3]) result A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,keys=None, levels=None, names=None,verify_integrity=False,copy=True)是concat的完整参数说明, objs：一个序列或dict，dataframe，或Panel对象。 axis：{ 0，1，…}，默认值0。将沿哪个轴操作。 join：{'inner', 'outer'}，默认为'outer'。如何处理其他轴的索引,outer为求并,inner为求交集. ignore_index：布尔值，默认为False。如果是True，则不使用索引值用于连接轴。由此产生的轴将被标记为(0，…，N - 1).常用于连接对象，连接轴为没有意义的索引信息。注意,其他轴上的索引值仍然在联接中有用. join_axes：索引对象的列表。用于其他n - 1轴而不执行内/外集逻辑的特定索引. keys：序列，默认无。使用传递key作为最外层级别构造分层索引.。如果多个层面通过，应包含元组。级别：序列列表，默认没有。具体水平（独特价值）用于构建多指标。否则，他们将从key推断。 names：列表，默认无。生成层次索引中的level的名称。 verify_integrity：布尔值，默认为false。检查新的连接轴是否包含重复。这相对真实的数据连接开销很大。 拷贝：布尔值，默认值为。如果FALSE，不复制数据 为每组添加外层索引 pd.concat([df1,df2,df3], keys=['x', 'y', 'z']) A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 pd.concat({'x': df1, 'y': df2, 'z': df3}) A B C D x 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 y 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 z 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 设置索引 df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'], 'D': ['D2', 'D3', 'D6', 'D7'], 'F': ['F2', 'F3', 'F6', 'F7']}, index=[2, 3, 6, 7]) df4 B D F 2 B2 D2 F2 3 B3 D3 F3 6 B6 D6 F6 7 B7 D7 F7 横向聚集 pd.concat([df1, df4], axis=1) A B C D B D F 0 A0 B0 C0 D0 NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3 6 NaN NaN NaN NaN B6 D6 F6 7 NaN NaN NaN NaN B7 D7 F7 使用交集 pd.concat([df1, df4], axis=1, join='inner') A B C D B D F 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3 只合并特定的索引 pd.concat([df1, df4], axis=1, join_axes=[df1.index]) A B C D B D F 0 A0 B0 C0 D0 NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN 2 A2 B2 C2 D2 B2 D2 F2 3 A3 B3 C3 D3 B3 D3 F3 除了向下连接行,同样可以向右连接列序列 s3 = pd.Series([0, 1, 2, 3], name='foo') s4 = pd.Series([0, 1, 2, 3]) s5 = pd.Series([0, 1, 4, 5]) pd.concat([s3, s4, s5], axis=1) foo 0 1 0 0 0 0 1 1 1 1 2 2 2 4 3 3 3 5 pd.concat([s3, s4, s5], axis=1, keys=['red','blue','yellow']) red blue yellow 0 0 0 0 1 1 1 1 2 2 2 4 3 3 3 5 使用append方法链式集聚 append(other, ignore_index=False, verify_integrity=False)并不想contact那样可以自己设定很多 df1.append(df2).append(df3) A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 df1.append([df2,df3]) A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 8 A8 B8 C8 D8 9 A9 B9 C9 D9 10 A10 B10 C10 D10 11 A11 B11 C11 D11 df1.append(df4) A B C D F 0 A0 B0 C0 D0 NaN 1 A1 B1 C1 D1 NaN 2 A2 B2 C2 D2 NaN 3 A3 B3 C3 D3 NaN 2 NaN B2 NaN D2 F2 3 NaN B3 NaN D3 F3 6 NaN B6 NaN D6 F6 7 NaN B7 NaN D7 F7 使用joining/merging类似在数据中一样的操作 merging 有经验的关系型数据库用户都熟悉用于描述连接两个SQL表的术语merging。有几种情况要考虑： 一对一的连接 多对一的连接 多对多联接 pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=('_x', '_y'), copy=True, indicator=False) on 连接的列名。必须同时在左边和右边的df对象中。 left_on/right_on 用左边/右边的某列作为key。可以是列名或长度等于DataFrame长度的列 left_index/right_index 如果是真的，使用索引（行标签）从左/右边的key加入。在一个多指标数据帧的情况下，数量必须匹配从右/左变key加入的数量 how 如何merging,有“left”，“right”，“outer”，“inner”可选。默认为inner Merge method SQL Join Name Description left LEFT OUTER JOIN Use keys from left frame only right RIGHT OUTER JOIN Use keys from right frame only outer FULL OUTER JOIN Use union of keys from both frames inner INNER JOIN Use intersection of keys from both frames sort 将结果排序。默认为true，通常设置为FALSE将大大改善性能 suffixes 后缀 indicator 用于指示merge的行为,如果是真的，一个范畴类型列为_merge将被添加到输出对象需要的值 Observation Origin _merge value Merge key only in 'left' frame left_only Merge key only in 'right' frame right_only Merge key in both frames both 最一般的连接,通过key列连接 left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']}) left A B key 0 A0 B0 K0 1 A1 B1 K1 2 A2 B2 K2 3 A3 B3 K3 right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}) right C D key 0 C0 D0 K0 1 C1 D1 K1 2 C2 D2 K2 3 C3 D3 K3 result = pd.merge(left, right, on='key') result A B key C D 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K2 C2 D2 3 A3 B3 K3 C3 D3 不同连接方式的不同结果 left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'], 'key2': ['K0', 'K1', 'K0', 'K1'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']}) left A B key1 key2 0 A0 B0 K0 K0 1 A1 B1 K0 K1 2 A2 B2 K1 K0 3 A3 B3 K2 K1 right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'], 'key2': ['K0', 'K0', 'K0', 'K0'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}) right C D key1 key2 0 C0 D0 K0 K0 1 C1 D1 K1 K0 2 C2 D2 K1 K0 3 C3 D3 K2 K0 pd.merge(left, right, on=['key1', 'key2']) A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A2 B2 K1 K0 C1 D1 2 A2 B2 K1 K0 C2 D2 pd.merge(left, right, how='left', on=['key1', 'key2']) A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A1 B1 K0 K1 NaN NaN 2 A2 B2 K1 K0 C1 D1 3 A2 B2 K1 K0 C2 D2 4 A3 B3 K2 K1 NaN NaN pd.merge(left, right, how='right', on=['key1', 'key2']) A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A2 B2 K1 K0 C1 D1 2 A2 B2 K1 K0 C2 D2 3 NaN NaN K2 K0 C3 D3 pd.merge(left, right, how='outer', on=['key1', 'key2']) A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A1 B1 K0 K1 NaN NaN 2 A2 B2 K1 K0 C1 D1 3 A2 B2 K1 K0 C2 D2 4 A3 B3 K2 K1 NaN NaN 5 NaN NaN K2 K0 C3 D3 pd.merge(left, right, how='inner', on=['key1', 'key2']) A B key1 key2 C D 0 A0 B0 K0 K0 C0 D0 1 A2 B2 K1 K0 C1 D1 2 A2 B2 K1 K0 C2 D2 df1 = pd.DataFrame({'col1': [0, 1], 'col_left':['a', 'b']}) df1 col1 col_left 0 0 a 1 1 b df2 = pd.DataFrame({'col1': [1, 2, 2],'col_right':[2, 2, 2]}) df2 col1 col_right 0 1 2 1 2 2 2 2 2 pd.merge(df1, df2, on='col1', how='outer', indicator=True) col1 col_left col_right _merge 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column') col1 col_left col_right indicator_column 0 0 a NaN left_only 1 1 b 2.0 both 2 2 NaN 2.0 right_only 3 2 NaN 2.0 right_only join方法 join之于merge就像上面的append之于contact,是一种简便方法 left = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=['K0', 'K1', 'K2']) right = pd.DataFrame({'C': ['C0', 'C2', 'C3'], 'D': ['D0', 'D2', 'D3']}, index=['K0', 'K2', 'K3']) left A B K0 A0 B0 K1 A1 B1 K2 A2 B2 right C D K0 C0 D0 K2 C2 D2 K3 C3 D3 left.join(right) A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 left.join(right, how='outer') A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 K3 NaN NaN C3 D3 left.join(right, how='inner') A B C D K0 A0 B0 C0 D0 K2 A2 B2 C2 D2 除了使用index索引作为key外,join也可以指定key left.join(right, on=key_or_keys) left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'key': ['K0', 'K1', 'K0', 'K1']}) right = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']}, index=['K0', 'K1']) left A B key 0 A0 B0 K0 1 A1 B1 K1 2 A2 B2 K0 3 A3 B3 K1 right C D K0 C0 D0 K1 C1 D1 left.join(right, on='key') A B key C D 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K0 C0 D0 3 A3 B3 K1 C1 D1 针对多索引的join left = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=pd.Index(['K0', 'K1', 'K2'], name='key')) index = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'), ('K2', 'Y2'), ('K2', 'Y3')], names=['key', 'Y']) right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=index) left A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 right C D key Y K0 Y0 C0 D0 K1 Y1 C1 D1 K2 Y2 C2 D2 Y3 C3 D3 index MultiIndex(levels=[['K0', 'K1', 'K2'], ['Y0', 'Y1', 'Y2', 'Y3']], labels=[[0, 1, 2, 2], [0, 1, 2, 3]], names=['key', 'Y']) left.join(right, how='inner') A B C D key Y K0 Y0 A0 B0 C0 D0 K1 Y1 A1 B1 C1 D1 K2 Y2 A2 B2 C2 D2 Y3 A2 B2 C3 D3 index = pd.MultiIndex.from_tuples([('K0', 'X0'), ('K0', 'X1'), ('K1', 'X2')], names=['key', 'X']) left = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=index) left A B key X K0 X0 A0 B0 X1 A1 B1 K1 X2 A2 B2 pd.merge(left.reset_index(), right.reset_index(), on=['key'], how='inner').set_index(['key','X','Y']) A B C D key X Y K0 X0 Y0 A0 B0 C0 D0 X1 Y0 A1 B1 C0 D0 K1 X2 Y1 A2 B2 C1 D1 \"打补丁\" 另一个相当普遍的情况是有两个像索引（或类似的索引）系列或数据帧的对象，一个想在另一个上\"打补丁\",把空值填上,这时候可以使用.combine_first方法 df1 = pd.DataFrame([[np.nan, 3., 5.], [-4.6, np.nan, np.nan], [np.nan, 7., np.nan]]) df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5., 1.6, 4]], index=[1, 2]) df1 0 1 2 0 NaN 3.0 5.0 1 -4.6 NaN NaN 2 NaN 7.0 NaN df2 0 1 2 1 -42.6 NaN -8.2 2 -5.0 1.6 4.0 df1.combine_first(df2) 0 1 2 0 NaN 3.0 5.0 1 -4.6 NaN -8.2 2 -5.0 7.0 4.0 注意这时候原来有值得地方并不会被替换 也可以使用update方法用df2的值替换df1中的对应值,这时是修改df1而不是生成新的表 df1.update(df2) df1 0 1 2 0 NaN 3.0 5.0 1 -42.6 NaN -8.2 2 -5.0 1.6 4.0 时间序列处理 merge_ordered merge_ordered()函数允许组合时间序列和其他有序数据。特别是它有一个可选的fill_method关键字来填充/内插缺失的数据： left = pd.DataFrame({'k': ['K0', 'K1', 'K1', 'K2'], 'lv': [1, 2, 3, 4], 's': ['a', 'b', 'c', 'd']}) right = pd.DataFrame({'k': ['K1', 'K2', 'K4'], 'rv': [1, 2, 3]}) left k lv s 0 K0 1 a 1 K1 2 b 2 K1 3 c 3 K2 4 d right k rv 0 K1 1 1 K2 2 2 K4 3 pd.merge_ordered(left, right, fill_method='ffill', left_by='s') k lv s rv 0 K0 1.0 a NaN 1 K1 1.0 a 1.0 2 K2 1.0 a 2.0 3 K4 1.0 a 3.0 4 K1 2.0 b 1.0 5 K2 2.0 b 2.0 6 K4 2.0 b 3.0 7 K1 3.0 c 1.0 8 K2 3.0 c 2.0 9 K4 3.0 c 3.0 10 K1 NaN d 1.0 11 K2 4.0 d 2.0 12 K4 4.0 d 3.0 merge_asof merge_asof()在行为上,除了匹配最相近的值而不是相等的值这点外,类似left-join. asof合并可以执行分组合并。除了on键上最接近的匹配之外，这与by键地位相似。 trades = pd.DataFrame({ 'time': pd.to_datetime(['20160525 13:30:00.023', '20160525 13:30:00.038', '20160525 13:30:00.048', '20160525 13:30:00.048', '20160525 13:30:00.048']), 'ticker': ['MSFT', 'MSFT', 'GOOG', 'GOOG', 'AAPL'], 'price': [51.95, 51.95, 720.77, 720.92, 98.00], 'quantity': [75, 155, 100, 100, 100]}, columns=['time', 'ticker', 'price', 'quantity']) quotes = pd.DataFrame({ 'time': pd.to_datetime(['20160525 13:30:00.023', '20160525 13:30:00.023', '20160525 13:30:00.030', '20160525 13:30:00.041', '20160525 13:30:00.048', '20160525 13:30:00.049', '20160525 13:30:00.072', '20160525 13:30:00.075']), 'ticker': ['GOOG', 'MSFT', 'MSFT', 'MSFT', 'GOOG', 'AAPL', 'GOOG', 'MSFT'], 'bid': [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01], 'ask': [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]}, columns=['time', 'ticker', 'bid', 'ask']) trades time ticker price quantity 0 2016-05-25 13:30:00.023 MSFT 51.95 75 1 2016-05-25 13:30:00.038 MSFT 51.95 155 2 2016-05-25 13:30:00.048 GOOG 720.77 100 3 2016-05-25 13:30:00.048 GOOG 720.92 100 4 2016-05-25 13:30:00.048 AAPL 98.00 100 quotes time ticker bid ask 0 2016-05-25 13:30:00.023 GOOG 720.50 720.93 1 2016-05-25 13:30:00.023 MSFT 51.95 51.96 2 2016-05-25 13:30:00.030 MSFT 51.97 51.98 3 2016-05-25 13:30:00.041 MSFT 51.99 52.00 4 2016-05-25 13:30:00.048 GOOG 720.50 720.93 5 2016-05-25 13:30:00.049 AAPL 97.99 98.01 6 2016-05-25 13:30:00.072 GOOG 720.50 720.88 7 2016-05-25 13:30:00.075 MSFT 52.01 52.03 pd.merge_asof(trades, quotes, on='time', by='ticker') time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 51.95 51.96 1 2016-05-25 13:30:00.038 MSFT 51.95 155 51.97 51.98 2 2016-05-25 13:30:00.048 GOOG 720.77 100 720.50 720.93 3 2016-05-25 13:30:00.048 GOOG 720.92 100 720.50 720.93 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN pd.merge_asof(trades, quotes, on='time', by='ticker', tolerance=pd.Timedelta('2ms')) time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 51.95 51.96 1 2016-05-25 13:30:00.038 MSFT 51.95 155 NaN NaN 2 2016-05-25 13:30:00.048 GOOG 720.77 100 720.50 720.93 3 2016-05-25 13:30:00.048 GOOG 720.92 100 720.50 720.93 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN pd.merge_asof(trades, quotes, on='time', by='ticker', tolerance=pd.Timedelta('10ms'), allow_exact_matches=False) time ticker price quantity bid ask 0 2016-05-25 13:30:00.023 MSFT 51.95 75 NaN NaN 1 2016-05-25 13:30:00.038 MSFT 51.95 155 51.97 51.98 2 2016-05-25 13:30:00.048 GOOG 720.77 100 NaN NaN 3 2016-05-25 13:30:00.048 GOOG 720.92 100 NaN NaN 4 2016-05-25 13:30:00.048 AAPL 98.00 100 NaN NaN Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的函数操作.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/pandas的函数操作.html","title":"pandas的函数操作","keywords":"","body":"pandas的函数操作 由于python本身对函数式编程的支持,以及pandas底层依赖的numpy优秀的向量化计算能力,pandas可以使用类似Universal Function的方式向量化的求值. 本文例子依然使用iris数据集 import pandas as pd iris_data = pd.read_csv(\"source/iris.csv\") iris_data[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 例:求出iris三类的信息熵 import scipy as sp slogs = lambda x:sp.log(x)*x entropy = lambda x:sp.exp((slogs(x.sum())-x.map(slogs).sum())/x.sum()) iris_data.groupby(\"class\").agg(entropy) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class Iris-setosa 49.878745 49.695242 49.654909 45.810069 Iris-versicolor 49.815081 49.680665 49.694505 49.452305 Iris-virginica 49.772059 49.714500 49.761700 49.545918 广播 所谓广播就是一个矢量和一个标量的运算,所有矢量中元素都被同样的操作,pandas可以支持这种操作 data1 = iris_data[:10].copy() data1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa data1*10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 51.0 35.0 14.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 1 49.0 30.0 14.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 2 47.0 32.0 13.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 3 46.0 31.0 15.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 4 50.0 36.0 14.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 5 54.0 39.0 17.0 4.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 6 46.0 34.0 14.0 3.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 7 50.0 34.0 15.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 8 44.0 29.0 14.0 2.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... 9 49.0 31.0 15.0 1.0 Iris-setosaIris-setosaIris-setosaIris-setosaIr... data1[\"sepal_length\"]*10 0 51.0 1 49.0 2 47.0 3 46.0 4 50.0 5 54.0 6 46.0 7 50.0 8 44.0 9 49.0 Name: sepal_length, dtype: float64 使用numpy的universal functiion import numpy as np np.exp(data1[\"sepal_length\"]) f_npexp = np.frompyfunc(lambda x :np.exp(x)+1,1,1) f_npexp(data1[\"sepal_length\"]) 0 165.022 1 135.29 2 110.947 3 100.484 4 149.413 5 222.406 6 100.484 7 149.413 8 82.4509 9 135.29 Name: sepal_length, dtype: object Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/基本的统计分析.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/基本的统计分析.html","title":"基本的统计分析","keywords":"","body":"基本的统计分析 pandas本身定位是表格工具,算法不是他的主要目标,所以他内置的算法只是坎坎够用,pandas本身依赖numpy,因此numpy有的统计方法他都有,比如观察他的均值方差标准差什么的,本文依然使用iris来作为源数据 import pandas as pd import matplotlib.pyplot as plt %matplotlib inline iris_data = pd.read_csv(\"source/iris.csv\") 基本的统计功能 pandas内置基本的统计功能 函数 作用 count 非NA值数量 describe 汇总统计 mean 求均值 min/max 最小最大值 argmin/argmax 获取最小最大值的index位置 idxmin/idxmax 获取最小最大值的index quantile 计算分位数 sum 求和 median 中位数 mad 根据均值计算平局绝对离差 var 方差 std 标准差 skew 偏度(三阶矩) kurt 锋度(四阶矩) cumsum 累计和 cummin/cummax 累计最小值累计最大值 cumprod 累计积 diff 一阶差分(对时间序列很有用) pct_change 百分数变化 corr 相关系数 cov 协方差 iris_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 iris_data[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]].pct_change()[1:].tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width 145 0.000000 -0.090909 -0.087719 -0.080000 146 -0.059701 -0.166667 -0.038462 -0.173913 147 0.031746 0.200000 0.040000 0.052632 148 -0.046154 0.133333 0.038462 0.150000 149 -0.048387 -0.117647 -0.055556 -0.217391 iris_data[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]].pct_change()[1:].sepal_length.corr(iris_data.petal_length) 0.15569820981689295 iris_data[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]].corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width sepal_length 1.000000 -0.109369 0.871754 0.817954 sepal_width -0.109369 1.000000 -0.420516 -0.356544 petal_length 0.871754 -0.420516 1.000000 0.962757 petal_width 0.817954 -0.356544 0.962757 1.000000 iris_data[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]].cov() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width sepal_length 0.685694 -0.039268 1.273682 0.516904 sepal_width -0.039268 0.188004 -0.321713 -0.117981 petal_length 1.273682 -0.321713 3.113179 1.296387 petal_width 0.516904 -0.117981 1.296387 0.582414 抽样 抽样的话,pandas提供了sample()方法可以做简单的抽样,你可以选择是有放回还是无放回的 iris_data_test=iris_data.sample(frac=0.4) iris_data_test = iris_data_test.sort_index() iris_data_test[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 剩余的数据可以这样得到 iris_data_train=iris_data.drop(iris_data_test.index) iris_data_train[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 10 5.4 3.7 1.5 0.2 Iris-setosa 也可以设定别的你自己的抽样方式,比如我觉得我希望用每行数据摇色子的方式确定是否进入样本,那么可以这样 import random temp = iris_data.copy() temp[\"cc\"]=[random.random() for i in range(len(iris_data))] len(iris_data[temp[\"cc\"]>0.3]) 107 len(iris_data[temp[\"cc\"] 43 相关性 numpy只默认支持协方差矩阵的计算 他们都可以带参数min_periods关键字，该关键字为每个列对指定所需的最小观测值数，以获得有效的结果 协方差矩阵 iris_copy = iris_data.copy() iris_cov = iris_copy[iris_copy.columns[:-1]].T.cov() iris_cov[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 140 141 142 143 144 145 146 147 148 149 0 4.750000 4.421667 4.353333 4.160000 4.696667 4.860000 4.215000 4.595000 3.9650 4.493333 ... 2.650000 3.090000 2.341667 2.730 2.596667 2.850000 2.741667 2.915000 2.475000 2.600000 1 4.421667 4.149167 4.055000 3.885000 4.358333 4.515000 3.907500 4.284167 3.7075 4.210000 ... 2.725000 3.128333 2.409167 2.805 2.661667 2.906667 2.820833 2.955833 2.504167 2.628333 2 4.353333 4.055000 3.990000 3.813333 4.303333 4.453333 3.861667 4.211667 3.6350 4.120000 ... 2.446667 2.850000 2.161667 2.520 2.396667 2.630000 2.531667 2.688333 2.281667 2.396667 3 4.160000 3.885000 3.813333 3.656667 4.110000 4.256667 3.688333 4.031667 3.4850 3.953333 ... 2.493333 2.856667 2.218333 2.580 2.443333 2.653333 2.571667 2.718333 2.321667 2.440000 4 4.696667 4.358333 4.303333 4.110000 4.650000 4.810000 4.175000 4.541667 3.9150 4.433333 ... 2.530000 2.963333 2.238333 2.610 2.483333 2.726667 2.615000 2.798333 2.381667 2.503333 5 rows × 150 columns 皮尔逊相关度 这个可以使用numpy来求了 import numpy as np iris_copy = iris_data.copy() iris_ = iris_copy[iris_copy.columns[:-1]] pd.DataFrame(np.corrcoef(iris_.values))[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 140 141 142 143 144 145 146 147 148 149 0 1.000000 0.995999 0.999974 0.998168 0.999347 0.999586 0.998811 0.999538 0.998077 0.996552 ... 0.597825 0.685581 0.574649 0.584668 0.603048 0.646865 0.605998 0.653473 0.633917 0.633158 1 0.995999 1.000000 0.996607 0.997397 0.992233 0.993592 0.990721 0.997118 0.998546 0.999033 ... 0.657750 0.742643 0.632574 0.642756 0.661387 0.705879 0.667114 0.708983 0.686257 0.684835 2 0.999974 0.996607 1.000000 0.998333 0.999061 0.999377 0.998438 0.999605 0.998356 0.996986 ... 0.602231 0.689931 0.578798 0.588854 0.607300 0.651305 0.610553 0.657556 0.637631 0.636806 3 0.998168 0.997397 0.998333 1.000000 0.996719 0.997833 0.996139 0.999546 0.999833 0.999307 ... 0.641080 0.722377 0.620453 0.629754 0.646729 0.686380 0.647851 0.694538 0.677737 0.677225 4 0.999347 0.992233 0.999061 0.996719 1.000000 0.999883 0.999914 0.998503 0.996031 0.993761 ... 0.576858 0.664510 0.555166 0.564947 0.582896 0.625491 0.584183 0.634029 0.616536 0.616138 5 rows × 150 columns 也可以使用pandas中的corr方法 corr可以使用的算法有: pearson (default)皮尔逊相关系数 kendall Kendall Tau相关系数 spearman 斯皮尔曼等级相关系数 可以使用'method'关键字指定.请注意，非数字列将从相关性计算中自动排除。为了自己看起来明确,要么写好注释,要么就自己手动排除或者处理 iris_.corr(method='spearman')[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width sepal_length 1.000000 -0.159457 0.881386 0.834421 sepal_width -0.159457 1.000000 -0.303421 -0.277511 petal_length 0.881386 -0.303421 1.000000 0.936003 petal_width 0.834421 -0.277511 0.936003 1.000000 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/时间序列/时间序列.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/时间序列/时间序列.html","title":"时间序列","keywords":"","body":"时间序列 pandas的一个特色就是时间序列操作.所谓时间序列可以理解为index为时间信息的序列对象(有时是数据框). import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline 生成时间序列 pandas提供了时间序列的生成函数pd.date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwargs)[source])可以像python使用range生成整数序列一样生成时间序列(DatetimeIndex类型), 其中 end/start和periods不共存,start,end标明起止日期,periods标明延续时常.用periods的话end/start必须只存在一个 freq定义间隔,参数为字符串形式,可以指定单位,比如5H,3D,具体由哪些单位可以看这里 pd.date_range(start='1/1/2018', end='1/08/2018') DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'], dtype='datetime64[ns]', freq='D') pd.date_range(start='1/1/2018', periods=5, freq='3M') DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31', '2019-01-31'], dtype='datetime64[ns]', freq='3M') 通常生成的时间序列都是给Series或者Dataframe做index的, index = pd.date_range(start='1/1/2018', end='1/08/2018') pd.Series(5,index=index) 2018-01-01 5 2018-01-02 5 2018-01-03 5 2018-01-04 5 2018-01-05 5 2018-01-06 5 2018-01-07 5 2018-01-08 5 Freq: D, dtype: int64 转化时间序列 pandas提供了函数.to_datetime()可以将由各种时间信息组成的序列转化成时间序列 import datetime import time pd.to_datetime([np.datetime64('2018-01-01'),datetime.datetime(2018, 1, 1)]) # 处理datetime类型 DatetimeIndex(['2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None) pd.to_datetime(['1/1/2018','1/2/2018','1/3/2018'],format='%m/%d/%Y') # 处理字符串 DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'], dtype='datetime64[ns]', freq=None) pd.to_datetime([1349720105, 1349806505, 1349892905,1349979305, 1350065705], unit='s') # 处理时间戳 DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05', '2012-10-10 18:15:05', '2012-10-11 18:15:05', '2012-10-12 18:15:05'], dtype='datetime64[ns]', freq=None) 提取指定时间段内的数据 s = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000)) s[:10] 2000-01-01 -0.995419 2000-01-02 0.299803 2000-01-03 -0.688532 2000-01-04 1.019509 2000-01-05 1.080041 2000-01-06 0.949750 2000-01-07 -0.049066 2000-01-08 -0.704941 2000-01-09 1.324811 2000-01-10 0.036751 Freq: D, dtype: float64 s[\"2000\"][:10] #取同一年 2000-01-01 -0.995419 2000-01-02 0.299803 2000-01-03 -0.688532 2000-01-04 1.019509 2000-01-05 1.080041 2000-01-06 0.949750 2000-01-07 -0.049066 2000-01-08 -0.704941 2000-01-09 1.324811 2000-01-10 0.036751 Freq: D, dtype: float64 s[\"2000-01\"][:40] # 取某年某月 2000-01-01 -0.995419 2000-01-02 0.299803 2000-01-03 -0.688532 2000-01-04 1.019509 2000-01-05 1.080041 2000-01-06 0.949750 2000-01-07 -0.049066 2000-01-08 -0.704941 2000-01-09 1.324811 2000-01-10 0.036751 2000-01-11 -1.154498 2000-01-12 -0.079264 2000-01-13 0.160674 2000-01-14 0.049950 2000-01-15 -0.901635 2000-01-16 1.255930 2000-01-17 -0.608129 2000-01-18 0.208124 2000-01-19 0.585852 2000-01-20 -0.304601 2000-01-21 -0.376128 2000-01-22 0.564078 2000-01-23 -0.703543 2000-01-24 1.595288 2000-01-25 -0.360244 2000-01-26 0.442470 2000-01-27 0.532481 2000-01-28 0.319283 2000-01-29 -1.402987 2000-01-30 0.716361 2000-01-31 -1.190318 Freq: D, dtype: float64 s['2000-1-28':'2000-2-3'] #一段日期 2000-01-28 0.319283 2000-01-29 -1.402987 2000-01-30 0.716361 2000-01-31 -1.190318 2000-02-01 -1.807341 2000-02-02 -0.954533 2000-02-03 -0.188877 Freq: D, dtype: float64 s['2000-1-28'] # 某一天 0.31928275714561777 窗口函数 对于处理时间序列数据,pandas提供了许多窗口函数用于计算公共窗口或滚动统计.其中包括计数,总和,平均值,中值,相关性,方差,协方差,标准偏差,偏度和峰度. 我们使用rolling,.expanding,ewm 对数据进行相应的处理 这三个函数的用法和groupby很像,他们的构造函数通常这些方法都有相同的接口. 他们都接受以下参数: window:移动窗口的大小 min_periods:要求非空数据点的阈值（否则结果为NA） center:boolean，是否在中间设置标签（默认为False） axis rolling函数 rolling(window,min_periods,center,axis) s = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000)) s = s.cumsum() s[:5] 2000-01-01 1.061108 2000-01-02 1.143829 2000-01-03 0.042465 2000-01-04 -1.061528 2000-01-05 -1.251423 Freq: D, dtype: float64 r = s.rolling(window=10) r Rolling [window=10,center=False,axis=0] r.mean()[:20] 2000-01-01 NaN 2000-01-02 NaN 2000-01-03 NaN 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 NaN 2000-01-07 NaN 2000-01-08 NaN 2000-01-09 NaN 2000-01-10 0.012929 2000-01-11 0.016536 2000-01-12 0.089529 2000-01-13 0.187641 2000-01-14 0.418945 2000-01-15 0.637028 2000-01-16 0.568675 2000-01-17 0.767808 2000-01-18 0.934924 2000-01-19 1.320222 2000-01-20 1.503517 Freq: D, dtype: float64 s.plot() r.mean().plot(style='k') df = pd.DataFrame(np.random.randn(1000, 4), index=pd.date_range('1/1/2000', periods=1000), columns=['A', 'B', 'C', 'D']) df = df.cumsum() df.rolling(window=60).sum().plot(subplots=True) df.plot(subplots=True) array([, , , ], dtype=object) 使用自定义的方法 s.rolling(window=60).apply(lambda x: np.fabs(x - x.mean()).mean(),raw=True).plot(style='k') rolling有一个特有关键字win_type,它表示窗口的类型,公认类型有: boxcar triang blackman hamming bartlett parzen bohman blackmanharris nuttall barthann kaiser (需要beta参数) gaussian (需要std参数) general_gaussian (需要 power, width参数) slepian (需要width参数). ser = pd.Series(np.random.randn(10), index=pd.date_range('1/1/2000', periods=10)) ser.rolling(window=5, win_type='triang').mean() 2000-01-01 NaN 2000-01-02 NaN 2000-01-03 NaN 2000-01-04 NaN 2000-01-05 -1.049692 2000-01-06 -1.115787 2000-01-07 -0.666003 2000-01-08 -0.149324 2000-01-09 -0.064220 2000-01-10 -0.378598 Freq: D, dtype: float64 ser.rolling(window=5, win_type='boxcar').mean() 2000-01-01 NaN 2000-01-02 NaN 2000-01-03 NaN 2000-01-04 NaN 2000-01-05 -0.898436 2000-01-06 -0.869136 2000-01-07 -0.524903 2000-01-08 -0.427698 2000-01-09 -0.414232 2000-01-10 -0.242575 Freq: D, dtype: float64 ser.rolling(window=5, win_type='gaussian').mean(std=0.1) 2000-01-01 NaN 2000-01-02 NaN 2000-01-03 NaN 2000-01-04 NaN 2000-01-05 -1.169154 2000-01-06 -1.717525 2000-01-07 -1.092201 2000-01-08 0.532417 2000-01-09 0.821946 2000-01-10 -0.683129 Freq: D, dtype: float64 时间感知滚动 这对于非规则的时间频率指数特别有用。第一个参数使用字符串表示时间间隔即可 dft = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}, index=pd.date_range('20130101 09:00:00', periods=5, freq='s')) dft .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:01 1.0 2013-01-01 09:00:02 2.0 2013-01-01 09:00:03 NaN 2013-01-01 09:00:04 4.0 dft.rolling('2s').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:01 1.0 2013-01-01 09:00:02 3.0 2013-01-01 09:00:03 2.0 2013-01-01 09:00:04 4.0 计算窗口的cov() 和 corr() 在金融数据分析和其他领域中,通常对于时间序列的集合计算协方差和相关矩阵.通常人们也对移动窗协方差和相关矩阵感兴趣.这可以通过传递pairwise关键字参数来实现,在DataFrame输入的情况下,将产生一个Panel,其中的items是有问题的日期.在单个DataFrame参数的情况下,成对参数甚至可以省略 df2 = df[:20] df2.rolling(window=5).corr(df2['B']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 NaN NaN NaN NaN 2000-01-02 NaN NaN NaN NaN 2000-01-03 NaN NaN NaN NaN 2000-01-04 NaN NaN NaN NaN 2000-01-05 -0.013771 1.0 0.051814 0.294500 2000-01-06 -0.153475 1.0 0.209701 0.729156 2000-01-07 -0.378544 1.0 0.497041 0.714231 2000-01-08 -0.226850 1.0 0.581944 0.435943 2000-01-09 -0.068159 1.0 0.529225 0.394325 2000-01-10 -0.547704 1.0 -0.079096 -0.071999 2000-01-11 -0.664832 1.0 0.187738 -0.463042 2000-01-12 -0.829057 1.0 0.425534 -0.610106 2000-01-13 -0.837847 1.0 0.581573 0.119215 2000-01-14 -0.812830 1.0 0.323542 0.736338 2000-01-15 -0.931585 1.0 -0.032185 0.627065 2000-01-16 -0.740344 1.0 -0.074195 0.523123 2000-01-17 -0.303898 1.0 -0.688740 0.268439 2000-01-18 -0.111382 1.0 -0.268512 0.694223 2000-01-19 0.133870 1.0 -0.276746 0.469622 2000-01-20 0.116302 1.0 0.805018 0.616604 covs = df[['B','C','D']].rolling(window=50).cov(df[['A','B','C']], pairwise=True) covs.loc[df.index[-50]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C D A -3.366503 -1.697049 2.308745 B 4.988816 2.207432 -0.370125 C 2.207432 3.962947 -0.601842 correls = df.rolling(window=50).corr() correls.loc[df.index[-50]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D A 1.000000 -0.604904 -0.342130 0.421968 B -0.604904 1.000000 0.496454 -0.075465 C -0.342130 0.496454 1.000000 -0.137680 D 0.421968 -0.075465 -0.137680 1.000000 correls.reorder_levels([1,0], axis=0).loc[\"A\",:][\"C\"] 2000-01-01 NaN 2000-01-02 NaN 2000-01-03 NaN 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 NaN 2000-01-07 NaN 2000-01-08 NaN 2000-01-09 NaN 2000-01-10 NaN 2000-01-11 NaN 2000-01-12 NaN 2000-01-13 NaN 2000-01-14 NaN 2000-01-15 NaN 2000-01-16 NaN 2000-01-17 NaN 2000-01-18 NaN 2000-01-19 NaN 2000-01-20 NaN 2000-01-21 NaN 2000-01-22 NaN 2000-01-23 NaN 2000-01-24 NaN 2000-01-25 NaN 2000-01-26 NaN 2000-01-27 NaN 2000-01-28 NaN 2000-01-29 NaN 2000-01-30 NaN ... 2002-08-28 -0.429215 2002-08-29 -0.463922 2002-08-30 -0.521507 2002-08-31 -0.557186 2002-09-01 -0.596870 2002-09-02 -0.630756 2002-09-03 -0.654455 2002-09-04 -0.695831 2002-09-05 -0.702939 2002-09-06 -0.716429 2002-09-07 -0.724767 2002-09-08 -0.742225 2002-09-09 -0.732229 2002-09-10 -0.743287 2002-09-11 -0.747569 2002-09-12 -0.703009 2002-09-13 -0.619047 2002-09-14 -0.528259 2002-09-15 -0.427015 2002-09-16 -0.322860 2002-09-17 -0.205540 2002-09-18 -0.133395 2002-09-19 -0.061589 2002-09-20 -0.017721 2002-09-21 0.047676 2002-09-22 0.088563 2002-09-23 0.131738 2002-09-24 0.133835 2002-09-25 0.112635 2002-09-26 0.063967 Freq: D, Name: C, Length: 1000, dtype: float64 correls.reorder_levels([1,0], axis=0).loc[\"A\",:][\"C\"].plot() 使用aggregate聚合 这个操作和groupby那个聚合非常类似,构建窗口后通过一系列算法获得了各窗口的值,我们可以通过传递一个函数到整个DataFrame 也可以用agg()一次应用多个function dfa = pd.DataFrame(np.random.randn(1000, 3), index=pd.date_range('1/1/2000', periods=1000), columns=['A', 'B', 'C']) r = dfa.rolling(window=60,min_periods=1) r.aggregate(np.sum)[:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C 2000-01-01 0.069055 -0.431206 -0.250284 2000-01-02 1.541963 -0.869075 -0.478704 2000-01-03 4.381829 -2.640809 0.837516 2000-01-04 4.824145 -2.998398 0.660984 2000-01-05 5.388716 -3.750861 1.237194 2000-01-06 4.671739 -2.204327 0.628633 2000-01-07 5.248314 -2.217534 1.141510 2000-01-08 5.711637 -2.554723 0.104990 2000-01-09 6.032947 -2.689234 -1.642611 2000-01-10 5.765271 -2.938515 -2.359754 r['A'].agg([np.sum, np.mean, np.std])[:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum mean std 2000-01-01 0.069055 0.069055 NaN 2000-01-02 1.541963 0.770981 0.992674 2000-01-03 4.381829 1.460610 1.385446 2000-01-04 4.824145 1.206036 1.240513 2000-01-05 5.388716 1.077743 1.111957 2000-01-06 4.671739 0.778623 1.235312 2000-01-07 5.248314 0.749759 1.130263 2000-01-08 5.711637 0.713955 1.051310 2000-01-09 6.032947 0.670327 0.992081 2000-01-10 5.765271 0.576527 0.981250 r['A'].agg({'result1' : np.sum,'result2' : np.mean})[:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } result1 result2 2000-01-01 0.069055 0.069055 2000-01-02 1.541963 0.770981 2000-01-03 4.381829 1.460610 2000-01-04 4.824145 1.206036 2000-01-05 5.388716 1.077743 2000-01-06 4.671739 0.778623 2000-01-07 5.248314 0.749759 2000-01-08 5.711637 0.713955 2000-01-09 6.032947 0.670327 2000-01-10 5.765271 0.576527 r.agg([np.sum, np.mean])[:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } A B C sum mean sum mean sum mean 2000-01-01 0.069055 0.069055 -0.431206 -0.431206 -0.250284 -0.250284 2000-01-02 1.541963 0.770981 -0.869075 -0.434537 -0.478704 -0.239352 2000-01-03 4.381829 1.460610 -2.640809 -0.880270 0.837516 0.279172 2000-01-04 4.824145 1.206036 -2.998398 -0.749599 0.660984 0.165246 2000-01-05 5.388716 1.077743 -3.750861 -0.750172 1.237194 0.247439 2000-01-06 4.671739 0.778623 -2.204327 -0.367388 0.628633 0.104772 2000-01-07 5.248314 0.749759 -2.217534 -0.316791 1.141510 0.163073 2000-01-08 5.711637 0.713955 -2.554723 -0.319340 0.104990 0.013124 2000-01-09 6.032947 0.670327 -2.689234 -0.298804 -1.642611 -0.182512 2000-01-10 5.765271 0.576527 -2.938515 -0.293852 -2.359754 -0.235975 expanding函数 expanding(window,min_periods,center,axis) 滚动统计的一个常见替代方法是使用扩展窗口,该窗口产生具有到达该时间点之前可用的所有数据的统计的值. 他的接口接近.rolling的接口.expanding方法返回一个Expanding对象.我们可以对比下一下两个函数,他们是等效的 df.rolling(window=len(df), min_periods=1).mean()[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 -1.037228 0.559174 -1.177712 0.076422 2000-01-02 -0.571626 0.326794 -1.091479 0.327348 2000-01-03 -0.862770 0.214503 -0.535658 0.471560 2000-01-04 -0.855107 0.560978 -0.370145 0.613405 2000-01-05 -0.806280 0.531091 -0.118997 0.714881 df.expanding(min_periods=1).mean()[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 -1.037228 0.559174 -1.177712 0.076422 2000-01-02 -0.571626 0.326794 -1.091479 0.327348 2000-01-03 -0.862770 0.214503 -0.535658 0.471560 2000-01-04 -0.855107 0.560978 -0.370145 0.613405 2000-01-05 -0.806280 0.531091 -0.118997 0.714881 s.plot(style='k--') s.expanding().mean().plot(style='k') 指数加权窗口ewm ewm是几个上述统计量的指数加权版本。 他支持的默认方法比较少 Function Description mean() EW moving average var() EW moving variance std() EW moving standard deviation corr() EW moving correlation cov() EW moving covariance s.plot(style='k--') s.ewm(span=20).mean().plot(style='k') Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/数据获取与保存.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/数据获取与保存.html","title":"数据获取与保存","keywords":"","body":"数据获取与保存 数据的来源途径无非几种 从网上直接爬取,这个属于写爬虫,不是本文范围 从原生的Python数据结构中获取 从json中获取 数据库中调取, 从excel中读取, 从csv文本文件中获得. 通过pickle序列化数据 而存储数据也无非以下几种方式: 通过pickle序列化数据 保存为json 保存到数据库 保存为excel 保存为csv pandas针对上面的每种获取途径,都提供了方便的获取方式 import pandas as pd 从csv文件中读取和保存 所谓csv文件是指使用特定符号分隔数据属性,换行分隔不同数据的文本文件. 这次的例子我们主要用的数据便来自于此. 我们惯例的用iris来作为是数据集.这个数据集在我们的项目下也有 让我先看看该数据是什么样子 with open(\"./source/iris.csv\") as f: print(f.readline()) print(f.readline()) sepal_length,sepal_width,petal_length,petal_width,class 5.1,3.5,1.4,0.2,Iris-setosa iris_data = pd.read_csv(\"./source/iris.csv\",encoding = \"utf-8\") iris_data[:5]#取前5行 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 保存到csv iris_data.to_csv(\"source/iris.csv\",index=False)# 记得不要把序号写进去 new_iris_data = pd.read_csv(\"source/iris.csv\") new_iris_data[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 从json中获取和保存 先随便来个json格式的文件,我们就自己写个例子people.json为例,看下内容: with open(\"./source/people.json\") as f: print(f.readline()) [{\"name\":\"Michael\"},{\"name\":\"Andy\", \"age\":30},{\"name\":\"Justin\", \"age\":19}] people_from_jsonfile = pd.read_json(\"./source/people.json\") people_from_jsonfile .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age name 0 NaN Michael 1 30.0 Andy 2 19.0 Justin 保存为json 要保存为json格式,也只需要是用to_json方法即可 people_from_jsonfile.to_json() '{\"age\":{\"0\":null,\"1\":30.0,\"2\":19.0},\"name\":{\"0\":\"Michael\",\"1\":\"Andy\",\"2\":\"Justin\"}}' people_from_jsonfile.to_json(\"./source/people_cp.json\") new_peoplefrom_jsonfile = pd.read_json(\"./source/people_cp.json\") new_peoplefrom_jsonfile .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age name 0 NaN Michael 1 30.0 Andy 2 19.0 Justin 数据库中读取 (需要SQLAlchemy库)和保存 我们以python自带的sqlite3来作测试 先创建一个数据库,还是用我们的people.json中的数据,如何制作具体看 制作好的people.db依然放在./source文件夹下 from sqlalchemy import create_engine conn = create_engine(\"sqlite:///source/people.db\") conn Engine(sqlite:///source/people.db) 保存到数据库 people_from_jsonfile.to_sql('people', conn) people_from_db = pd.read_sql('people', conn) people_from_db .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index age name 0 0 NaN Michael 1 1 30.0 Andy 2 2 19.0 Justin 读取数据库中的表 people_from_db = pd.read_sql('people', conn) people_from_db .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index age name 0 0 NaN Michael 1 1 30.0 Andy 2 2 19.0 Justin 使用查询语句获得数据 peole_from_db_query = pd.read_sql_query('SELECT * FROM people', conn) peole_from_db_query .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index age name 0 0 NaN Michael 1 1 30.0 Andy 2 2 19.0 Justin 从excel中读取数据(需要xlrd)和保存 一样的我们还是拿people作为数据,创建一个excel文件放入./source # using the ExcelFile class xls = pd.ExcelFile('./source/people.xlsx') data_fromExcel = xls.parse(u'工作表1', index_col=None, na_values=['NA']) data_fromExcel .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Michael NaN 1 Andy 30.0 2 Justin 19.0 people_fromExcel = pd.read_excel('./source/people.xlsx', u'工作表1', index_col=None, na_values=['NA']) people_fromExcel .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Michael NaN 1 Andy 30.0 2 Justin 19.0 保存到excel iris_data.to_excel('source/iris.xlsx', sheet_name='Sheet1',index=False) 通过pickle序列化数据 要将表格序列化只需要使用to_pickle方法就好 iris_data.to_pickle(\"source/iris.pickle\") 读取也是只要pd.read_pickle(path)即可 iris_data_copy = pd.read_pickle(\"source/iris.pickle\") iris_data_copy[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/数据可视化/数据可视化.html":{"url":"工具链篇/计算工具/数值计算/使用pandas处理结构化数据/数据可视化/数据可视化.html","title":"数据可视化","keywords":"","body":"数据可视化 pandas针对表格数据有对matplotlib的封装,通过polt()方法可以方便的绘制图形 import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use('ggplot') %matplotlib inline 基本绘图 最基础的绘图就是为序列绘图polt()了 我们以随机游走模拟股市行情 如果索引由日期组成，它将调用gcf().autofmt_xdate()来尝试按索引(index)的格式格式化x轴。 ps:累积和(cumsum) S_0=0 \\\\ S_{n+1}=\\max(0, S_n+x_n-\\omega_n) 用以在某个相对稳定的数据序列中，检测出开始发生异常的数据点。所谓异常的数据点，比如说，从这点开始，整个数列的平均值或者均方差开始发生改变，进而影响到整组数据的稳定。所以累积和最典型的应用是在“改变检测”（Change Detection）中对参量变化的检测。由于累积和管制法能充分利用数据变化之顺序与大小,故相当适合用于侦测制程的微量变化(small shifts) 这边使用累积和用来获得当天的值 ts_o = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2016', periods=1000)) ts = ts_o.cumsum()# 累积和 ts[:5] 2016-01-01 -0.943073 2016-01-02 0.645978 2016-01-03 -0.162719 2016-01-04 -0.593420 2016-01-05 -1.552354 Freq: D, dtype: float64 ts.plot() 如果是针对的df,那么它会按列分别绘制图形.它会自动用不同颜色来区分 df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD')) df = df.cumsum() df[:5] A B C D 2016-01-01 1.768804 -1.509527 -1.630513 -0.407891 2016-01-02 1.631288 0.145392 -1.067315 -1.405604 2016-01-03 1.153486 -0.335480 -1.320116 -2.216602 2016-01-04 0.916897 -1.356709 -1.410290 -1.637204 2016-01-05 1.472524 -1.381097 -2.646981 -1.246137 df.plot() 指定x轴y轴 df3 = pd.DataFrame(np.random.randn(1000, 2), columns=['B', 'C']).cumsum() df3['A'] = pd.Series(list(range(len(df)))) df3[:5] B C A 0 -0.487310 1.489582 0 1 0.018866 2.249854 1 2 1.536721 0.929395 2 3 2.390767 -0.671159 3 4 4.575829 -2.713449 4 df3.plot(x='A', y='B') 图形设置 和在matplotlib中一样,我们可以设置style,label,color等,如 style='k--', label='Series','legend=False' plot的完整接口如下: .plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False, 其中 legend表示用户不用图例 logy 表示是否使用对数标度Y轴 secondary_y表示是否使用第二根y轴,这常在有两个图形在同一张图中时用 subplots=True和figsize=(x,x)配合用于绘制子图 子图的布局可以通过layout关键字指定。它可以接受（行，列）。 layout关键字也可以在hist和boxplot中使用。如果输入无效，将引发ValueError。由布局指定的行x列可包含的轴数必须大于所需子图的数量。如果布局可以包含比所需更多的轴，则不绘制空白轴。与numpy数组的重塑方法类似，对于一个维度，您可以使用-1自动计算所需的行数或列数，而另一个维度。比如plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False) 使用table=True绘制表格 df3.plot(x='A', y='B',style='k--', label='Series') Colormaps 绘制大量列时的潜在问题是，由于默认颜色的重复，可能难以区分某些系列。为了解决这个问题，DataFrame绘图支持使用colormap =参数，它接受一个Matplotlib色彩映射或一个字符串，它是用Matplotlib注册的色彩映射的名称。默认的matplotlib色彩图可以在matplotlib的cm模块中找到,同时cm模块中的对象也可以作为输入参数 from matplotlib import cm df_color = pd.DataFrame(np.random.randn(1000, 10), index=ts.index) df_color = df_color.cumsum() df_color.plot(colormap='cubehelix') accent df_color.plot(colormap=cm.cubehelix) 绘制带errorbar的图形 ix_err = pd.MultiIndex.from_arrays([['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], ['foo', 'foo', 'bar', 'bar', 'foo', 'foo', 'bar', 'bar']], names=['letter', 'word']) df_err = pd.DataFrame({'data1': [3, 2, 4, 3, 2, 4, 3, 2], 'data2': [6, 5, 7, 5, 4, 5, 6, 5]}, index=ix_err) gp_err = df_err.groupby(level=('letter', 'word')) means = gp_err.mean() errors = gp_err.std() means data1 data2 letter word a bar 3.5 6.0 foo 2.5 5.5 b bar 2.5 5.5 foo 3.0 4.5 errors data1 data2 letter word a bar 0.707107 1.414214 foo 0.707107 0.707107 b bar 0.707107 0.707107 foo 1.414214 0.707107 fig, ax = plt.subplots() means.plot.bar(yerr=errors, ax=ax) 额外的绘图工具 pandas.tools.plotting模块中还提供了另外几个绘图工具 散点矩阵图scatter_matrix from pandas.tools.plotting import scatter_matrix df1 = pd.DataFrame(np.random.randn(1000, 4), columns=['a', 'b', 'c', 'd']) scatter_matrix(df1, alpha=0.2, figsize=(6, 6), diagonal='kde') array([[, , , ], [, , , ], [, , , ], [, , , ]], dtype=object) 安德鲁斯曲线 安德鲁斯曲线允许将多变量数据绘制为使用样本的属性创建的大量曲线作为傅里叶级数的系数。通过为每个类不同地着色这些曲线，可以可视化数据聚类。属于相同类别的样品的曲线通常更靠近在一起并形成更大的结构。 以iris为例 from pandas.tools.plotting import andrews_curves iris = pd.read_csv('source/iris.csv') iris[:5] sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa andrews_curves(iris, 'class') 平行坐标parallel_coordinates 平行坐标是绘制多变量数据的绘图技术。它允许人们看到数据中的簇，并可视地估计其他统计量。使用平行坐标点表示为连接的线段。每个垂直线表示一个属性。一组连接的线段表示一个数据点。倾向于聚类的点将更靠近在一起。 from pandas.tools.plotting import parallel_coordinates parallel_coordinates(iris, 'class') RadViz图 RadViz是一种可视化多变量数据的方法。它基于简单的弹簧张力最小化算法(spring tension minimization algorithm)。基本上你在画板上设置了一堆点。在我们的例子中，它们在单位圆上等间隔。每个点表示单个属性。然后假设数据集中的每个样本都通过弹簧附加到这些点中的每一个上，其刚度与该属性的数值成比例（它们被归一化为单位间隔）。在平面中我们的样本沉降的点（其中作用在我们的样本上的力处于平衡）是表示我们的样本的点将被绘制的点。根据样本属于哪个类别，它将有不同的颜色。 from pandas.tools.plotting import radviz radviz(iris, 'class') 滞后图 滞后图用于检查数据集或时间序列是否是随机的。随机数据在滞后图中不应显示任何结构。非随机结构意味着底层数据不是随机的。 from pandas.tools.plotting import lag_plot unr = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(np.linspace(-99 * np.pi, 99 * np.pi, num=1000))) lag_plot(unr) lag_plot(ts) lag_plot(ts_o)#随机 自相关图autocorrelation_plot 自相关图通常用于检查时间序列中的随机性。这是通过计算在不同时间滞后的数据值的自相关来完成的。如果时间序列是随机的，对于任何和时间滞后分离，这种自相关应该接近零。如果时间序列是非随机的，则一个或多个自相关将显着地非零。图中显示的水平线对应于95％和99％置信带。虚线是99％置信带。 from pandas.tools.plotting import autocorrelation_plot data_autocorrelation = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(np.linspace(-9 * np.pi, 9 * np.pi, num=1000))) autocorrelation_plot(data_autocorrelation) 引导图bootstrap_plot 引导图用于视觉评估统计量的不确定性，例如平均值，中值，中等范围等。从数据集中选择指定大小的随机子集，针对该子集计算所讨论的统计量，并且该过程是根据指定次数重复的。图形和直方图的结果构成了引导图。 from pandas.tools.plotting import bootstrap_plot data_bootstrap = pd.Series(np.random.rand(1000)) bootstrap_plot(data_bootstrap, size=50, samples=500, color='grey') 常见统计图 除了折线图,当然还有常见统计图形比如栅栏图,直方图,饼图,箱图了 绘制这些图形最简单的方式就是使用plot.()方法了,他可以在一副图中绘制出想要的内容 使用缺失数据绘制 Pandas试图做实用的可以绘制包含缺失数据的DataFrames或Series的图形。它会默认的根据绘图类型，丢弃，舍弃或填充缺失值。当然更好的方式是先处理缺失值再绘图 Plot Type NaN Handling Line 在NaNs处留下空隙 Line (stacked) 用0填充 Bar 用0填充 Scatter 删除Nan Histogram 删除Nan Box 删除Nan Area 用0填充 KDE 删除Nan Hexbin 删除Nan Pie 用0填充 绘制图表table from pandas.tools.plotting import table df_table = pd.DataFrame(np.random.rand(5, 3), columns=['a', 'b', 'c']) df_table a b c 0 0.907814 0.577342 0.972353 1 0.040087 0.499157 0.191772 2 0.474351 0.452796 0.955516 3 0.872288 0.324289 0.467717 4 0.045308 0.264322 0.081304 fig, ax = plt.subplots(1, 1) table(ax, np.round(df_table.describe(), 2), loc='upper right', colWidths=[0.2, 0.2, 0.2]) df_table.plot(ax=ax, ylim=(0, 2), legend=None) 散点图scatter 观察两组数据的特点最好的方法就是使用散点图 df_scatter = pd.DataFrame(np.random.rand(50, 4), columns=['a', 'b', 'c', 'd']) df_scatter.plot.scatter(x='a', y='b'); 散点分组 ax = df_scatter.plot.scatter(x='a', y='b', color='DarkBlue', label='Group 1') df_scatter.plot.scatter(x='c', y='d', color='DarkGreen', label='Group 2', ax=ax) 使用灰度区分组 df_scatter.plot.scatter(x='a', y='b', c='c', s=50) 使用点的大小来区分组别 df_scatter.plot.scatter(x='a', y='b', s=df_scatter['c']*200) 散点密度图kde ser = pd.Series(np.random.randn(1000)) ser.plot.kde() 六边形图hexbin 如果您的数据太密集，则Hexbin图可能是散点图的有用替代方法，无法单独绘制每个点。 df_hexbin = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b']) df_hexbin['b'] = df_hexbin['b'] + np.arange(1000) df_hexbin[:5] a b 0 0.408026 -0.811502 1 -0.587273 0.383572 2 2.415451 3.996042 3 1.264680 2.171141 4 2.843472 3.536401 df_hexbin.plot.hexbin(x='a', y='b', gridsize=25) 一个有用的关键字参数是gridsize;它控制x方向上的六边形数量，默认为100.较大的网格尺寸意味着更多，更小的箱柜(bin). 默认情况下，计算每个（x，y）点周围的计数的直方图。您可以通过将值传递给C和reduce_C_function参数来指定备用聚合。 C指定每个（x，y）点的值，reduce_C_function是一个参数的函数，将bin中的所有值减少为单个数字(例如mean，max，sum，std).在该示例中，位置由列a和b给出，而值由列z给出.bin是用numpy的max函数聚合的. df_hexbin = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b']) df_hexbin['b'] = df_hexbin['b'] = df_hexbin['b'] + np.arange(1000) df_hexbin['z'] = np.random.uniform(0, 3, 1000) df_hexbin.plot.hexbin(x='a', y='b', C='z', reduce_C_function=np.max,gridsize=25) 栅栏图bar df.ix[5].plot.bar() plt.figure() df.ix[5].plot(kind='bar') plt.axhline(0, color='k') plt.show() df2 = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd']) df2.plot.bar(); df2.plot.bar(stacked=True) 橫置的栅栏图barh df2.plot.barh(stacked=True) 直方图hist 直方图常用来体现不同区间的分布情况 df4 = pd.DataFrame({'a': np.random.randn(1000) + 1, 'b': np.random.randn(1000), 'c': np.random.randn(1000) - 1}, columns=['a', 'b', 'c']) df4[:5] a b c 0 2.098046 0.700889 -1.788784 1 2.478216 -1.295595 -2.413717 2 0.309454 0.795296 -1.801240 3 0.186927 -1.212297 -0.716213 4 -0.196420 -0.072681 -1.323029 df4.plot.hist(alpha=0.5) df4.plot.hist(stacked=True, bins=20) 您可以传递由matplotlib hist支持的其他关键字。例如，水平和累积的histgram可以通过orientation ='horizontal'和cumulative ='True'绘制。 df4['a'].plot.hist(orientation='horizontal', cumulative=True) df.diff().hist(color='k', alpha=0.5, bins=50) array([[, ], [, ]], dtype=object) data = pd.Series(np.random.randn(1000)) data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4)) array([[, ], [, ]], dtype=object) 箱图 箱图是直方图的进化,可以更好的观察不同组别的统计学分布情况 df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E']) df A B C D E 0 0.530801 0.879414 0.746797 0.769493 0.268179 1 0.879360 0.067813 0.834677 0.123594 0.004283 2 0.746788 0.239194 0.459012 0.197933 0.022791 3 0.673286 0.937733 0.746696 0.128579 0.905100 4 0.424843 0.606296 0.218359 0.800959 0.175384 5 0.121017 0.558144 0.954226 0.585608 0.772449 6 0.281973 0.599172 0.485583 0.793592 0.833444 7 0.696017 0.876120 0.009563 0.142212 0.747473 8 0.777619 0.834691 0.816196 0.038078 0.518234 9 0.252343 0.666446 0.248015 0.023385 0.592396 df.plot.box() Boxplot可以通过传递color关键字来着色。你可以传递一个dict，它的键是box，whiskers，medians和caps。如果dict中缺少某些键，则默认颜色用于相应的艺术家。此外，boxplot有sym关键字来指定传单风格。当你通过color关键字传递其他类型的参数时，它将直接传递给matplotlib用于所有框，whiskers，medians和caps的着色。颜色应用于每个要绘制的框。如果你想要更复杂的着色，你可以通过传递return_type来获得每个绘制的艺术家。 color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray') df.plot.box(color=color, sym='r+') df.plot.box(vert=False, positions=[1, 4, 5, 6, 8]) df = pd.DataFrame(np.random.rand(10,5)) bp = df.boxplot() 您可以使用by关键字参数创建分层箱形图以创建分组。 df = pd.DataFrame(np.random.rand(10,2), columns=['Col1', 'Col2'] ) df['X'] = pd.Series(['A','A','A','A','A','B','B','B','B','B']) bp = df.boxplot(by='X') 您还可以传递要绘制的列的子集，以及按多个列分组 df = pd.DataFrame(np.random.rand(10,3), columns=['Col1', 'Col2', 'Col3']) df['X'] = pd.Series(['A','A','A','A','A','B','B','B','B','B']) df['Y'] = pd.Series(['A','B','A','B','A','B','A','B','A','B']) bp = df.boxplot(column=['Col1','Col2'], by=['X','Y']) Groupby.boxplot总是返回一系列return_type。 np.random.seed(1234) df_box = pd.DataFrame(np.random.randn(50, 2)) df_box['g'] = np.random.choice(['A', 'B'], size=50) df_box.loc[df_box['g'] == 'B', 1] += 3 bp = df_box.boxplot(by='g') bp = df_box.groupby('g').boxplot() 面积图area 面积图强调数量随时间而变化的程度，也可用于引起人们对总值趋势的注意。例如，表示随时间而变化的利润的数据可以绘制在面积图中以强调总利润 df = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd']) df.plot.area() df.plot.area(stacked=False); 饼图Pie 饼图常用在直观的观察比例 series = pd.Series(3 * np.random.rand(4), index=['a', 'b', 'c', 'd'], name='series') series.plot.pie(figsize=(6, 6)) 对于饼图，最好使用正方形的数字，一个具有相等的宽高比。您可以创建具有相等宽度和高度的图形，或者在绘制后通过在返回的轴对象上调用ax.set_aspect（'equal'）来强制长宽比相等。请注意，使用DataFrame的饼图需要您通过y参数指定目标列，或者subplots = True。指定y时，将绘制所选列的饼图。如果指定了subplots = True，则每个列的饼图将绘制为子图。默认情况下，每个饼图中将绘制一个图例;指定legend = False以隐藏它。 df = pd.DataFrame(3 * np.random.rand(4, 2), index=['a', 'b', 'c', 'd'], columns=['x', 'y']) df.plot.pie(subplots=True, figsize=(8, 4)) array([, ], dtype=object) 如果要隐藏楔形标签，请指定labels = None。 如果指定fontsize，该值将应用于楔形标签。 此外，可以使用由matplotlib.pyplot.pie（）支持的其他关键字。 series.plot.pie(labels=['AA', 'BB', 'CC', 'DD'], colors=['r', 'g', 'b', 'c'],autopct='%.2f', fontsize=20, figsize=(6, 6)) 如果传递总和小于1.0的值，则matplotlib绘制一个半圆 series = pd.Series([0.1] * 4, index=['a', 'b', 'c', 'd'], name='series2') series.plot.pie(figsize=(6, 6)) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/":{"url":"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/","title":"使用dask处理大规模数据的数值计算问题","keywords":"","body":"使用dask做分布式计算 dask是纯python的分布式科学计算框架,其旨在让熟悉python下数据科学工具的数据开发人员可以无痛的从单机版本的numpy,scipy,pandas,sklearn迁移到分布式计算,以适应大数据分析的需求,于此同时不失去python语言的灵活性. 标准的dask集群部署方式(命令行方式)有大致3块结构: 分布式的数据结构接口用于构造计算图 调度器用于分发任务(启动命令dask-scheduler) worker运算节点(启动命令dask-worker tcp://{host}:8786) 在此基础上还有几个借助已有分布式工具的部署方式: docker方式,其实就是命令行方式,只是借助docker集群提供算力,也是本文使用的方式 Kubernetes方式,借助Kubernetes集群提供算力,它提供了docker方式不具备的动态调节资源的能力 YARN/Hadoop方式,借助YARN/Hadoop集群提供算力,这种方式相当于把调度器和worker承包给了yarn 借助高性能计算框架作为调度器.这种一般公司没有. docker方式部署dask集群 下面是一个简易的docker-compose文件 version: \"3.6\" services: scheduler: image: daskdev/dask hostname: dask-scheduler logging: options: max-size: \"10m\" max-file: \"3\" ports: - \"8786:8786\" - \"8787:8787\" command: [\"dask-scheduler\"] worker1: image: daskdev/dask hostname: dask-worker1 logging: options: max-size: \"10m\" max-file: \"3\" command: [\"dask-worker\", \"tcp://scheduler:8786\"] worker2: image: daskdev/dask hostname: dask-worker2 logging: options: max-size: \"10m\" max-file: \"3\" command: [\"dask-worker\", \"tcp://scheduler:8786\"] worker3: image: daskdev/dask hostname: dask-worker3 logging: options: max-size: \"10m\" max-file: \"3\" command: [\"dask-worker\", \"tcp://scheduler:8786\"] 这个配置方式用来测试绰绰有余,但并不适合生产环境使用,生产环境建议使用Kubernetes方式或者YARN/Hadoop方式.如果非要用docker方式,那也注意一定不要用warm自带的overlay网络,可以使用host方式pubish端口,所有的网络流量通过宿主机ip走内网流量. 无论是swarm方式还是k8s方式部署dask集群,我们都需要用到dask的环境镜像daskdev/dask.我们应该保持集群中每个节点使用的镜像一致,以防止不必要的麻烦 命令行方式的helloworld 命令行方式的核心是启动调度器,调度器有一个默认的管理界面在8787端口.这个界面上我们可以看到连接着的集群的各个节点信息,以及任务节点的分布情况. 而要使用集群计算我们需要连接到调度器的8786端口.这里我把远程机器上的调度器端口映射到了本地. from dask.distributed import Client client = Client('localhost:8786') def square(x): return x ** 2 A = client.map(square, range(10000)) total = client.submit(sum, A) print(total.result()) 333283335000 dask集群任务的工作流基本都是一样: 连接集群,实例化一个客户端对象 利用封装好的分布式数据结构或者底层api构建计算图 提交任务,调度执行 其他部署方式只是连接集群的方式不一样了而已 dask的应用场景 dask是为大数据设计的,因此如果数据规模小实际上并不适合使用它.如果你的数据无法放到单机内存中那就可以使用它,反之,好好使用numpy吧. dask不会让你的计算更快,它只是解决数据过大单机无法计算的问题,如果是为了让运算更快,建议使用numba或者cython加速,这个就是另一个话题了. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/分布式数据结构.html":{"url":"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/分布式数据结构.html","title":"分布式数据结构","keywords":"","body":"分布式数据结构 dask提供的分布式数据结构有3种 array类似numpy的多维数组 bag类似python的list DataFrame类似pandas的DataFrame 这三种数据结构基本已经可以满足多有需求.更可贵的是他们的接口和对照版基本一致 这边只简单介绍,DataFrame会在后面数据科学部分介绍好pandas后再详细介绍 from dask.distributed import Client client = Client('localhost:8786') Array array的接口参考的是numpy的ndarray,实现的接口范围有: 基本的运算符 reduce相关的操作函数如:sum(), mean(),std()... 矩阵计算 切片操作 Array protocols比如__array__和 __array_ufunc__ 一些线性代数运算比如:svd,qr,solve,solve_triangular,lstsq 未实现的接口包括: 全局排序,但有部分排序 tolist这样的操作 shape不确定的序列无法操作 np.linalg中的多数内容 import dask.array as da x = da.random.random((100000, 100000), chunks=(1000, 1000)) y = x + x.T - x.mean(axis=0) y Array Chunk Bytes 80.00 GB 8.00 MB Shape (100000, 100000) (1000, 1000) Count 53500 Tasks 10000 Chunks Type float64 numpy.ndarray 100000 100000 print(y[0,0].compute()) 1.4348119494257066 Bag Bag的接口参考的是list,但比python的list接口更丰富,它是最通用的数据类型,类似spark中的rdd,但代价就是性能比不上array和dataframe. import dask.bag as db graph = db.from_sequence(range(10000), partition_size=2).map(lambda x: x**2).sum() graph.compute() 333283335000 DataFame 和pandas一样,DataFame数据结构通常作为dask的交互层,因此接口最丰富.它可以无缝和pandas交互 import pandas as pd import dask.dataframe as dd pdf1 = pd.DataFrame([{\"a\":1,\"b\":2},{\"a\":3,\"b\":4}]) pdf2 = pd.DataFrame([{\"a\":1,\"b\":2},{\"a\":2,\"b\":5}]) ddf1 = dd.from_pandas(pdf1,npartitions=2) ddf2 = dd.from_pandas(pdf2,npartitions=2) ddf = dd.concat([ddf1, ddf2], axis=0) ddf Dask DataFrame Structure: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b npartitions=2 int64 int64 ... ... ... ... Dask Name: concat, 4 tasks z = ddf.groupby([\"a\"]).count() z.compute() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } b a 1 2 3 1 2 1 数据块和分隔 我们在指定数据结构的时候通常会设定参数chunks或者npartitions.这两个参数确定了分布式计算过程中的并行度和执行效率. chunks chunks固定了一个计算单元的内存占用和数据形状,如何调优chunks的取值直接影响计算的性能,通常有如下几个要考虑的方面 不能过大:肯定不能超过单机的内存大小,一台机器可以处理多个chunks越小内存的使用率就会越高 不能过小:chunks过小就意味着计算被拆分的越小,任务也就越多,这样调度耗时也会越大,dask的一次调度大约是1ms,通常要保证每个task至少执行100ms,否则效率太低 chunks的内存占用大小应该在10MB到1GB之间,具体取决于内存的可用性和计算的持续时间. chunks应该尽量和数据的形状对齐 npartitions npartitions指定了操作执行时的分片数量,分布式计算说白了就是将巨大复杂的任务拆解成多个小任务来一批一批地执行,那使用多少个计算资源来执行分片就是npartitions确定的. 和chunks类似,这个参数也直接影响性能--npartitions越高并行度越高,每次任务执行的也会越快,但任务数量也会变高,因此调度时间会占比越多;反之单次执行就会越久,但调度次数减少,这就是要优化的部分了. 惰性计算 dask这类分布式计算框架和一般单机计算框架比最大的区别是:单机计算框架多是实时的,给出指令得出结果,dask则是将指令缓存为任务,只有在使用compute()接口后才会真的发布到集群上执行计算过程. 我们可以看到上面的计算过程得到的结果只是打印出了结果的数据结构,内存占用等元信息. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/dask作为算力池/dask作为算力池.html":{"url":"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/dask作为算力池/dask作为算力池.html","title":"dask作为算力池","keywords":"","body":"dask作为算力池 可以理解为dask的数据结构只是计算框架的一个语法糖,它的本质还是任务调度系统,事实上基本所有分布式计算框架的本质都是任务调度系统.因为分布式计算说白了就是把大任务拆分为小任务然后交给很多机器一起执行,然后再回收结果. 那我们就可以利用dask的底层api将其作为算力池来使用. 需要注意的是分布式计算通常都要求函数无副作用.也就是说我们定义的函数不应该使用全局变量,不应该改变对象属性. 使用delayed接口构造任务链 注意:delayed接口中不能使用标准dask的数结构 delayed接口被设计成用于计算标准dask数据结构无法解决的问题.当然通常我们需要使用delayed的场景也都用不到这个接口.这个接口的作用相当于是向集群发布一条由python函数组成的命令,因此使用delayed接口就分解为了两个问题: 怎样构造由python函数组成的命令(计算图) 集群如何执行由python函数组成的命令 from dask.distributed import Client client = Client('localhost:8786') 构造由python函数组成的命令 dask通过delayed接口注册函数构造计算图,使用计算图来控制分发运算.在构造好计算图后,执行.compute()接口发布命令计算结果.下面的例子是官方给出的典型计算---每个元素加一加上每个元素加2之后再所有结果合起来相加 import dask def inc(x): return x + 1 def double(x): return x + 2 def add(x, y): return x + y data = [1, 2, 3, 4, 5] output = [] for x in data: a = dask.delayed(inc)(x) b = dask.delayed(double)(x) c = dask.delayed(add)(a, b) output.append(c) total = dask.delayed(sum)(output) total.compute() 45 使用装饰器delayed构造计算图 delayed也可以作为装饰器,这样在定义函数时就可以同时构造计算图了.这种方式其实并不推荐,因为这样反而降低了灵活性,不利于扩展 @dask.delayed def inc1(x): return x + 1 @dask.delayed def double1(x): return x + 2 @dask.delayed def add1(x, y): return x + y data = [1, 2, 3, 4, 5] output = [] for x in data: a = inc1(x) b = double1(x) c = add1(a, b) output.append(c) total1 = dask.delayed(sum)(output) total1.compute() 45 自定义计算图 实际上dask可以用户自定义计算图,我们通过将计算过程编码到一个dict对象构造计算图,然后使用dask.threaded.get接口将计算图送入集群来计算,get的第一个参数是计算图,第二个是计算图中定义的key.注意计算图中的key可以是字符串或者tuple.更多关于自定义计算图的优化方法需要看下相关文档 from dask.threaded import get def inc(i): return i + 1 def add(a, b): return a + b d = {'x': 1, 'y': (inc, 'x'), 'z': (add, 'y', 10)} get(d, 'x') 1 get(d, 'z') 12 计算图的高级接口 计算图的高级接口允许我们为计算分层 from dask.highlevelgraph import HighLevelGraph import pandas as pd import operator layers = { 'read-csv': {('read-csv', 0): (pd.read_csv, 'myfile.0.csv'), ('read-csv', 1): (pd.read_csv, 'myfile.1.csv'), ('read-csv', 2): (pd.read_csv, 'myfile.2.csv'), ('read-csv', 3): (pd.read_csv, 'myfile.3.csv')}, 'add': {('add', 0): (operator.add, ('read-csv', 0), 100), ('add', 1): (operator.add, ('read-csv', 1), 100), ('add', 2): (operator.add, ('read-csv', 2), 100), ('add', 3): (operator.add, ('read-csv', 3), 100)}, 'filter':{('filter', 0): (lambda part: part[part.name == 'Alice'], ('add', 0)), ('filter', 1): (lambda part: part[part.name == 'Alice'], ('add', 1)), ('filter', 2): (lambda part: part[part.name == 'Alice'], ('add', 2)), ('filter', 3): (lambda part: part[part.name == 'Alice'], ('add', 3))} } dependencies = {'read-csv': set(), 'add': {'read-csv'}, 'filter': {'add'}} graph = HighLevelGraph(layers, dependencies) 计算图可视化 visualize()接口可以将计算图可视化,这需要安装可选依赖python-graphviz total1.visualize() 集群执行由python函数组成的命令 要让集群可以执行命令,需要整个集群的python环境有对应需要的全部依赖,如果我们使用的swarm模式或者k8s模式部署的集群,那我们就需要为镜像单独安装依赖.一个比较好的方式是继承镜像重新打包: FROM daskdev/dask:latest RUN conda install pytorch ENTRYPOINT [\"tini\", \"-g\", \"--\", \"/usr/bin/prepare.sh\"] Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/实时任务提交.html":{"url":"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/实时任务提交.html","title":"实时任务提交","keywords":"","body":"实时任务提交 实时任务提交使用接口Futures,它只能是使用集群部署才可以使用,它可以将任务直接提交到集群,然后等待结果,任务执行是异步的. 它的接口基本参照标准库concurrent.futures主要接口包括: submit提交任务 map将任务分发到多个计算节点 as_completed构造一个生成器,Futures对象完成获取结果后yield出来 wait等待计算完成,可以设置超时 同时也提供了Queue,Lock,pub/sub,全局变量这样的工具,以应付数据同步这样的需求 实时任务提交主要用户构造服务和流数据处理.有点类似spark中spark-streaming的地位 同步提交 这种方式再README中就已经有演示,它和concurrent.futures几乎完全一样 from dask.distributed import Client client = Client('localhost:8786') def square(x): return x ** 2 A = client.map(square, range(10000)) total = client.submit(sum, A) print(total.result()) 333283335000 异步提交 dask的异步接口基于Tornado或者asyncio,默认使用的是Tornado,我们可以指定loop来指定使用什么事件循环. 在初始化Client对象时使用asynchronous=True,这样整个client将只有异步接口 %%writefile code/realtime/async_submit.py import asyncio import platform import multiprocessing from dask.distributed import Client def square(x): return x ** 2 async def f(): client = await Client(\"localhost:8786\", asynchronous=True) A = client.map(square, range(10000)) result = await client.submit(sum, A) print(result) await client.close() return result if __name__ == '__main__': if \"Windows\" in platform.platform(): multiprocessing.freeze_support() asyncio.get_event_loop().run_until_complete(f()) Overwriting code/realtime/async_submit.py 上面的方式我们使用的是基于进程池的Client对象,可以使用参数processes=False设定改为使用线程池的Client对象,这样就不需要在windows上使用multiprocessing.freeze_support()了 Actor模式[实验] 这可能是dask下一个大版本的主要新特性,这个模式下我们的任务不走调度器,而是直接交给worker通过actor对象执行,这样的好处是减少了调度时间可以响应更快,但坏处是无法通过调度器自动优化,debug,而且一旦在worker上创建了actor,那除非worker关闭否则完全无法关闭. Actor使用类定义,它可以快速的改变状态,我们可以像使用类实例一样的使用它,它同样也支持同步和异步两种方式 同步提交 class Counter: n = 0 def __init__(self): self.n = 0 def increment(self): self.n += 1 return self.n future = client.submit(Counter, actor=True) counter = future.result() counter Actor模式的对象submit后返回的是ActorFutures类型,它可以调用定义好的方法,也可以访问其中的属性 future = counter.increment() result = future.result() result 1 counter.n 1 异步提交 这个模式下异步接口更加方便,上面的内容可以写成如下异步实现 %%writefile code/realtime/async_actor.py import asyncio import platform import multiprocessing from dask.distributed import Client class Counter: n = 0 def __init__(self): self.n = 0 def increment(self): self.n += 1 return self.n async def f(): client = await Client(\"localhost:8786\", asynchronous=True) counter = await client.submit(Counter, actor=True) await counter.increment() n = await counter.n print(n) await client.close() return n if __name__ == '__main__': if \"Windows\" in platform.platform(): multiprocessing.freeze_support() asyncio.get_event_loop().run_until_complete(f()) Overwriting code/realtime/async_actor.py 上面的方式我们使用的是基于进程池的Client对象,可以使用参数processes=False设定改为使用线程池的Client对象,这样就不需要在windows上使用multiprocessing.freeze_support()了 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/流计算/流计算.html":{"url":"工具链篇/计算工具/数值计算/使用dask处理大规模数据的数值计算问题/流计算/流计算.html","title":"流计算","keywords":"","body":"流计算 dask的实时任务提交接口非常适合做流处理,通过结合streamz我们可以构造复杂的流处理系统. 流处理常用于处理数据流,构造数据处理管道.具体的内容我们后面数据科学部分再说. 使用streamz做流处理 streamz的基本接口主要有如下几类: 基本的的流处理结构 map(upstream, func, *args, **kwargs)用于将流中的每个数据分发给处理函数; Stream.emit(self, x[, asynchronous])用于将数据提交到流; sink(upstream, func, *args, **kwargs)则将函数应用于每个结果. 一个最简单最基本的流处理单元如下: from streamz import Stream def increment(x): return x**2 + 1 source = Stream() source.map(increment).sink(print) for i in range(10): source.emit(i) 1 2 5 10 17 26 37 50 65 82 设置缓冲区防止流堵塞 由于机器的性能或者短时间内大量数据的涌入,流处理可能会堵塞,我们可以使用buffer(upstream, n, **kwargs)来设置一个缓冲区,以应付这种情况.在上一个节点未能处理完之前如果有新的节点进来,那么它会被放在缓冲区. from streamz import Stream def increment(x): return x**2 + 1 source = Stream() source.buffer(100).map(increment).sink(print) for i in range(10): source.emit(i) 1 2 5 10 17 26 37 50 65 82 积累状态 使用接口accumulate(upstream, func[, start, …])可以积累流中的状态,类似reduce操作 from streamz import Stream def increment(x): return x**2 + 1 def add(x,y): return x+y source = Stream() source.map(increment).accumulate(add).sink(print) for i in range(10): source.emit(i) 1 3 8 18 35 61 98 148 213 295 操作流中元素 确保唯一性 可以使用unique(upstream[, maxsize, key, hashable])保证流中元素唯一性,这个接口可以设置maxsize参数 用于控制这个唯一性的范围(最近多少条没有重复),重复的元素则会被过滤掉;而参数key和python中其他itertools一样用于确定不重复的键如何取到. source = Stream() #source.unique(maxsize=1).sink(print) source.unique(maxsize=4).sink(print) for i in [1,2,3,2,3,2,2,4,5,6]: source.emit(i) print(\"##################\") source = Stream() source.unique(maxsize=1).sink(print) for i in [1,2,3,2,3,2,2,4,5,6]: source.emit(i) 1 2 3 4 5 6 ################## 1 2 3 2 3 2 4 5 6 过滤元素 可以使用filter(upstream, predicate, *args, **kwargs)来主动过滤一些符合条件的元素. source = Stream() source.filter(lambda x: x%2 == 0).sink(print) for i in range(10): source.emit(i) 0 2 4 6 8 将流数据压扁 类似spark中的flatten,flatten()也是一个功能, source = Stream() source.flatten().sink(print) for i in [[1, 2, 3], [4, 5], [6, 7, 7]]: source.emit(i) 1 2 3 4 5 6 7 7 延迟处理 借助事件循环,我们可以使用delay(upstream, interval, **kwargs)接口来推迟执行的时间. source = Stream() source.delay(1).filter(lambda x: x%2 == 0).sink(print) for i in range(10): source.emit(i) 0 2 4 6 8 固定时间间隔执行 rate_limit(upstream, interval, **kwargs)可以实现这个功能,它可以用于平稳流的执行. import time source = Stream() source.rate_limit(interval=1).sink(print) for i in range(10): source.emit(i) 0 1 2 3 4 5 6 7 8 9 截断缓存流 流处理一个很重要的功能就是计算序列中元素附近的一些统计量,这种时候截断与缓存就相当有用了. 每隔一定数量截断流构建元素 partition(upstream, n, **kwargs) 可以实现这个功能,不过一旦停止,多出来的元素会被抛弃 source = Stream() source.partition(n=3).sink(print) for i in range(10): source.emit(i) (0, 1, 2) (3, 4, 5) (6, 7, 8) 将元素保存在缓存中 streamz.collect(upstream, cache=None, **kwargs)可以将流中的元素保存到缓存中,并在调用对象的flush时将它们作为集合发出并清空缓存. source1 = Stream() source2 = Stream() collector = source1.collect() collector.sink(print) source2.sink(collector.flush) for i in range(10): source1.emit(i) source2.emit('anything') (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) 我们也可以设置cache参数,通常我们使用一个deque来限定缓存长度.外部定义的缓存也可以在流处理外部使用,这通常用于保存近期数据 from collections import deque cache = deque([],5) source1 = Stream() source2 = Stream() collector = source1.collect(cache=cache) collector.sink(print) source2.sink(collector.flush) for i in range(10): source1.emit(i) print(len(cache)) source2.emit('anything') 1 2 3 4 5 5 5 5 5 5 (5, 6, 7, 8, 9) cache deque([]) 按元素个数窗口截断并缓存元素 sliding_window(upstream, n, return_partial=True, **kwargs)可以实现这个功能,它会保存当前元素和最近的(n-1)个元素组成tuple构成一个新的元素,参数return_partial=True意味着在流开始时,元素前面元素不足够的情况下也会被构成tuple. source = Stream() source.sliding_window( n=3, return_partial=True).sink(print) for i in range(10): source.emit(i) (0,) (0, 1) (0, 1, 2) (1, 2, 3) (2, 3, 4) (3, 4, 5) (4, 5, 6) (5, 6, 7) (6, 7, 8) (7, 8, 9) source = Stream() source.sliding_window( n=3, return_partial=False).sink(print) for i in range(10): source.emit(i) (0, 1, 2) (1, 2, 3) (2, 3, 4) (3, 4, 5) (4, 5, 6) (5, 6, 7) (6, 7, 8) (7, 8, 9) 按时间窗口截断并缓存元素 timed_window(upstream, interval, **kwargs)可以实现这个功能.它与上面sliding_window不同之处就在于它是按时间截取,注意参数interval单位是秒.要注意一旦使用这个接口,它会启动一个事件循环来控制时间. import time source = Stream() source.timed_window( interval=3).sink(print) for i in range(30): time.sleep(0.5) source.emit(i) [] [0, 1, 2, 3, 4] [5, 6, 7, 8, 9, 10, 11] [12, 13, 14, 15, 16] [17, 18, 19, 20, 21, 22, 23] [24, 25, 26, 27, 28, 29] [] [] [] 组合流 python有生成器本身其实已经有一定的流操作能力,那为什么要用streamz呢,用它更多的是为了构造复杂的流组合.在业务上流操作也时常需要组合,比如计算一个 流分叉 我们的source对象可以用于构造流处理的计算图, def increment(x): return x + 1 def decrement(x): return x - 1 source = Stream() a = source.map(increment).sink(print) b = source.map(decrement).sink(print) b.visualize(rankdir='LR') for i in range(5): source.emit(i) 1 -1 2 0 3 1 4 2 5 3 流合并 union(*upstreams, **kwargs)将不同的流做合并操作 source1 = Stream() source2 = Stream() view = source1.union(source2).map(lambda x: x**2).sink(print) view.visualize(rankdir='LR') for i,j in zip(range(10),range(20,30)): source1.emit(i) source2.emit(j) 0 400 1 441 4 484 9 529 16 576 25 625 36 676 49 729 64 784 81 841 combine_latest(*upstreams, **kwargs)将不同流的最后两个元素组成tuple后合并,注意这个接口只要有一条流中有新数据传入就会触发计算 source1 = Stream() source2 = Stream() source1.combine_latest(source2).sink(print) for i,j in zip(range(10),range(20,30)): source1.emit(i) if j%2==0: source2.emit(j) (0, 20) (1, 20) (2, 20) (2, 22) (3, 22) (4, 22) (4, 24) (5, 24) (6, 24) (6, 26) (7, 26) (8, 26) (8, 28) (9, 28) zip(*upstreams, **kwargs)将不同流按次序对齐并组成tuple,这个行为和python默认的zip一致,只是这个接口会丢弃无法对齐的数据 source1 = Stream() source2 = Stream() source1.zip(source2).sink(print) for i,j in zip(range(10),range(20,30)): source1.emit(i) if j%2==0: source2.emit(j) (0, 20) (1, 22) (2, 24) (3, 26) (4, 28) zip_latest(lossless, *upstreams, **kwargs)这个接口会保证所有元素都被放出来,并且最长的一条不会重 source1 = Stream() source2 = Stream() source1.zip_latest(source2).sink(print) for i,j in zip(range(10),range(20,30)): source1.emit(i) if j%2==0: source2.emit(j) (0, 20) (1, 20) (2, 20) (3, 22) (4, 22) (5, 24) (6, 24) (7, 26) (8, 26) (9, 28) 递归与反馈 .connect(downstream)和.disconnect(downstream)可以用于将流连接起来,这常用于构造递归与反馈循环,下面的例子我们构造一个流用于遍历我们的文件系统 from streamz import Stream from pathlib import Path source = Stream() my_path = source.unique() my_path.sink(print) def inner_dir(x): p = Path(x) return [str(i) for i in p.iterdir()] content = my_path.filter(lambda x: Path(x).is_dir()) links = content.map(inner_dir).flatten() links.connect(source) # pipe new links back into pages source.emit('./') ./ 分布式数据结构.ipynb .DS_Store mystream.png source source/collections-schedulers.png README.ipynb dask作为算力池.ipynb code code/.DS_Store code/streaming code/streaming/streaming_async.py code/realtime code/realtime/async_actor.py code/realtime/.vscode code/realtime/.vscode/settings.json code/realtime/async_submit.py 实时任务提交.ipynb README.md .ipynb_checkpoints .ipynb_checkpoints/分布式数据结构-checkpoint.ipynb .ipynb_checkpoints/外部数据读取-checkpoint.ipynb .ipynb_checkpoints/README-checkpoint.ipynb .ipynb_checkpoints/流处理-checkpoint.ipynb .ipynb_checkpoints/dask作为算力池-checkpoint.ipynb .ipynb_checkpoints/实时任务提交-checkpoint.ipynb mydask.png 流处理.ipynb 服务化.ipynb 结合dask的实时流处理 最基础的流处理就是来一条处理一条的实时处理,这种模式模型最简单,但对系统的处理能力和网络io有较高要求,往往需要更多的资源.这个模式最知名的是storm框架.我们可以结合dask作为算力池直接利用集群算力来做实时流处理. 在streamz框架下,我们使用scatter()接口提交计算图,使用gather()接收流结果 同步写法 def increment(x): \"\"\" A blocking increment function Simulates a computational function that was not designed to work asynchronously \"\"\" time.sleep(0.1) return x + 1 def write(x): print(x) from dask.distributed import Client client = Client(\"localhost:8786\",processes=False) from streamz import Stream source = Stream() (source.scatter() # scatter local elements to cluster, creating a DaskStream .map(increment) # map a function remotely .buffer(5) # allow five futures to stay on the cluster at any time .gather() # bring results back to local process .sink(write)) # call write locally for x in range(10): source.emit(x) 1 2 3 4 5 6 7 8 9 10 异步写法 异步写法下,sink接口的参数可以是一个协程函数 %%writefile code/streaming/streaming_async.py import asyncio import time from dask.distributed import Client from streamz import Stream def increment(x): \"\"\" A blocking increment function Simulates a computational function that was not designed to work asynchronously \"\"\" time.sleep(0.1) return x + 1 async def write(x): print(x) async def f(): client = await Client(\"localhost:8786\", processes=False, asynchronous=True) source = Stream(asynchronous=True) source.scatter().map(increment).rate_limit('500ms').gather().sink(write) for x in range(10): await source.emit(x) if __name__ == '__main__': asyncio.get_event_loop().run_until_complete(f()) Overwriting code/streaming/streaming_async.py 批处理 更加常见的场景是使用流做批处理,我们当然可以直接使用上面的数据截断缓存方法来构造批数据,然后使用python函数处理.但显然这并不高效优雅. 另一种方式则是使用streamz提供的两种流数据结构 batch结构 batch结构用于构造由python数组组成的批数据, from dask.distributed import Client import json from streamz import Stream client = Client(\"localhost:8786\",processes=False) stream = Stream() example = [{'name': 'Alice', 'x': 1, 'y': 2}] batch = stream.to_batch(example=example) batch.stream.scatter().map(json.dumps).gather().sink(print) batch.emit(example) [{\"name\": \"Alice\", \"x\": 1, \"y\": 2}] dataframe结构,借助pandas,dask,甚至cupy import pandas as pd from streamz import Stream from streamz.dataframe import DataFrame stream = Stream() df_sample = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]}) sdf = DataFrame(example=df_sample, stream=stream) L = sdf.stream.scatter().map(lambda x: x[\"x\"]**2+3).gather().sink(print) sdf.emit(df_sample) 0 4 1 7 2 12 Name: x, dtype: int64 dataframe只能处理结构化数据,这个例子中我们首先使用pandas构造了一个dataframe的Scheme(设计方案),df_sample,后续的所有使用emit传入的dataframe就固定好了从流中解析出的是这个样式.这个例子可以看出我们可以构造数据类型dataframe来做批处理,但如何构造批呢? 将流构造为批进行批处理 我们可以一批一批的处理数据了,但流并不会自己成为批数据,这时候我们就可以使用前面提到的截断缓存流数据的方法构造批数据了.这边以partition方法为例. 下面的例子的过程如下: 我们使用partition(5)将流截断缓存为3个一组,再将这5个一组的流转换为批对象,接着将批对象转换为dataframe,然后用dataframe的接口求取每5个的value字段的均值.这定义了流数据批处理的整个过程 在构造好这个处理流程后,我们提出流程的stream对象,定义它的数据提交,收集和下沉过程. 最后我们向流对象中喂入数据. import pandas as pd import random from streamz import Stream from dask.distributed import Client from streamz.dataframe import DataFrame # 定义流 client = Client(\"localhost:8786\",processes=False) stream = Stream() # 定义批处理过程 keys = [\"a\",\"b\",\"c\"] batch_sample = [{'key': \"a\", 'value': 20}] batch = stream.partition(5).to_batch(example=batch_sample) sdf = batch.to_dataframe() process = sdf.value.mean() # 定义数据提交,收集,下沉过程 process.stream.scatter().gather().sink(print) # 向流中注入数据 for i in range(100): stream.emit({\"key\":random.choice(keys),\"value\":random.randint(20,100)}) 64.8 65.5 67.53333333333333 65.8 64.84 68.76666666666667 71.2 70.75 68.31111111111112 66.8 65.89090909090909 65.71666666666667 65.8 66.48571428571428 64.88 63.675 64.0 64.17777777777778 63.61052631578947 63.11 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/":{"url":"工具链篇/数据可视化工具/","title":"数据可视化工具","keywords":"","body":"数据可视化 数据可视化只是工具,它本身并不产生结论,它只是表现而已.但数据可视化却非常重要,人是视觉动物,图表远比文字表现力强.何况数据可视化服务的对象通常还是未受过数据方面专业训练的人. 本文将主要介绍如何利用matplotlib和seaborn实现数据可视化. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/","title":"使用matplotlib做数据可视化","keywords":"","body":"使用matplotlib做数据可视化 matplotlib和numpy一样,是事实上的python\"标准库\",是被使用最多的二维绘图Python包.它不仅提供一个非常快捷的用python可视化数据的方法,而且提供了出版质量的多种格式图像.不过遗憾的是pypy目前并不支持.matplotlib主要是一个绘图工具,大多数的数据科学工具都对他支持良好. 它体系庞大复杂,可以绘制各种常规图形,也可以绘制点线构成自定义的图形,可以是2d图形也可以画3d图形,可以绘制图片也可以构建简单动画,甚至于还有个模块爬取美股信息 本文将从多个角度介绍matplotlib Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/matplotlib的基本设置/matplotlib的基本设置.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/matplotlib的基本设置/matplotlib的基本设置.html","title":"matplotlib的基本设置","keywords":"","body":"matplotlib的基本设置 绘图从来就是个很复杂的东西,各种样式各种设置非常复杂,不信的同学可以拿latex类比下. matplotlib设置方式可以分为三种: 使用内置的配置主题 临时设置 使用配置文件matplotlibrc import matplotlib.pyplot as plt import matplotlib import numpy as np %matplotlib inline 通常matplotlib在linux下的设置文件放在~/.config/matplotlib/下但也会有特殊,我们可以用下面的代码查看配置文件的位置 matplotlib.get_configdir() '/Users/huangsizhe/.matplotlib' 使用内置的配置主题 matplotlib内置了许多基本的设置主题可以通过matplotlib.pyplot.style.available查看 plt.style.available ['bmh', 'classic', 'dark_background', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'seaborn', 'animation_support', 'chinese_support'] 样式的用法有两种, 一种是全局使用,plt.style.use('seaborn-darkgrid') 设定好之后所有下面的图将都是这一样式 另一种是临时使用样式,可以使用plt.style.context结合with语句,构建上下文环境 我们来看看他们的效果大约是啥样 x=[1,2,3,4,5,6,7,8] y=[2,1,3,5,2,6,12,7] with plt.style.context(('seaborn-darkgrid')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-notebook')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('classic')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-ticks')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('grayscale')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('bmh')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-talk')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('dark_background')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('ggplot')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('fivethirtyeight')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-colorblind')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-deep')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-whitegrid')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-bright')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-poster')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-muted')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-paper')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-white')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-pastel')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-dark')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-dark-palette')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() 这些主题都是可以组合使用的 with plt.style.context(('fivethirtyeight','seaborn-whitegrid','seaborn-pastel')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() with plt.style.context(('seaborn-whitegrid','seaborn-pastel','fivethirtyeight')): plt.figure(figsize=(5,4)) plt.plot(x,y) plt.show() 可以看出,如果有冲突,后定义的会覆盖先定义的. 临时修改配置 matplotlib允许临时修改配置,使用的是matplotlib.rcParams matplotlib.rcParams['lines.linewidth'] = 2 matplotlib.rcParams['lines.color'] = 'r' 或者使用matplotlib.rc(group, **kwargs)来快速的为分组做设定 matplotlib.rc('lines', linewidth=2, color='r') 使用matplotlibrc文件全局的设置 matplotlib会以 本地->环境变量MATPLOTLIBRC/matplotlibrc指定位置->用户配置位置(linux:.config/matplotlib/matplotlibrc,other:.matplotlib/matplotlibrc),->matplotlib安装根目录/matplotlib/mpl-data/matplotlibrc 的顺序查找matplotlibrc,用它就可以配置需要的设置了,这个方法是全局默认加载的. 一份设置文档大约是这个样子,和python规则一样,#代表注释 ### MATPLOTLIBRC FORMAT # This is a sample matplotlib configuration file - you can find a copy # of it on your system in # site-packages/matplotlib/mpl-data/matplotlibrc. If you edit it # there, please note that it will be overwritten in your next install. # If you want to keep a permanent local copy that will not be # overwritten, place it in the following location: # unix/linux: # $HOME/.config/matplotlib/matplotlibrc or # $XDG_CONFIG_HOME/matplotlib/matplotlibrc (if $XDG_CONFIG_HOME is set) # other platforms: # $HOME/.matplotlib/matplotlibrc # # See http://matplotlib.org/users/customizing.html#the-matplotlibrc-file for # more details on the paths which are checked for the configuration file. # # This file is best viewed in a editor which supports python mode # syntax highlighting. Blank lines, or lines starting with a comment # symbol, are ignored, as are trailing comments. Other lines must # have the format # key : val # optional comment # # Colors: for the color values below, you can either use - a # matplotlib color string, such as r, k, or b - an rgb tuple, such as # (1.0, 0.5, 0.0) - a hex string, such as ff00ff - a scalar # grayscale intensity such as 0.75 - a legal html color name, e.g., red, # blue, darkslategray #### CONFIGURATION BEGINS HERE # The default backend; one of GTK GTKAgg GTKCairo GTK3Agg GTK3Cairo # MacOSX Qt4Agg Qt5Agg TkAgg WX WXAgg Agg Cairo GDK PS PDF SVG # Template. # You can also deploy your own backend outside of matplotlib by # referring to the module name (which must be in the PYTHONPATH) as # 'module://my_backend'. backend : tkagg # If you are using the Qt4Agg backend, you can choose here # to use the PyQt4 bindings or the newer PySide bindings to # the underlying Qt4 toolkit. #backend.qt4 : PyQt4 # PyQt4 | PySide # Note that this can be overridden by the environment variable # QT_API used by Enthought Tool Suite (ETS); valid values are # \"pyqt\" and \"pyside\". The \"pyqt\" setting has the side effect of # forcing the use of Version 2 API for QString and QVariant. # The port to use for the web server in the WebAgg backend. # webagg.port : 8888 # If webagg.port is unavailable, a number of other random ports will # be tried until one that is available is found. # webagg.port_retries : 50 # When True, open the webbrowser to the plot that is shown # webagg.open_in_browser : True # When True, the figures rendered in the nbagg backend are created with # a transparent background. # nbagg.transparent : False # if you are running pyplot inside a GUI and your backend choice # conflicts, we will automatically try to find a compatible one for # you if backend_fallback is True #backend_fallback: True #interactive : False #toolbar : toolbar2 # None | toolbar2 (\"classic\" is deprecated) #timezone : UTC # a pytz timezone string, e.g., US/Central or Europe/Paris # Where your matplotlib data lives if you installed to a non-default # location. This is where the matplotlib fonts, bitmaps, etc reside #datapath : /home/jdhunter/mpldata ### LINES # See http://matplotlib.org/api/artist_api.html#module-matplotlib.lines for more # information on line properties. #lines.linewidth : 1.5 # line width in points #lines.linestyle : - # solid line #lines.color : C0 # has no affect on plot(); see axes.prop_cycle #lines.marker : None # the default marker #lines.markeredgewidth : 1.0 # the line width around the marker symbol #lines.markersize : 6 # markersize, in points #lines.dash_joinstyle : miter # miter|round|bevel #lines.dash_capstyle : butt # butt|round|projecting #lines.solid_joinstyle : miter # miter|round|bevel #lines.solid_capstyle : projecting # butt|round|projecting #lines.antialiased : True # render lines in antialiased (no jaggies) # The three standard dash patterns. These are scaled by the linewidth. #lines.dashed_pattern : 2.8, 1.2 #lines.dashdot_pattern : 4.8, 1.2, 0.8, 1.2 #lines.dotted_pattern : 1.1, 1.1 #lines.scale_dashes : True #markers.fillstyle: full # full|left|right|bottom|top|none ### PATCHES # Patches are graphical objects that fill 2D space, like polygons or # circles. See # http://matplotlib.org/api/artist_api.html#module-matplotlib.patches # information on patch properties #patch.linewidth : 1 # edge width in points. #patch.facecolor : C0 #patch.edgecolor : black # if forced, or patch is not filled #patch.force_edgecolor : False # True to always use edgecolor #patch.antialiased : True # render patches in antialiased (no jaggies) ### HATCHES #hatch.color : k #hatch.linewidth : 1.0 ### Boxplot #boxplot.notch : False #boxplot.vertical : True #boxplot.whiskers : 1.5 #boxplot.bootstrap : None #boxplot.patchartist : False #boxplot.showmeans : False #boxplot.showcaps : True #boxplot.showbox : True #boxplot.showfliers : True #boxplot.meanline : False #boxplot.flierprops.color : 'k' #boxplot.flierprops.marker : 'o' #boxplot.flierprops.markerfacecolor : 'none' #boxplot.flierprops.markeredgecolor : 'k' #boxplot.flierprops.markersize : 6 #boxplot.flierprops.linestyle : 'none' #boxplot.flierprops.linewidth : 1.0 #boxplot.boxprops.color : 'k' #boxplot.boxprops.linewidth : 1.0 #boxplot.boxprops.linestyle : '-' #boxplot.whiskerprops.color : 'k' #boxplot.whiskerprops.linewidth : 1.0 #boxplot.whiskerprops.linestyle : '-' #boxplot.capprops.color : 'k' #boxplot.capprops.linewidth : 1.0 #boxplot.capprops.linestyle : '-' #boxplot.medianprops.color : 'C1' #boxplot.medianprops.linewidth : 1.0 #boxplot.medianprops.linestyle : '-' #boxplot.meanprops.color : 'C2' #boxplot.meanprops.marker : '^' #boxplot.meanprops.markerfacecolor : 'C2' #boxplot.meanprops.markeredgecolor : 'C2' #boxplot.meanprops.markersize : 6 #boxplot.meanprops.linestyle : 'none' #boxplot.meanprops.linewidth : 1.0 ### FONT # # font properties used by text.Text. See # http://matplotlib.org/api/font_manager_api.html for more # information on font properties. The 6 font properties used for font # matching are given below with their default values. # # The font.family property has five values: 'serif' (e.g., Times), # 'sans-serif' (e.g., Helvetica), 'cursive' (e.g., Zapf-Chancery), # 'fantasy' (e.g., Western), and 'monospace' (e.g., Courier). Each of # these font families has a default list of font names in decreasing # order of priority associated with them. When text.usetex is False, # font.family may also be one or more concrete font names. # # The font.style property has three values: normal (or roman), italic # or oblique. The oblique style will be used for italic, if it is not # present. # # The font.variant property has two values: normal or small-caps. For # TrueType fonts, which are scalable fonts, small-caps is equivalent # to using a font size of 'smaller', or about 83%% of the current font # size. # # The font.weight property has effectively 13 values: normal, bold, # bolder, lighter, 100, 200, 300, ..., 900. Normal is the same as # 400, and bold is 700. bolder and lighter are relative values with # respect to the current weight. # # The font.stretch property has 11 values: ultra-condensed, # extra-condensed, condensed, semi-condensed, normal, semi-expanded, # expanded, extra-expanded, ultra-expanded, wider, and narrower. This # property is not currently implemented. # # The font.size property is the default font size for text, given in pts. # 10 pt is the standard value. # #font.family : sans-serif #font.style : normal #font.variant : normal #font.weight : medium #font.stretch : normal # note that font.size controls default text sizes. To configure # special text sizes tick labels, axes, labels, title, etc, see the rc # settings for axes and ticks. Special text sizes can be defined # relative to font.size, using the following values: xx-small, x-small, # small, medium, large, x-large, xx-large, larger, or smaller #font.size : 10.0 #font.serif : DejaVu Serif, Bitstream Vera Serif, New Century Schoolbook, Century Schoolbook L, Utopia, ITC Bookman, Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, serif #font.sans-serif : DejaVu Sans, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif #font.cursive : Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, cursive #font.fantasy : Comic Sans MS, Chicago, Charcoal, Impact, Western, Humor Sans, xkcd, fantasy #font.monospace : DejaVu Sans Mono, Bitstream Vera Sans Mono, Andale Mono, Nimbus Mono L, Courier New, Courier, Fixed, Terminal, monospace ### TEXT # text properties used by text.Text. See # http://matplotlib.org/api/artist_api.html#module-matplotlib.text for more # information on text properties #text.color : black ### LaTeX customizations. See http://wiki.scipy.org/Cookbook/Matplotlib/UsingTex #text.usetex : False # use latex for all text handling. The following fonts # are supported through the usual rc parameter settings: # new century schoolbook, bookman, times, palatino, # zapf chancery, charter, serif, sans-serif, helvetica, # avant garde, courier, monospace, computer modern roman, # computer modern sans serif, computer modern typewriter # If another font is desired which can loaded using the # LaTeX \\usepackage command, please inquire at the # matplotlib mailing list #text.latex.unicode : False # use \"ucs\" and \"inputenc\" LaTeX packages for handling # unicode strings. #text.latex.preamble : # IMPROPER USE OF THIS FEATURE WILL LEAD TO LATEX FAILURES # AND IS THEREFORE UNSUPPORTED. PLEASE DO NOT ASK FOR HELP # IF THIS FEATURE DOES NOT DO WHAT YOU EXPECT IT TO. # preamble is a comma separated list of LaTeX statements # that are included in the LaTeX document preamble. # An example: # text.latex.preamble : \\usepackage{bm},\\usepackage{euler} # The following packages are always loaded with usetex, so # beware of package collisions: color, geometry, graphicx, # type1cm, textcomp. Adobe Postscript (PSSNFS) font packages # may also be loaded, depending on your font settings #text.dvipnghack : None # some versions of dvipng don't handle alpha # channel properly. Use True to correct # and flush ~/.matplotlib/tex.cache # before testing and False to force # correction off. None will try and # guess based on your dvipng version #text.hinting : auto # May be one of the following: # 'none': Perform no hinting # 'auto': Use FreeType's autohinter # 'native': Use the hinting information in the # font file, if available, and if your # FreeType library supports it # 'either': Use the native hinting information, # or the autohinter if none is available. # For backward compatibility, this value may also be # True === 'auto' or False === 'none'. #text.hinting_factor : 8 # Specifies the amount of softness for hinting in the # horizontal direction. A value of 1 will hint to full # pixels. A value of 2 will hint to half pixels etc. #text.antialiased : True # If True (default), the text will be antialiased. # This only affects the Agg backend. # The following settings allow you to select the fonts in math mode. # They map from a TeX font name to a fontconfig font pattern. # These settings are only used if mathtext.fontset is 'custom'. # Note that this \"custom\" mode is unsupported and may go away in the # future. #mathtext.cal : cursive #mathtext.rm : serif #mathtext.tt : monospace #mathtext.it : serif:italic #mathtext.bf : serif:bold #mathtext.sf : sans #mathtext.fontset : dejavusans # Should be 'dejavusans' (default), # 'dejavuserif', 'cm' (Computer Modern), 'stix', # 'stixsans' or 'custom' #mathtext.fallback_to_cm : True # When True, use symbols from the Computer Modern # fonts when a symbol can not be found in one of # the custom math fonts. #mathtext.default : it # The default font to use for math. # Can be any of the LaTeX font names, including # the special name \"regular\" for the same font # used in regular text. ### AXES # default face and edge color, default tick sizes, # default fontsizes for ticklabels, and so on. See # http://matplotlib.org/api/axes_api.html#module-matplotlib.axes #axes.facecolor : white # axes background color #axes.edgecolor : black # axes edge color #axes.linewidth : 0.8 # edge linewidth #axes.grid : False # display grid or not #axes.titlesize : large # fontsize of the axes title #axes.titlepad : 4.0 # pad between axes and title in points #axes.labelsize : medium # fontsize of the x any y labels #axes.labelpad : 4.0 # space between label and axis #axes.labelweight : normal # weight of the x and y labels #axes.labelcolor : black #axes.axisbelow : 'line' # draw axis gridlines and ticks below # patches (True); above patches but below # lines ('line'); or above all (False) #axes.formatter.limits : -7, 7 # use scientific notation if log10 # of the axis range is smaller than the # first or larger than the second #axes.formatter.use_locale : False # When True, format tick labels # according to the user's locale. # For example, use ',' as a decimal # separator in the fr_FR locale. #axes.formatter.use_mathtext : False # When True, use mathtext for scientific # notation. #axes.formatter.useoffset : True # If True, the tick label formatter # will default to labeling ticks relative # to an offset when the data range is # small compared to the minimum absolute # value of the data. #axes.formatter.offset_threshold : 4 # When useoffset is True, the offset # will be used when it can remove # at least this number of significant # digits from tick labels. # axes.spines.left : True # display axis spines # axes.spines.bottom : True # axes.spines.top : True # axes.spines.right : True #axes.unicode_minus : True # use unicode for the minus symbol # rather than hyphen. See # http://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes #axes.prop_cycle : cycler('color', # ['1f77b4', 'ff7f0e', '2ca02c', 'd62728', # '9467bd', '8c564b', 'e377c2', '7f7f7f', # 'bcbd22', '17becf']) # color cycle for plot lines # as list of string colorspecs: # single letter, long name, or # web-style hex #axes.autolimit_mode : data # How to scale axes limits to the data. # Use \"data\" to use data limits, plus some margin # Use \"round_number\" move to the nearest \"round\" number #axes.xmargin : .05 # x margin. See `axes.Axes.margins` #axes.ymargin : .05 # y margin See `axes.Axes.margins` #polaraxes.grid : True # display grid on polar axes #axes3d.grid : True # display grid on 3d axes ### DATES # These control the default format strings used in AutoDateFormatter. # Any valid format datetime format string can be used (see the python # `datetime` for details). For example using '%%x' will use the locale date representation # '%%X' will use the locale time representation and '%%c' will use the full locale datetime # representation. # These values map to the scales: # {'year': 365, 'month': 30, 'day': 1, 'hour': 1/24, 'minute': 1 / (24 * 60)} # date.autoformatter.year : %Y # date.autoformatter.month : %Y-%m # date.autoformatter.day : %Y-%m-%d # date.autoformatter.hour : %H:%M # date.autoformatter.minute : %H:%M:%S # date.autoformatter.second : %H:%M:%S # date.autoformatter.microsecond : %H:%M:%S.%f ### TICKS # see http://matplotlib.org/api/axis_api.html#matplotlib.axis.Tick #xtick.top : False # draw ticks on the top side #xtick.bottom : True # draw ticks on the bottom side #xtick.major.size : 3.5 # major tick size in points #xtick.minor.size : 2 # minor tick size in points #xtick.major.width : 0.8 # major tick width in points #xtick.minor.width : 0.6 # minor tick width in points #xtick.major.pad : 3.5 # distance to major tick label in points #xtick.minor.pad : 3.4 # distance to the minor tick label in points #xtick.color : k # color of the tick labels #xtick.labelsize : medium # fontsize of the tick labels #xtick.direction : out # direction: in, out, or inout #xtick.minor.visible : False # visibility of minor ticks on x-axis #xtick.major.top : True # draw x axis top major ticks #xtick.major.bottom : True # draw x axis bottom major ticks #xtick.minor.top : True # draw x axis top minor ticks #xtick.minor.bottom : True # draw x axis bottom minor ticks #ytick.left : True # draw ticks on the left side #ytick.right : False # draw ticks on the right side #ytick.major.size : 3.5 # major tick size in points #ytick.minor.size : 2 # minor tick size in points #ytick.major.width : 0.8 # major tick width in points #ytick.minor.width : 0.6 # minor tick width in points #ytick.major.pad : 3.5 # distance to major tick label in points #ytick.minor.pad : 3.4 # distance to the minor tick label in points #ytick.color : k # color of the tick labels #ytick.labelsize : medium # fontsize of the tick labels #ytick.direction : out # direction: in, out, or inout #ytick.minor.visible : False # visibility of minor ticks on y-axis #xtick.major.left : True # draw y axis left major ticks #xtick.major.right : True # draw y axis right major ticks #xtick.minor.left : True # draw y axis left minor ticks #xtick.minor.right : True # draw y axis right minor ticks ### GRIDS #grid.color : b0b0b0 # grid color #grid.linestyle : - # solid #grid.linewidth : 0.8 # in points #grid.alpha : 1.0 # transparency, between 0.0 and 1.0 ### Legend #legend.loc : best #legend.frameon : True # if True, draw the legend on a background patch #legend.framealpha : 0.8 # legend patch transparency #legend.facecolor : inherit # inherit from axes.facecolor; or color spec #legend.edgecolor : 0.8 # background patch boundary color #legend.fancybox : True # if True, use a rounded box for the # legend background, else a rectangle #legend.shadow : False # if True, give background a shadow effect #legend.numpoints : 1 # the number of marker points in the legend line #legend.scatterpoints : 1 # number of scatter points #legend.markerscale : 1.0 # the relative size of legend markers vs. original #legend.fontsize : medium # Dimensions as fraction of fontsize: #legend.borderpad : 0.4 # border whitespace #legend.labelspacing : 0.5 # the vertical space between the legend entries #legend.handlelength : 2.0 # the length of the legend lines #legend.handleheight : 0.7 # the height of the legend handle #legend.handletextpad : 0.8 # the space between the legend line and legend text #legend.borderaxespad : 0.5 # the border between the axes and legend edge #legend.columnspacing : 2.0 # column separation ### FIGURE # See http://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure #figure.titlesize : large # size of the figure title (Figure.suptitle()) #figure.titleweight : normal # weight of the figure title #figure.figsize : 6.4, 4.8 # figure size in inches #figure.dpi : 100 # figure dots per inch #figure.facecolor : white # figure facecolor; 0.75 is scalar gray #figure.edgecolor : white # figure edgecolor #figure.autolayout : False # When True, automatically adjust subplot # parameters to make the plot fit the figure #figure.max_open_warning : 20 # The maximum number of figures to open through # the pyplot interface before emitting a warning. # If less than one this feature is disabled. # The figure subplot parameters. All dimensions are a fraction of the #figure.subplot.left : 0.125 # the left side of the subplots of the figure #figure.subplot.right : 0.9 # the right side of the subplots of the figure #figure.subplot.bottom : 0.11 # the bottom of the subplots of the figure #figure.subplot.top : 0.88 # the top of the subplots of the figure #figure.subplot.wspace : 0.2 # the amount of width reserved for blank space between subplots, # expressed as a fraction of the average axis width #figure.subplot.hspace : 0.2 # the amount of height reserved for white space between subplots, # expressed as a fraction of the average axis height ### IMAGES #image.aspect : equal # equal | auto | a number #image.interpolation : nearest # see help(imshow) for options #image.cmap : viridis # A colormap name, gray etc... #image.lut : 256 # the size of the colormap lookup table #image.origin : upper # lower | upper #image.resample : True #image.composite_image : True # When True, all the images on a set of axes are # combined into a single composite image before # saving a figure as a vector graphics file, # such as a PDF. ### CONTOUR PLOTS #contour.negative_linestyle : dashed # dashed | solid #contour.corner_mask : True # True | False | legacy ### ERRORBAR PLOTS #errorbar.capsize : 0 # length of end cap on error bars in pixels ### HISTOGRAM PLOTS #hist.bins : 10 # The default number of histogram bins. # If Numpy 1.11 or later is # installed, may also be `auto` ### SCATTER PLOTS #scatter.marker : o # The default marker type for scatter plots. ### Agg rendering ### Warning: experimental, 2008/10/10 #agg.path.chunksize : 0 # 0 to disable; values in the range # 10000 to 100000 can improve speed slightly # and prevent an Agg rendering failure # when plotting very large data sets, # especially if they are very gappy. # It may cause minor artifacts, though. # A value of 20000 is probably a good # starting point. ### SAVING FIGURES #path.simplify : True # When True, simplify paths by removing \"invisible\" # points to reduce file size and increase rendering # speed #path.simplify_threshold : 0.1 # The threshold of similarity below which # vertices will be removed in the simplification # process #path.snap : True # When True, rectilinear axis-aligned paths will be snapped to # the nearest pixel when certain criteria are met. When False, # paths will never be snapped. #path.sketch : None # May be none, or a 3-tuple of the form (scale, length, # randomness). # *scale* is the amplitude of the wiggle # perpendicular to the line (in pixels). *length* # is the length of the wiggle along the line (in # pixels). *randomness* is the factor by which # the length is randomly scaled. # the default savefig params can be different from the display params # e.g., you may want a higher resolution, or to make the figure # background white #savefig.dpi : figure # figure dots per inch or 'figure' #savefig.facecolor : white # figure facecolor when saving #savefig.edgecolor : white # figure edgecolor when saving #savefig.format : png # png, ps, pdf, svg #savefig.bbox : standard # 'tight' or 'standard'. # 'tight' is incompatible with pipe-based animation # backends but will workd with temporary file based ones: # e.g. setting animation.writer to ffmpeg will not work, # use ffmpeg_file instead #savefig.pad_inches : 0.1 # Padding to be used when bbox is set to 'tight' #savefig.jpeg_quality: 95 # when a jpeg is saved, the default quality parameter. #savefig.directory : ~ # default directory in savefig dialog box, # leave empty to always use current working directory #savefig.transparent : False # setting that controls whether figures are saved with a # transparent background by default # tk backend params #tk.window_focus : False # Maintain shell focus for TkAgg # ps backend params #ps.papersize : letter # auto, letter, legal, ledger, A0-A10, B0-B10 #ps.useafm : False # use of afm fonts, results in small files #ps.usedistiller : False # can be: None, ghostscript or xpdf # Experimental: may produce smaller files. # xpdf intended for production of publication quality files, # but requires ghostscript, xpdf and ps2eps #ps.distiller.res : 6000 # dpi #ps.fonttype : 3 # Output Type 3 (Type3) or Type 42 (TrueType) # pdf backend params #pdf.compression : 6 # integer from 0 to 9 # 0 disables compression (good for debugging) #pdf.fonttype : 3 # Output Type 3 (Type3) or Type 42 (TrueType) # svg backend params #svg.image_inline : True # write raster image data directly into the svg file #svg.fonttype : 'path' # How to handle SVG fonts: # 'none': Assume fonts are installed on the machine where the SVG will be viewed. # 'path': Embed characters as paths -- supported by most SVG renderers # 'svgfont': Embed characters as SVG fonts -- supported only by Chrome, # Opera and Safari #svg.hashsalt : None # if not None, use this string as hash salt # instead of uuid4 # docstring params #docstring.hardcopy = False # set this when you want to generate hardcopy docstring # Set the verbose flags. This controls how much information # matplotlib gives you at runtime and where it goes. The verbosity # levels are: silent, helpful, debug, debug-annoying. Any level is # inclusive of all the levels below it. If your setting is \"debug\", # you'll get all the debug and helpful messages. When submitting # problems to the mailing-list, please set verbose to \"helpful\" or \"debug\" # and paste the output into your report. # # The \"fileo\" gives the destination for any calls to verbose.report. # These objects can a filename, or a filehandle like sys.stdout. # # You can override the rc default verbosity from the command line by # giving the flags --verbose-LEVEL where LEVEL is one of the legal # levels, e.g., --verbose-helpful. # # You can access the verbose instance in your code # from matplotlib import verbose. #verbose.level : silent # one of silent, helpful, debug, debug-annoying #verbose.fileo : sys.stdout # a log filename, sys.stdout or sys.stderr # Event keys to interact with figures/plots via keyboard. # Customize these settings according to your needs. # Leave the field(s) empty if you don't need a key-map. (i.e., fullscreen : '') #keymap.fullscreen : f # toggling #keymap.home : h, r, home # home or reset mnemonic #keymap.back : left, c, backspace # forward / backward keys to enable #keymap.forward : right, v # left handed quick navigation #keymap.pan : p # pan mnemonic #keymap.zoom : o # zoom mnemonic #keymap.save : s # saving current figure #keymap.quit : ctrl+w, cmd+w # close the current figure #keymap.grid : g # switching on/off a grid in current axes #keymap.yscale : l # toggle scaling of y-axes ('log'/'linear') #keymap.xscale : L, k # toggle scaling of x-axes ('log'/'linear') #keymap.all_axes : a # enable all axes # Control location of examples data files #examples.directory : '' # directory to look in for custom installation ###ANIMATION settings #animation.html : 'none' # How to display the animation as HTML in # the IPython notebook. 'html5' uses # HTML5 video tag. #animation.writer : ffmpeg # MovieWriter 'backend' to use #animation.codec : h264 # Codec to use for writing movie #animation.bitrate: -1 # Controls size/quality tradeoff for movie. # -1 implies let utility auto-determine #animation.frame_format: 'png' # Controls frame format used by temp files #animation.ffmpeg_path: 'ffmpeg' # Path to ffmpeg binary. Without full path # $PATH is searched #animation.ffmpeg_args: '' # Additional arguments to pass to ffmpeg #animation.avconv_path: 'avconv' # Path to avconv binary. Without full path # $PATH is searched #animation.avconv_args: '' # Additional arguments to pass to avconv #animation.mencoder_path: 'mencoder' # Path to mencoder binary. Without full path # $PATH is searched #animation.mencoder_args: '' # Additional arguments to pass to mencoder #animation.convert_path: 'convert' # Path to ImageMagick's convert binary. # On Windows use the full path since convert # is also the name of a system tool. 自定义主题(支持中文字体) 我们当然可以自己定义自己的主题,他的内容和matplotlibrc一样,但以.mplstyle作为后缀,自定义的主题放在mpl_configdir/stylelib下就可以被识别,比如我们定义一个专用于可以显示中文的主题chinese_support.mplstyle 第一步,下载字体,我们使用[微软雅黑],下载好后放在自己的设置文件夹matplotlib安装根目录/matplotlib/mpl-data/下的fonts/ttf文件夹中(这步如果已经有字体文件可以省略) 第二步,在你的设置文件夹下的stylelib文件夹下(没有就自己创建)写下 font.family : LiHei ProLi,Song Pro,Microsoft YaHei, sans-serif 为了跨平台,可以把Microsoft YaHei放到前面,但个人觉得没苹果的字体好看,就算了 第三步,删除fontList.cache文件然后重启即可 查看自己有哪些字体可以使用如下命令 from matplotlib.font_manager import FontManager import subprocess fm = FontManager() mat_fonts = set(f.name for f in fm.ttflist) mat_fonts {'.Keyboard', '.LastResort', 'Andale Mono', 'Apple Braille', 'Apple Chancery', 'AppleGothic', 'AppleMyungjo', 'Arial', 'Arial Black', 'Arial Narrow', 'Arial Rounded MT Bold', 'Arial Unicode MS', 'Ayuthaya', 'Big Caslon', 'Bodoni 72 Smallcaps', 'Bodoni Ornaments', 'Bradley Hand', 'Brush Script MT', 'Chalkduster', 'Comic Sans MS', 'Courier New', 'DIN Alternate', 'DIN Condensed', 'DejaVu Sans', 'DejaVu Sans Display', 'DejaVu Sans Mono', 'DejaVu Serif', 'DejaVu Serif Display', 'Diwan Thuluth', 'Farisi', 'Georgia', 'GungSeo', 'Gurmukhi MT', 'HeadLineA', 'Herculanum', 'Hoefler Text', 'Impact', 'InaiMathi', 'Khmer Sangam MN', 'Kokonor', 'Krungthep', 'Lao Sangam MN', 'LiHei Pro', 'LiSong Pro', 'Luminari', 'Microsoft Sans Serif', 'Mishafi', 'Mishafi Gold', 'Osaka', 'PCMyungjo', 'PilGi', 'Plantagenet Cherokee', 'STFangsong', 'STHeiti', 'STIXGeneral', 'STIXIntegralsD', 'STIXIntegralsSm', 'STIXIntegralsUp', 'STIXIntegralsUpD', 'STIXIntegralsUpSm', 'STIXNonUnicode', 'STIXSizeFiveSym', 'STIXSizeFourSym', 'STIXSizeOneSym', 'STIXSizeThreeSym', 'STIXSizeTwoSym', 'STIXVariants', 'Sathu', 'SignPainter', 'Silom', 'Skia', 'Symbol', 'System Font', 'Tahoma', 'Times New Roman', 'Trattatello', 'Trebuchet MS', 'Verdana', 'Wawati SC', 'Wawati TC', 'Webdings', 'Weibei SC', 'Weibei TC', 'Wingdings', 'Wingdings 2', 'Wingdings 3', 'YuGothic', 'Yuppy SC', 'Yuppy TC', 'Zapf Dingbats', 'Zapfino', 'cmb10', 'cmex10', 'cmmi10', 'cmr10', 'cmss10', 'cmsy10', 'cmtt10'} X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\",label=u\"余弦\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\",label=u\"正弦\") plt.legend(loc='upper left')#图例位置 plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) plt.show() plt.style.use('chinese_support') X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\",label=u\"余弦\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\",label=u\"正弦\") plt.legend(loc='upper left')#图例位置 plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) plt.show() matplotlib 对notebook的特殊支持 matplotlib提供了一个backend为jupyter notebook 提供了控件支持 from __future__ import print_function from imp import reload import matplotlib reload(matplotlib) matplotlib.use('nbagg') import matplotlib.backends.backend_nbagg reload(matplotlib.backends.backend_nbagg) 非交互模式 import matplotlib.backends.backend_webagg_core reload(matplotlib.backends.backend_webagg_core) import matplotlib.pyplot as plt plt.interactive(False) fig1 = plt.figure() plt.plot(range(10)) plt.show() 定义了第一张图后,后面的定义就可以不再使用plt.figure() plt.plot([3, 2, 1]) plt.show() 我们可以用connection_info()查看每张图片的ui状态 print(matplotlib.backends.backend_nbagg.connection_info()) Figure 1 - Figure 1 Figure 2 - Figure 2 Figures pending show: 0 也可以关闭一副图的ui plt.close(fig1) 在非交互模式下没有plt.show就不会显示 plt.plot(range(10)) [] 显示以前创建的图 plt.show() plt.figure() plt.plot(range(5)) plt.show() 交互模式 使用plt.interactive(True)开启交互模式,交互模式下不需要show就可以显示图片 plt.interactive(True) plt.figure() plt.plot([3, 2, 1]) [] 后续行应添加到现有图形，而不是创建一个新的图形。 plt.plot(range(3)) [] 在交互模式下调用connection_info不应显示任何未决数字 print(matplotlib.backends.backend_nbagg.connection_info()) Figure 2 - Figure 2 Figure 3 - Figure 3 Figure 4 - Figure 4 Figure 5 - Figure 5 这种模式用来调试不错,并不适合用来做图 plt.interactive(False) 多个显示 plt.gcf().canvas.manager.reshow() 动画 import matplotlib.animation as animation import numpy as np fig, ax = plt.subplots() x = np.arange(0, 2*np.pi, 0.01) # x-array line, = ax.plot(x, np.sin(x)) def animate(i): line.set_ydata(np.sin(x+i/10.0)) # update the data return line, #Init only required for blitting to give a clean slate. def init(): line.set_ydata(np.ma.array(x, mask=True)) return line, ani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init, interval=32., blit=True) plt.show() 绑定事件动作 按任何键盘键或鼠标按钮（或滚动）应该在图形有焦点时循环线条线。该图在创建时应默认具有焦点，并通过单击画布重新获得。单击图形外的任何位置都应该释放焦点，但将鼠标移出图形不应该释放焦点。 import itertools fig, ax = plt.subplots() x = np.linspace(0,10,10000) y = np.sin(x) ln, = ax.plot(x,y) evt = [] colors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c'])) def on_event(event): if event.name.startswith('key'): fig.suptitle('%s: %s' % (event.name, event.key)) elif event.name == 'scroll_event': fig.suptitle('%s: %s' % (event.name, event.step)) else: fig.suptitle('%s: %s' % (event.name, event.button)) evt.append(event) ln.set_color(next(colors)) fig.canvas.draw() fig.canvas.draw_idle() fig.canvas.mpl_connect('button_press_event', on_event) fig.canvas.mpl_connect('button_release_event', on_event) fig.canvas.mpl_connect('scroll_event', on_event) fig.canvas.mpl_connect('key_press_event', on_event) fig.canvas.mpl_connect('key_release_event', on_event) plt.show() 计时器 import time fig, ax = plt.subplots() text = ax.text(0.5, 0.5, '', ha='center') def update(text): text.set(text=time.ctime()) text.axes.figure.canvas.draw() timer = fig.canvas.new_timer(500, [(update, [text], {})]) timer.start() plt.show() Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/绘图工具pyplot/绘图工具pyplot.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/绘图工具pyplot/绘图工具pyplot.html","title":"绘图工具pyplot","keywords":"","body":"绘图工具pyplot matplotlib.pylot是matplotlib的绘图工具 我们将会由一个绘制sin(x)曲线的例子开始,由简单到复杂的学习这个库 最简单的实现 最基本的函数就是plt.plot()了,它会产生一个图形, import matplotlib.pyplot as plt import pylab %matplotlib inline import numpy as np plt.style.use('chinese_support') X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plt.plot(X,C);plt.plot(X,S) [] 修改一些设置 plot接收参数,可以使用color指定线的颜色,用linewidth指定线条粗细,linestyle指定线条形状 plt.figure(figsize=(10,6), dpi=80)#设置图片大小和dpi plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\");plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"--\") [] 边界扩大 我们可以i为plt对象绑定xlim和ylim来指定坐标轴的范围 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\") plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.show() 设置y轴记号标签 xtick和ytick则是可以接收一个序列来确定刻度 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\") plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 plt.show() 移动脊柱 实际上每幅图有四条脊柱（上下左右），为了将脊柱放在图的中间，我们必须将其中的两条（上和右）设置为无色，然后调整剩下的两条到合适的位置——数据空间的 0 点。 脊柱使用对象gca来操作 它有 .spines选择'right','top','bottom','left'来确定要操作的是哪条脊柱 .set_color设置颜色 .set_position设定位置 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\") plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) plt.show() 图例 plt.legend(loc=)可以用来初始化图例位置 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\",label=\"cosine\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\",label=\"sine\") plt.legend(loc='upper left')#图例位置 plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) plt.show() 给一些特殊点做注释 我们希望在 $2\\pi/3$ 的位置给两条函数曲线加上一个注释。首先，我们在对应的函数图像位置上画一个点；然后，向横轴引一条垂线，以虚线标记；最后，写上标签。 plt.text可以在图上指定位置配上文字 plt.annotate可以用来画出图片上的说明文字 plt.plot用来画直线 plt.scatter 可以用来画交点 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.text(0.25, 0.75, r'$cos(x)$') plt.text(1.25, 0.75, r'$sin(x)$') plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\",label=\"cosine\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\",label=\"sine\") plt.legend(loc='upper left')#图例位置 plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) t = 2*np.pi/3 #特殊点x轴位置 plt.plot([t,t],[0,np.cos(t)], color ='blue', linewidth=2.5, linestyle=\"--\")#竖线从0到与cos(t)交点,蓝色虚线 plt.scatter([t,],[np.cos(t),], 50, color ='blue')# 画交点 plt.annotate(r'$\\sin(\\frac{2\\pi}{3})=\\frac{\\sqrt{3}}{2}$', xy=(t, np.sin(t)), xycoords='data', xytext=(+10, +30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))#指出交点并说明公式 plt.plot([t,t],[0,np.sin(t)], color ='red', linewidth=2.5, linestyle=\"--\")#竖线从0到与sin(t)交点,红色虚线 plt.scatter([t,],[np.sin(t),], 50, color ='red')# 画交点 plt.annotate(r'$\\cos(\\frac{2\\pi}{3})=-\\frac{1}{2}$', xy=(t, np.cos(t)), xycoords='data', xytext=(-90, -50), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))#指出交点并说明公式 plt.show() 精益求精 坐标轴上的记号标签被曲线挡住了，作为强迫症患者这是不能忍的。我们可以把它们放大，然后添加一个白色的半透明底色。这样可以保证标签和曲线同时可见。 并且我们给图片加上格子 plt.figure(figsize=(8,5), dpi=80)#设置图片大小和dpi plt.subplot(111) plt.plot(X, C, color=\"blue\", linewidth=2.5, linestyle=\"-\",label=\"cosine\") plt.plot(X, S, color=\"red\", linewidth=2.5, linestyle=\"-\",label=\"sine\") plt.legend(loc='upper left')#图例位置 plt.xlim(X.min()*1.1, X.max()*1.1)#边界扩大1.1倍 plt.ylim(C.min()*1.1,C.max()*1.1)#边界扩大1.1倍 plt.xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r'$-\\pi$',r'$-\\pi / 2$',r'$0$',r'$\\pi / 2$',r'$\\pi$'])#设置x轴记号标签,用latex符号替代具体数 plt.yticks([-1, 0, +1])#设置y轴记号标签 ax = plt.gca()#脊柱 ax.spines['right'].set_color('none')#右脊柱设为无色 ax.spines['top'].set_color('none')#上脊柱设为无色 ax.xaxis.set_ticks_position('bottom')#下脊柱设定位置 ax.spines['bottom'].set_position(('data',0)) ax.yaxis.set_ticks_position('left')#左脊柱设定位置 ax.spines['left'].set_position(('data',0)) # 添加一个白色的半透明底色 for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(16) label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.65 )) t = 2*np.pi/3 #特殊点x轴位置 plt.plot([t,t],[0,np.cos(t)], color ='blue', linewidth=2.5, linestyle=\"--\")#竖线从0到与cos(t)交点,蓝色虚线 plt.scatter([t,],[np.cos(t),], 50, color ='blue')# 画交点 plt.annotate(r'$\\sin(\\frac{2\\pi}{3})=\\frac{\\sqrt{3}}{2}$', xy=(t, np.sin(t)), xycoords='data', xytext=(+10, +30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))#指出交点并说明公式 plt.plot([t,t],[0,np.sin(t)], color ='red', linewidth=2.5, linestyle=\"--\")#竖线从0到与sin(t)交点,红色虚线 plt.scatter([t,],[np.sin(t),], 50, color ='red')# 画交点 plt.annotate(r'$\\cos(\\frac{2\\pi}{3})=-\\frac{1}{2}$', xy=(t, np.cos(t)), xycoords='data', xytext=(-90, -50), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))#指出交点并说明公式 plt.grid(True) plt.show() 填充颜色 fill_between方法来填充两个线条间的内容 n = 256 X = np.linspace(-np.pi,np.pi,n,endpoint=True) Y = np.sin(2*X) plt.axes([0.025,0.025,0.95,0.95]) plt.plot (X, Y+1, color='blue', alpha=1.00) plt.fill_between(X, 1, Y+1, color='blue', alpha=.25) plt.plot (X, Y-1, color='blue', alpha=1.00) plt.fill_between(X, -1, Y-1, (Y-1) > -1, color='blue', alpha=.25) plt.fill_between(X, -1, Y-1, (Y-1) 图像、子图、坐标轴和记号 到目前为止，我们都用隐式的方法来绘制图像和坐标轴。快速绘图中，这是很方便的。我们也可以显式地控制图像、子图、坐标轴。Matplotlib 中的「图像」指的是用户界面看到的整个窗口内容。在图像里面有所谓「子图」。子图的位置是由坐标网格确定的，而「坐标轴」却不受此限制，可以放在图像的任意位置。我们已经隐式地使用过图像和子图：当我们调用 plot 函数的时候，matplotlib 调用 gca() 函数以及 gcf() 函数来获取当前的坐标轴和图像；如果无法获取图像，则会调用 figure() 函数来创建一个——严格地说，是用 subplot(1,1,1) 创建一个只有一个子图的图像。 子图像 你可以用子图来将图样（plot）放在均匀的坐标网格中。用 subplot 函数的时候，你需要指明网格的行列数量，以及你希望将图样放在哪一个网格区域中。此外，gridspec 的功能更强大，你也可以选择它来实现这个功能。 plt.subplot(2,2,1) plt.xticks([]), plt.yticks([]) plt.text(0.5,0.5, 'subplot(2,2,1)',ha='center',va='center',size=20,alpha=.5) plt.subplot(2,2,2) plt.xticks([]), plt.yticks([]) plt.text(0.5,0.5, 'subplot(2,2,2)',ha='center',va='center',size=20,alpha=.5) plt.subplot(2,2,3) plt.xticks([]),plt.yticks([]) plt.text(0.5,0.5, 'subplot(2,2,3)',ha='center',va='center',size=20,alpha=.5) plt.subplot(2,2,4) plt.xticks([]), plt.yticks([]) plt.text(0.5,0.5, 'subplot(2,2,4)',ha='center',va='center',size=20,alpha=.5) # savefig('../figures/subplot-grid.png', dpi=64) plt.show() 格子grid ax = plt.axes([0.025,0.025,0.95,0.95]) ax.set_xlim(0,4) ax.set_ylim(0,3) ax.xaxis.set_major_locator(plt.MultipleLocator(1.0)) ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1.0)) ax.yaxis.set_minor_locator(plt.MultipleLocator(0.1)) ax.grid(which='major', axis='x', linewidth=0.75, linestyle='-', color='0.75') ax.grid(which='minor', axis='x', linewidth=0.25, linestyle='-', color='0.75') ax.grid(which='major', axis='y', linewidth=0.75, linestyle='-', color='0.75') ax.grid(which='minor', axis='y', linewidth=0.25, linestyle='-', color='0.75') ax.set_xticklabels([]) ax.set_yticklabels([]) plt.show() 多重网格 plt.subplot(2,2,1) plt.subplot(2,2,3) plt.subplot(2,2,4) plt.show() fig = plt.figure() fig.subplots_adjust(bottom=0.025, left=0.025, top = 0.975, right=0.975) plt.subplot(2,1,1) plt.xticks([]), plt.yticks([]) plt.subplot(2,3,4) plt.xticks([]), plt.yticks([]) plt.subplot(2,3,5) plt.xticks([]), plt.yticks([]) plt.subplot(2,3,6) plt.xticks([]), plt.yticks([]) plt.show() 坐标轴 坐标轴和子图功能类似，不过它可以放在图像的任意位置。因此，如果你希望在一副图中绘制一个小图，就可以用这个功能。 plt.axes([0.1,0.1,.8,.8]) plt.xticks([]), plt.yticks([]) plt.text(0.6,0.6, 'axes([0.1,0.1,.8,.8])',ha='center',va='center',size=20,alpha=.5) plt.axes([0.2,0.2,.3,.3]) plt.xticks([]), plt.yticks([]) plt.text(0.5,0.5, 'axes([0.2,0.2,.3,.3])',ha='center',va='center',size=16,alpha=.5) #plt.savefig(\"../figures/axes.png\",dpi=64) plt.show() plt.axes([0.1,0.1,.5,.5]) plt.xticks([]), plt.yticks([]) plt.text(0.1,0.1, 'axes([0.1,0.1,.8,.8])',ha='left',va='center',size=16,alpha=.5) plt.axes([0.2,0.2,.5,.5]) plt.xticks([]), plt.yticks([]) plt.text(0.1,0.1, 'axes([0.2,0.2,.5,.5])',ha='left',va='center',size=16,alpha=.5) plt.axes([0.3,0.3,.5,.5]) plt.xticks([]), plt.yticks([]) plt.text(0.1,0.1, 'axes([0.3,0.3,.5,.5])',ha='left',va='center',size=16,alpha=.5) plt.axes([0.4,0.4,.5,.5]) plt.xticks([]), plt.yticks([]) plt.text(0.1,0.1, 'axes([0.4,0.4,.5,.5])',ha='left',va='center',size=16,alpha=.5) # plt.savefig(\"../figures/axes-2.png\",dpi=64) plt.show() 记号 良好的记号是图像的重要组成部分。Matplotlib 里的记号系统里的各个细节都是可以由用户个性化配置的。你可以用 Tick Locators 来指定在那些位置放置记号，用 Tick Formatters 来调整记号的样式。主要和次要的记号可以以不同的方式呈现。默认情况下，每一个次要的记号都是隐藏的，也就是说，默认情况下的次要记号列表是空的——NullLocator。 下面有为不同需求设计的一些 Locators: NullLocatorNo ticks. IndexLocatorPlace a tick on every multiple of some base number of points plotted. FixedLocatorTick locations are fixed. LinearLocatorDetermine the tick locations. MultipleLocatorSet a tick on every integer that is multiple of some base. AutoLocatorSelect no more than n intervals at nice locations. LogLocatorDetermine the tick locations for log axes. 特殊图形 除了点线等基本工具,还可以直接使用设置好的类型画一些基本图形 散点图scatter n = 1024 X = np.random.normal(0,1,n) Y = np.random.normal(0,1,n) plt.scatter(X,Y) plt.show() n = 1024 X = np.random.normal(0,1,n) Y = np.random.normal(0,1,n) T = np.arctan2(Y,X) # 计算出象限 plt.axes([0.025,0.025,0.95,0.95]) plt.scatter(X,Y, s=75, c=T, alpha=.5) plt.xlim(-1.5,1.5), plt.xticks([]) plt.ylim(-1.5,1.5), plt.yticks([]) plt.show() 栅栏图bar n = 12 X = np.arange(n) Y1 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) Y2 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) plt.bar(X, +Y1, facecolor='#9999ff', edgecolor='white') plt.bar(X, -Y2, facecolor='#ff9999', edgecolor='white') for x,y in zip(X,Y1): plt.text(x+0.4, y+0.05, '%.2f' % y, ha='center', va= 'bottom') plt.ylim(-1.25,+1.25) plt.show() n = 12 X = np.arange(n) Y1 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) Y2 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) plt.axes([0.025,0.025,0.95,0.95]) plt.bar(X, +Y1, facecolor='#9999ff', edgecolor='white') plt.bar(X, -Y2, facecolor='#ff9999', edgecolor='white') for x,y in zip(X,Y1): plt.text(x+0.4, y+0.05, '%.2f' % y, ha='center', va= 'bottom') for x,y in zip(X,Y2): plt.text(x+0.4, -y-0.05, '%.2f' % y, ha='center', va= 'top') plt.xlim(-.5,n), plt.xticks([]) plt.ylim(-1.25,+1.25), plt.yticks([]) plt.show() 等高线图meshgrid def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 256 x = np.linspace(-3,3,n) y = np.linspace(-3,3,n) X,Y = np.meshgrid(x,y) plt.contourf(X, Y, f(X,Y), 8, alpha=.75, cmap='jet') C = plt.contour(X, Y, f(X,Y), 8, colors='black', linewidth=.5) plt.show() def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 256 x = np.linspace(-3,3,n) y = np.linspace(-3,3,n) X,Y = np.meshgrid(x,y) plt.axes([0.025,0.025,0.95,0.95]) plt.contourf(X, Y, f(X,Y), 8, alpha=.75, cmap=plt.cm.hot) C = plt.contour(X, Y, f(X,Y), 8, colors='black', linewidth=.5) plt.clabel(C, inline=1, fontsize=10) plt.xticks([]), plt.yticks([]) plt.show() 灰度图（Imshow） def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 10 x = np.linspace(-3,3,4*n) y = np.linspace(-3,3,3*n) X,Y = np.meshgrid(x,y) plt.imshow(f(X,Y)) plt.show() def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 10 x = np.linspace(-3,3,3.5*n) y = np.linspace(-3,3,3.0*n) X,Y = np.meshgrid(x,y) Z = f(X,Y) plt.axes([0.025,0.025,0.95,0.95]) plt.imshow(Z,interpolation='nearest', cmap='bone', origin='lower') plt.colorbar(shrink=.92) plt.xticks([]), plt.yticks([]) plt.show() 饼状图 n = 20 Z = np.random.uniform(0,1,n) plt.pie(Z) plt.show() n = 20 Z = np.ones(n) Z[-1] *= 2 plt.axes([0.025,0.025,0.95,0.95]) plt.pie(Z, explode=Z*.05, colors = ['%f' % (i/float(n)) for i in range(n)]) plt.gca().set_aspect('equal') plt.xticks([]), plt.yticks([]) plt.show() 柱状图hist mu, sigma = 100, 15 x = mu + sigma * np.random.randn(10000) plt.hist(x, 50, normed=1, facecolor='g', alpha=0.75) plt.xlabel('Smarts') plt.ylabel('Probability') plt.title('Histogram of IQ') plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$') plt.axis([40, 160, 0, 0.03]) plt.grid(True) 量场图--箭头（Quiver Plots） n = 8 X,Y = np.mgrid[0:n,0:n] plt.quiver(X,Y) plt.show() n = 8 X,Y = np.mgrid[0:n,0:n] T = np.arctan2(Y-n/2.0, X-n/2.0) R = 10+np.sqrt((Y-n/2.0)**2+(X-n/2.0)**2) U,V = R*np.cos(T), R*np.sin(T) plt.axes([0.025,0.025,0.95,0.95]) plt.quiver(X,Y,U,V,R, alpha=.5) plt.quiver(X,Y,U,V, edgecolor='k', facecolor='None', linewidth=.5) plt.xlim(-1,n), plt.xticks([]) plt.ylim(-1,n), plt.yticks([]) plt.show() 极轴图 plt.axes([0,0,1,1]) N = 20 theta = np.arange(0.0, 2*np.pi, 2*np.pi/N) radii = 10*np.random.rand(N) width = np.pi/4*np.random.rand(N) bars = plt.bar(theta, radii, width=width, bottom=0.0) for r,bar in zip(radii, bars): bar.set_facecolor( plt.cm.jet(r/10.)) bar.set_alpha(0.5) plt.show() ax = plt.axes([0.025,0.025,0.95,0.95], polar=True) N = 20 theta = np.arange(0.0, 2*np.pi, 2*np.pi/N) radii = 10*np.random.rand(N) width = np.pi/4*np.random.rand(N) bars = plt.bar(theta, radii, width=width, bottom=0.0) for r,bar in zip(radii, bars): bar.set_facecolor( plt.cm.jet(r/10.)) bar.set_alpha(0.5) ax.set_xticklabels([]) ax.set_yticklabels([]) plt.show() 3D 图 from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = Axes3D(fig) X = np.arange(-4, 4, 0.25) Y = np.arange(-4, 4, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='hot') plt.show() fig = plt.figure() ax = Axes3D(fig) X = np.arange(-4, 4, 0.25) Y = np.arange(-4, 4, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.hot) ax.contourf(X, Y, Z, zdir='z', offset=-2, cmap=plt.cm.hot) ax.set_zlim(-2,2) plt.show() 手稿 eqs = [] eqs.append((r\"$W^{3\\beta}_{\\delta_1 \\rho_1 \\sigma_2} = U^{3\\beta}_{\\delta_1 \\rho_1} + \\frac{1}{8 \\pi 2} \\int^{\\alpha_2}_{\\alpha_2} d \\alpha^\\prime_2 \\left[\\frac{ U^{2\\beta}_{\\delta_1 \\rho_1} - \\alpha^\\prime_2U^{1\\beta}_{\\rho_1 \\sigma_2} }{U^{0\\beta}_{\\rho_1 \\sigma_2}}\\right]$\")) eqs.append((r\"$\\frac{d\\rho}{d t} + \\rho \\vec{v}\\cdot\\nabla\\vec{v} = -\\nabla p + \\mu\\nabla^2 \\vec{v} + \\rho \\vec{g}$\")) eqs.append((r\"$\\int_{-\\infty}^\\infty e^{-x^2}dx=\\sqrt{\\pi}$\")) eqs.append((r\"$E = mc^2$\")) eqs.append((r\"$F_G = G\\frac{m_1m_2}{r^2}$\")) plt.axes([0.025,0.025,0.95,0.95]) for i in range(24): index = np.random.randint(0,len(eqs)) eq = eqs[index] size = np.random.uniform(12,32) x,y = np.random.uniform(0,1,2) alpha = np.random.uniform(0.25,.75) plt.text(x, y, eq, ha='center', va='center', color=\"#11557c\", alpha=alpha, transform=plt.gca().transAxes, fontsize=size, clip_on=True) plt.xticks([]), plt.yticks([]) # savefig('../figures/text_ex.png',dpi=48) plt.show() 箱形图 箱形图可以用来集中化的体现数据的特点 np.random.seed(937) data = np.random.lognormal(size=(37, 4), mean=1.5, sigma=1.75) labels = list('ABCD') fs = 10 # fontsize # demonstrate how to toggle the display of different elements: fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6, 6)) axes[0, 0].boxplot(data, labels=labels) axes[0, 0].set_title('Default', fontsize=fs) axes[0, 1].boxplot(data, labels=labels, showmeans=True) axes[0, 1].set_title('showmeans=True', fontsize=fs) axes[0, 2].boxplot(data, labels=labels, showmeans=True, meanline=True) axes[0, 2].set_title('showmeans=True,\\nmeanline=True', fontsize=fs) axes[1, 0].boxplot(data, labels=labels, showbox=False, showcaps=False) axes[1, 0].set_title('Tufte Style \\n(showbox=False,\\nshowcaps=False)', fontsize=fs) axes[1, 1].boxplot(data, labels=labels, notch=True, bootstrap=10000) axes[1, 1].set_title('notch=True,\\nbootstrap=10000', fontsize=fs) axes[1, 2].boxplot(data, labels=labels, showfliers=False) axes[1, 2].set_title('showfliers=False', fontsize=fs) for ax in axes.flatten(): ax.set_yscale('log') ax.set_yticklabels([]) fig.subplots_adjust(hspace=0.4) plt.show() # demonstrate how to customize the display different elements: boxprops = dict(linestyle='--', linewidth=3, color='darkgoldenrod') flierprops = dict(marker='o', markerfacecolor='green', markersize=12, linestyle='none') medianprops = dict(linestyle='-.', linewidth=2.5, color='firebrick') meanpointprops = dict(marker='D', markeredgecolor='black', markerfacecolor='firebrick') meanlineprops = dict(linestyle='--', linewidth=2.5, color='purple') fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6, 6)) axes[0, 0].boxplot(data, boxprops=boxprops) axes[0, 0].set_title('Custom boxprops', fontsize=fs) axes[0, 1].boxplot(data, flierprops=flierprops, medianprops=medianprops) axes[0, 1].set_title('Custom medianprops\\nand flierprops', fontsize=fs) axes[0, 2].boxplot(data, whis='range') axes[0, 2].set_title('whis=\"range\"', fontsize=fs) axes[1, 0].boxplot(data, meanprops=meanpointprops, meanline=False, showmeans=True) axes[1, 0].set_title('Custom mean\\nas point', fontsize=fs) axes[1, 1].boxplot(data, meanprops=meanlineprops, meanline=True, showmeans=True) axes[1, 1].set_title('Custom mean\\nas line', fontsize=fs) axes[1, 2].boxplot(data, whis=[15, 85]) axes[1, 2].set_title('whis=[15, 85]\\n#percentiles', fontsize=fs) for ax in axes.flatten(): ax.set_yscale('log') ax.set_yticklabels([]) fig.suptitle(\"I never said they'd be pretty\") fig.subplots_adjust(hspace=0.4) plt.show() 更多的图形可以在http://matplotlib.org/api/pyplot_summary.html中查看 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/非结构网络/非结构网络.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/非结构网络/非结构网络.html","title":"非结构网络","keywords":"","body":"非结构网络 非结构网格是没有规则拓扑关系的网格，它通常由多边形三角形组成。 网格中的每个每个元素都可以是二维的多边形或者三维多面体，其中最常见的是二维的三角形以及三维的四面体。 在每个元素之间没有隐含的连通性。 由于结构网格面对复杂几何外形时生成困难，以及耗费大量人工，自动化程度不高等缺点，非结构网格逐渐发展起来.使用它的技术主要有流体分析,空气动力学,有限元分析等细分领域. matplotlib中有针对它的作图工具matplotlib.tri模块 核心类class matplotlib.tri.Triangulation(x, y, triangles=None, mask=None) 由n个point点和n个三角形组成的非结构化三角形网格。三角形可以由用户指定或使用Delaunay三角测量自动生成。 x,y对应网格点的坐标 triangles 对于每个三角形，组成三角形的三个点的索引以逆时针方式排序。如果未指定，则计算Delaunay三角剖分。 mask 指定哪些三角形的屏蔽数组 它的方法有: calculate_plane_coefficients(z) 从点（x，y）坐标和指定的z形阵列（n points）计算所有未屏蔽三角形的平面方程系数。返回的数组具有形状（n points，3）并且允许使用 z = array [tri，0] x array [tri，1] y array [tri，2] edges 返回包含非屏蔽三角形的所有边的整型数组形状（nedges，2）。每个边是起点索引和终点索引。每个边（开始，结束和结束，开始）只出现一次。 get_masked_triangles() 返回未屏蔽的三角形数组。 get_trifinder() 返回此三角剖分的默认matplotlib.tri.TriFinder,如果需要,创建它.这允许轻松共享相同的TriFinder对象。 neighbors 邻点 set_mask(mask) 设置或清除屏蔽数组. 寻找是三角形算法 matplotlib.tri.TriFinder(triangulation) TriFinder类使用来自M.de Berg，M.van Kreveld，M.Overmars和O. Schwarzkopf的书“Computational Geometry，Algorithms and Applications”，第二版中的梯形映射算法来实现。三角测量必须有效，即它不能具有重复的点，由共线点形成的三角形或重叠的三角形。该算法对于由共线点形成的三角形具有一些公差，但是这不应被依赖。 这个类的实例可以被调用 trifinder(x,y),调用后会 返回包含指定x，y点所在的三角形的索引的数组，或者对于不在三角形内的点返回-1。 x，y是相同形状和任意数量维度的类阵列x和y坐标。 返回具有相同形状和x和y的整数数组。 三角形网格线性插值 matplotlib.tri.LinearTriInterpolator(triangulation, z, trifinder=None) LinearTriInterpolator对三角形网格执行线性插值。每个三角形由平面表示，使得点（x，y）处的内插值位于包含（x，y）的三角形的平面上。因此，内插值在三角形上是连续的，但是它们的一阶导数在三角形之间的边缘处是不连续的。 他的实例有方法gradient(x, y),可以返回在指定的x，y点包含插值导数的2个包含屏蔽数组的列表。 而实例被调用会返回在指定的x，y点包含插值的屏蔽数组。 三角形网格执行三次插值 matplotlib.tri.CubicTriInterpolator(triangulation, z, kind='min_E', trifinder=None, dz=None) CubicTriInterpolator对三角形网格执行三次插值。在一维, 一段上 做三次插值,函数由函数的值和其两端的导数定义。这在三角形内的2-d中几乎相同，除了函数的值及其2导数必须在每个三角形节点处定义。CubicTriInterpolator获取每个节点（由用户提供）的函数值，并在内部计算导数的值，从而实现平滑插值。 （作为一个特殊功能，用户还可以在每个节点强加导数的值，但这不应该是常见的用法。） kind 选择平滑算法，以便计算内插导数（默认为“min_E”）：如果'min_E'：（默认）计算每个节点处的导数以最小化弯曲能量。如果'geom'：每个节点的导数被计算为相关三角形法线的加权平均值。用于速度优化（大网格）。如果'user'：用户提供参数dz，因此不需要计算。 trifinder 如果未指定，Triangulation的默认TriFinder将通过调用matplotlib.tri.Triangulation.get_trifinder（）来使用。 dz 仅在kind ='user'时使用。在这种情况下，dz必须提供为（dzdx，dzdy），其中dzdx，dzdy是与z相同形状的数组，并且是三角点处的内插一阶导数。 内插基于三角网格的Clough-Tocher细分方案（为了使其更清楚，网格的每个三角形将被划分为3个子三角形，并且在每个子三角形上，内插函数是2的三次多项式坐标）。这种技术源自FEM（有限元方法）分析;使用的元件是还原的Hsieh-Clough-Tocher（HCT）元件。其形状函数在[R1]中描述。组合函数保证是C1平滑的，即它是连续的，并且其一阶导数也是连续的（这在三角形内容中是容易显示的，但当穿过边缘时也是如此）。 在默认情况下（种类='min_E'），内插器使由HCT元素形状函数生成的函数空间上的曲率能量最小化 - 利用施加的值，但在每个节点处的任意导数。最小化的函数是所谓的总曲率的积分（基于来自[R2] -PCG稀疏求解器的算法的实现）： $ E(z) = {\\frac 1 2 } \\int_\\Omega ((\\frac {\\partial^2 z} {\\partial x^2} )^2 + (\\frac {\\partial^2 z} {\\partial y^2} )^2 +2(\\frac {\\partial^2 z} {\\partial y \\partial x} )^2)dxdy $ 如果用户选择case type ='geom'，则使用简单的几何近似（三角形法线向量的加权平均），这可以在非常大的网格上提高速度。 例子: from matplotlib.tri import Triangulation, UniformTriRefiner,\\ CubicTriInterpolator import matplotlib.pyplot as plt import matplotlib.cm as cm import numpy as np import math # 计算偶极子的电位 def dipole_potential(x, y): \"\"\" The electric dipole potential V \"\"\" r_sq = x**2 + y**2 theta = np.arctan2(y, x) z = np.cos(theta)/r_sq return (np.max(z) - z) / (np.max(z) - np.min(z)) # 创建三角网格 #----------------------------------------------------------------------------- # 首先创建点的x和y坐标 n_angles = 30 n_radii = 10 min_radius = 0.2 radii = np.linspace(min_radius, 0.95, n_radii) angles = np.linspace(0, 2*math.pi, n_angles, endpoint=False) angles = np.repeat(angles[..., np.newaxis], n_radii, axis=1) angles[:, 1::2] += math.pi/n_angles x = (radii*np.cos(angles)).flatten() y = (radii*np.sin(angles)).flatten() V = dipole_potential(x, y) V[:5] array([ 0. , 0.25222984, 0.35123967, 0.40177562, 0.4296875 ]) triang = Triangulation(x, y) # 屏蔽掉不需要的值 xmid = x[triang.triangles].mean(axis=1) ymid = y[triang.triangles].mean(axis=1) mask = np.where(xmid*xmid + ymid*ymid # 精细化数据 - 内插电位V refiner = UniformTriRefiner(triang) tri_refi, z_test_refi = refiner.refine_field(V, subdiv=3) # 计算电场（Ex，Ey）作为电位梯度 tci = CubicTriInterpolator(triang, -V) # 这里,gradient()需要 网格节点，但可以在其他任何地方 (Ex, Ey) = tci.gradient(triang.x, triang.y) E_norm = np.sqrt(Ex**2 + Ey**2) #作图 plt.figure() plt.gca().set_aspect('equal') plt.triplot(triang, color='0.8') levels = np.arange(0., 1., 0.01) cmap = cm.get_cmap(name='hot', lut=None) # 三角等高线 plt.tricontour(tri_refi, z_test_refi, levels=levels, cmap=cmap, linewidths=[2.0, 1.0, 1.0, 1.0]) # 用quiver绘制电矢量场的方向 plt.quiver(triang.x, triang.y, Ex/E_norm, Ey/E_norm, units='xy', scale=10., zorder=3, color='blue', width=0.007, headwidth=3., headlength=4.) plt.title('Gradient plot: an electrical dipole') plt.show() 通过递归细分的均匀网格细化 matplotlib.tri.UniformTriRefiner(triangulation)类 通过递归细分的均匀网格细化。 它有方法 refine_field(z, triinterpolator=None, subdiv=3) 用来优化在封装三角定义上定义的字段 triinterpolator插值器用于场插值。如果未指定，将使用CubicTriInterpolator。 subdiv细分的递归级别。默认为3.每个三角形将被划分为4个**子细分三角形。 refine_triangulation(return_tri_index=False, subdiv=3) 计算封装三角测量的均匀精细三角测量refi_triangulation。此函数通过将每个父三角形递归地（递归细分的水平）分割成在边中间节点上构建的4个子子三角形，来细化封装的三角形。最后，每个三角形因此被划分为4 **个子三角形。 subdiv的默认值为3，从而为初始三角形的每个三角形产生64个精细子三角形。 return_tri_index布尔值，指示是否将返回指示每个点的父三角形索引的索引表。默认值False。 例子:在粗糙的三角形网格（例如，由相对稀疏的测试数据构建的三角测量）上绘制高质量等高线： # 在用户定义的三角网格上演示高分辨率三轴定位用matplotlib.tri.UniformTriRefiner import matplotlib.tri as tri # 要分析测试的function def function_z(x, y): \"\"\" A function of 2 variables \"\"\" r1 = np.sqrt((0.5 - x)**2 + (0.5 - y)**2) theta1 = np.arctan2(0.5 - x, 0.5 - y) r2 = np.sqrt((-x - 0.2)**2 + (-y - 0.2)**2) theta2 = np.arctan2(-x - 0.2, -y - 0.2) z = -(2*(np.exp((r1/10)**2) - 1)*30. * np.cos(7.*theta1) + (np.exp((r2/10)**2) - 1)*30. * np.cos(11.*theta2) + 0.7*(x**2 + y**2)) return (np.max(z) - z)/(np.max(z) - np.min(z)) # 构建三角网络中的点 n_angles = 20 n_radii = 10 min_radius = 0.15 radii = np.linspace(min_radius, 0.95, n_radii) angles = np.linspace(0, 2*math.pi, n_angles, endpoint=False) angles = np.repeat(angles[..., np.newaxis], n_radii, axis=1) angles[:, 1::2] += math.pi/n_angles x = (radii*np.cos(angles)).flatten() y = (radii*np.sin(angles)).flatten() z = function_z(x, y) #开始构建三角网络 triang = tri.Triangulation(x, y) # 屏蔽不要的店 xmid = x[triang.triangles].mean(axis=1) ymid = y[triang.triangles].mean(axis=1) mask = np.where(xmid*xmid + ymid*ymid # 精细化数据 refiner = tri.UniformTriRefiner(triang) tri_refi, z_test_refi = refiner.refine_field(z, subdiv=3) plt.figure() plt.gca().set_aspect('equal') plt.triplot(triang, lw=0.5, color='white') levels = np.arange(0., 1., 0.025) cmap = cm.get_cmap(name='terrain', lut=None) plt.tricontourf(tri_refi, z_test_refi, levels=levels, cmap=cmap) plt.tricontour(tri_refi, z_test_refi, levels=levels, colors=['0.25', '0.5', '0.5', '0.5', '0.5'], linewidths=[1.0, 0.5, 0.5, 0.5, 0.5]) plt.title(\"High-resolution tricontouring\") plt.show() 三角网格分析和改进的基本工具 matplotlib.tri.TriAnalyzer(triangulation) 定义三角网格分析和改进的基本工具。TriAnalizer封装了一个Triangulation对象，并提供了用于网格分析和网格改进的基本工具。 它有三个方法 circle_ratios(rescale=True) 返回三角形的三角形平坦度的度量。 圆周半径与外接圆半径的比率是广泛使用的三角形平坦度的指标。对于等边三角形，它总是 get_flat_tri_mask(min_circle_ratio=0.01, rescale=True) 消除三角测量中过分平坦的边界三角形。返回一个屏蔽数组new_mask(布尔值)，它允许从边界定位的平面三角形（根据他们的circle_ratios（））清除封装的三角剖分。这个屏蔽数组意味着随后应用于使用matplotlib.tri.Triangulation.set_mask（）的三角测量。 new_mask是初始三角形掩模的扩展，在初始掩模的三角形将保持掩蔽的意义上。new_mask数组是递归计算的;在每个步骤，只有当它们与当前网格边界共享一侧时，才移除平面三角形。因此，在三角域中将不产生新的空穴。 + min_circle_ratio 如果内圆/外圆半径比r/R 这个函数的基本原理是Delaunay三角形(一个非结构化的点集合,有时在边界处包含几乎平坦的三角形)，导致绘图中的伪像（特别是对于高分辨率轮廓化）。用计算的new_mask掩蔽，封装的三角剖分将不包含具有低于min_circle_ratio的圆比率的更多未掩蔽的边界三角形，从而改进后续绘图或插值的网格质量。 例子:随机集合的高分辨率定向 本演示的初始数据点和三角网格为： 在[-1,1]×[-1,1]正方形内部实例化一组随机点 然后计算这些点的Delaunay三角剖分，其中a 随机子集的三角形被用户掩盖（基于 init_mask_frac 参数）。 这将模拟无效的数据。 提出的通用程序获得高分辨率轮廓的这种 数据集如下： 使用matplotlib.tri.TriAnalyzer计算扩展屏蔽,从边框中排除形状不好（平）的三角形三角测量。 将屏蔽应用于三角剖分（使用set_mask）。 使用a来细化和内插数据matplotlib.tri.UniformTriRefiner。 用tricontour绘制精制数据。 from matplotlib.tri import Triangulation, TriAnalyzer, UniformTriRefiner #----------------------------------------------------------------------------- # 用于测试的函数 #----------------------------------------------------------------------------- def experiment_res(x, y): \"\"\" 表实验结果的分析函数\"\"\" x = 2.*x r1 = np.sqrt((0.5 - x)**2 + (0.5 - y)**2) theta1 = np.arctan2(0.5 - x, 0.5 - y) r2 = np.sqrt((-x - 0.2)**2 + (-y - 0.2)**2) theta2 = np.arctan2(-x - 0.2, -y - 0.2) z = (4*(np.exp((r1/10)**2) - 1)*30. * np.cos(3*theta1) + (np.exp((r2/10)**2) - 1)*30. * np.cos(5*theta2) + 2*(x**2 + y**2)) return (np.max(z) - z)/(np.max(z) - np.min(z)) #----------------------------------------------------------------------------- # 生成初始数据测试点和演示的三角测量 #----------------------------------------------------------------------------- n_test = 200 # 测试数据点数，对于subdiv = 3从3到5000进行测试 subdiv = 3 # 平滑图的初始网格的递归细分数。 #值> 3可能导致精细网格的三角形数量非常多：new triangles numbering =（4 ** subdiv）* ntri init_mask_frac = 0.0 min_circle_ratio = .01 # 随机点 random_gen = np.random.mtrand.RandomState(seed=127260) x_test = random_gen.uniform(-1., 1., size=n_test) y_test = random_gen.uniform(-1., 1., size=n_test) z_test = experiment_res(x_test, y_test) # 使用Delaunay三角网格划分 tri = Triangulation(x_test, y_test) ntri = tri.triangles.shape[0] # 剔除一些要屏蔽的点 mask_init = np.zeros(ntri, dtype=np.bool) masked_tri = random_gen.randint(0, ntri, int(ntri*init_mask_frac)) mask_init[masked_tri] = True tri.set_mask(mask_init) #----------------------------------------------------------------------------- # 在高分辨率绘图之前改进三角测量：删除平面三角形 #----------------------------------------------------------------------------- # 掩蔽在三角形网格的边界处的不良形状的三角形 mask = TriAnalyzer(tri).get_flat_tri_mask(min_circle_ratio) tri.set_mask(mask) # 精细化数据 refiner = UniformTriRefiner(tri) tri_refi, z_test_refi = refiner.refine_field(z_test, subdiv=subdiv) # 用于与分析进行比较 z_expected = experiment_res(tri_refi.x, tri_refi.y) flat_tri = Triangulation(x_test, y_test) flat_tri.set_mask(~mask) #开始画图 plot_tri = True # plot of base triangulation plot_masked_tri = True # plot of excessively flat excluded triangles plot_refi_tri = False # plot of refined triangulation plot_expected = False # plot of analytical function values for comparison # Graphical options for tricontouring levels = np.arange(0., 1., 0.025) cmap = cm.get_cmap(name='Blues', lut=None) plt.figure() plt.gca().set_aspect('equal') plt.title(\"Filtering a Delaunay mesh\\n\" + \"(application to high-resolution tricontouring)\") # 1) 精确（计算）数据轮廓的图： plt.tricontour(tri_refi, z_test_refi, levels=levels, cmap=cmap, linewidths=[2.0, 0.5, 1.0, 0.5]) # 2) 预期（分析）数据轮廓（虚线）的图： if plot_expected: plt.tricontour(tri_refi, z_expected, levels=levels, cmap=cmap, linestyles='--') # 3) 进行内插的细网格的图： if plot_refi_tri: plt.triplot(tri_refi, color='0.97') # 4) 初始“粗糙”网格的图： if plot_tri: plt.triplot(tri, color='0.7') # 4) 从原生的Delaunay三角形而来的未经验证的三角形的图： if plot_masked_tri: plt.triplot(flat_tri, color='red') plt.show() scale_factors 将三角划分为以平方为单位。 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/桑基图/桑基图.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/桑基图/桑基图.html","title":"桑基图","keywords":"","body":"桑基图 桑基图（Sankey diagram），即桑基能量分流图，也叫桑基能量平衡图。一种特定类型的流程图，图中延伸的分支的宽度对应数据流量的大小, 通常应用于能源、材料成分、金融等数据的可视化分析。 因1898年Matthew Henry Phineas Riall Sankey绘制的\"蒸汽机的能源效率图\"而闻名，此后便以其名字命名为“桑基图”。 桑基图最明显的特征就是，始末端的分支宽度总和相等，即所有主支宽度的总和应与所有分出去的分支宽度的总和相等，保持能量的平衡。 import numpy as np import matplotlib.pyplot as plt from matplotlib.sankey import Sankey %matplotlib inline matplotlib.sankey.Sankey(ax=None, scale=1.0, unit='', format='%G', gap=0.25, radius=0.1, shoulder=0.03, offset=0.15, head_angle=100, margin=0.4, tolerance=1e-06, **kwargs)是matplotlib构建桑基图的工具 他有两个方法来构造图形 .add(patchlabel='', flows=None, orientations=None, labels='', trunklength=1.0, pathlengths=0.25, prior=None, connect=(0, 0), rotation=0, **kwargs) flows就是流入的百分比了负数表示为流出 labels是每个流的标签 orientations 的取值范围为[-1,0,1]有效值为1（从顶部到顶部），0（从左到右）或-1（从底部到底部）.如果orientation = 0，输入将从左边突入，输出将向右边突破。 add()返回的还是一个Sankey对象,因此链式操作一直add() .finish()构造结束 S=Sankey() S.add(flows=[0.25, 0.15, 0.60, -0.20, -0.15, -0.05, -0.50, -0.10], labels=['', '', '', 'First', 'Second', 'Third', 'Fourth', 'Fifth'], orientations=[-1, 1, 0, 1, 1, 1, 0, -1]).finish() S 我们也可以直接在构造函数里定义流 Sankey(flows=[0.25, 0.15, 0.60, -0.20, -0.15, -0.05, -0.50, -0.10], labels=['', '', '', 'First', 'Second', 'Third', 'Fourth', 'Fifth'], orientations=[-1, 1, 0, 1, 1, 1, 0, -1]).finish() plt.title(\"The default settings produce a diagram like this.\") 我们可以在finish()之后通过一些针对其中元素的操作做特殊化处理 fig = plt.figure() ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[], title=\"Flow Diagram of a Widget\") sankey = Sankey(ax=ax, scale=0.01, offset=0.2, head_angle=180, format='%.0f', unit='%')# 单位unit定义 sankey.add(flows=[25, 0, 60, -10, -20, -5, -15, -10, -40], labels=['', '', '', 'First', 'Second', 'Third', 'Fourth', 'Fifth', 'Hurray!'], orientations=[-1, 1, 0, 1, 1, 1, -1, -1, 0], pathlengths=[0.25, 0.25, 0.25, 0.25, 0.25, 0.6, 0.25, 0.25, 0.25], patchlabel=\"Widget\\nA\", alpha=0.2, lw=2.0) # Arguments to matplotlib.patches.PathPatch() diagrams = sankey.finish() diagrams[0].patch.set_facecolor('#37c959') diagrams[0].texts[-1].set_color('r') diagrams[0].text.set_fontweight('bold') 如果有两张图用来表现两个系统的关系,可以像下面这么做 fig = plt.figure() ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[], title=\"Two Systems\") flows = [0.25, 0.15, 0.60, -0.10, -0.05, -0.25, -0.15, -0.10, -0.35] sankey = Sankey(ax=ax, unit=None) sankey.add(flows=flows, label='one', orientations=[-1, 1, 0, 1, 1, 1, -1, -1, 0]) sankey.add(flows=[-0.25, 0.15, 0.1], fc='#37c959', label='two', orientations=[-1, -1, -1], prior=0, connect=(0, 0)) diagrams = sankey.finish() diagrams[-1].patch.set_hatch('/') plt.legend(loc='best') Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/图片加载/图片加载.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/图片加载/图片加载.html","title":"图片加载","keywords":"","body":"图片加载 matplotlib.image模块提供了读取图片和进行简单处理的能力,他的底层是pillow import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg img=mpimg.imread('./source/cat.jpg') 通过imread()方法读取的图片会被转换成像素矩阵(numpy的narray对象),其shape与图像分辨率有关,比如: 上图是342x220的图片,那么 img.shape (220, 342, 3) 其中的3为每个像素表现为一个RGB的三位数组 img array([[[254, 254, 254], [254, 254, 254], [254, 254, 254], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]], [[254, 254, 254], [254, 254, 254], [254, 254, 254], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]], [[254, 254, 254], [254, 254, 254], [255, 255, 255], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]], ..., [[255, 255, 255], [255, 255, 255], [255, 255, 255], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]], [[255, 255, 255], [255, 255, 255], [255, 255, 255], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]], [[255, 255, 255], [255, 255, 255], [255, 255, 255], ..., [255, 255, 255], [255, 255, 255], [255, 255, 255]]], dtype=uint8) 我们可以通过plt.imshow(img)将这个数组初始化为一个plot对象 imgplot = plt.imshow(img) plt.show() 当然了只要是相同格式的数组都可以通过这个方式初始化为一个plot对象 将假彩色方案应用于图像绘图 伪彩色可以是一个有用的工具，用于增强对比度和更容易地可视化数据。这在使用投影仪对数据进行演示时尤其有用(它们的对比度通常很差)。假彩色仅与单通道，灰度，光度图像相关。我们目前有一个RGB图像。由于R，G和B都是相似的（见上面或在你的数据中的自己），我们可以只选择一个通道的数据： lum_img_r = img[:,:,0] lum_img_r array([[254, 254, 254, ..., 255, 255, 255], [254, 254, 254, ..., 255, 255, 255], [254, 254, 255, ..., 255, 255, 255], ..., [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255]], dtype=uint8) lum_img_r.shape (220, 342) plt.imshow(lum_img_r) plt.show() lum_img_g = img[:,:,1] plt.imshow(lum_img_g) plt.show() lum_img_g = img[:,:,2] plt.imshow(lum_img_g) plt.show() 现在我们以使用R为通道的图片,使用亮度（2D，无颜色）图像，应用默认色彩映射（也称为查找表，LUT）。默认值称为jet。有很多其他的也可以选择。 plt.imshow(lum_img_r, cmap=\"hot\") plt.show() 也还可以使用set_cmap()方法更改现有绘图对象上的颜色 imgplot = plt.imshow(lum_img_r) imgplot.set_cmap('spectral') plt.show() /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The spectral and spectral_r colormap was deprecated in version 2.0. Use nipy_spectral and nipy_spectral_r instead. warnings.warn(message, mplDeprecation, stacklevel=1) 色标参考 它有助于了解颜色代表什么值。我们可以通过添加颜色条来做到这一点。 imgplot = plt.imshow(lum_img_r) plt.colorbar() plt.show() 检查特定数据范围 有时，您想要增强图像的对比度，或者在特定区域中扩大对比度，同时牺牲不会变化很大的颜色的细节，或者无关紧要。找到感兴趣区域的好工具是直方图。要创建我们的图像数据的直方图，我们使用hist（）函数。 plt.hist(lum_img_r.ravel(), bins=256,fc='k', ec='k') plt.show() 下图就显示出了各个色值的分布状态,看打出来25x为值的是大多数中的大多数 通常，图像的“有趣”部分在峰值附近，您可以通过剪切峰值上方和/或下方的区域获得额外的对比度。在我们的直方图中，看起来在高端没有太多有用的信息（图像中不是很多白色的东西）。让我们调整上限，以便我们有效地“放大”直方图的一部分。我们通过将clim参数传递给imshow来实现。你也可以通过调用图像绘图对象的set_clim（）方法来做到这一点，但是要确保你在使用IPython Notebook时在plot命令的同一个单元格中这样做 - 它不会改变以前单元格的绘图。 imgplot = plt.imshow(lum_img_r, clim=(0, 200)) plt.show() 阵列插值方案 插值根据不同的数学方案计算像素的“应该”的颜色或值。这种情况发生的一个常见的地方是当你调整图像的大小。像素的数量变化，但你想要相同的信息。由于像素是离散的，因此缺少空间。插值是如何填补这个空间。这就是为什么你的图像有时拉伸会出来看起来像素化。当原始图像和扩展图像之间的差异较大时，效果更加明显。比如windows显示像素如果过分低于你的屏幕像素,那么你拉伸到屏幕那么大,看起来就都是马赛克,就是这个效果. 而插值算法就是拉伸时如何模拟的去显示出缺失信息的技术 from PIL import Image img = Image.open('./source/cat.jpg') img.thumbnail((64, 64), Image.ANTIALIAS) # 将图片压缩到64x64像素 img.height,img.width#41是因为图片比例 (41, 64) imgplot = plt.imshow(img) plt.show() 内置的插值算法有 'nearest', 最近值,也就是马赛克块 'bilinear', 双线性插值 'bicubic',双三次插值 'spline16'/'spline36', 样条插值 'hanning'/'hamming'/'gaussian'/'kaiser'/'bessel'/'sinc' 窗插值算法 'hermite',埃尔米特插值 'quadric',二次曲面插值 'catrom',[Catmull-Rom插值算法]https://en.wikipedia.org/wiki/Centripetal_Catmull%E2%80%93Rom_spline 'mitchell' 'lanczos'Lanczos算法 imgplot = plt.imshow(img, interpolation=\"nearest\") plt.show() 使用双三次插值(bicubic)模糊处理 imgplot = plt.imshow(img, interpolation=\"bicubic\") plt.show() 图片修改分辨率 img模块还提供了一个缩略图的工具 matplotlib.image.thumbnail(infile, thumbfile, scale=0.1, interpolation='bilinear', preview=False) 它可以直接修改文件并保存为另一文件,只是类型限制在png,svg和pdf三种 mpimg.thumbnail(\"./source/cat.jpg\", \"./source/cat_min.png\", scale=0.15, interpolation='bicubic') Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/保存图片/保存图片.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/保存图片/保存图片.html","title":"保存图片","keywords":"","body":"保存图片 保存图片可以使用matplotlib.pyplot.savefig来实现 from matplotlib import pyplot as plt import numpy as np %matplotlib inline fig = plt.figure(figsize=(40,40)) x=np.linspace(-4,4,30) y=np.sin(x) plt.plot(x,y,'--*b') fig.savefig(\"./source/sin_ex.png\", dpi=15) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/控件.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/控件.html","title":"控件","keywords":"","body":"控件 很神奇的,matplot还提供了简单的可供交互的控件包括 buttons 按钮 check_buttons 选择按钮 radio_buttons 多选按钮 menu 目录 from __future__ import print_function from imp import reload import matplotlib reload(matplotlib) matplotlib.use('nbagg') import matplotlib.backends.backend_nbagg reload(matplotlib.backends.backend_nbagg) import matplotlib.backends.backend_webagg_core reload(matplotlib.backends.backend_webagg_core) import numpy as np import matplotlib.pyplot as plt plt.interactive(False) 按钮bottom 添加按钮可以使用plt.subplots_adjust(bottom=xx)为其在底部留下足够空间 用plt.axes([0.7, 0.05, 0.1, 0.075])为按钮划定大小和位置 添加按钮可以使用Button(ax, label, image=None, color='0.85', hovercolor='0.95') 而为其添加回调函数,则可以用为其绑定on_clicked方法 bnext = Button(axnext, 'Next') bnext.on_clicked(callback.next) 这种形式 from matplotlib.widgets import Button freqs = np.arange(2, 20, 3) fig, ax = plt.subplots() plt.subplots_adjust(bottom=0.2) t = np.arange(0.0, 1.0, 0.001) s = np.sin(2*np.pi*freqs[0]*t) l, = plt.plot(t, s, lw=2) class Index(object): ind = 0 def next(self, event): self.ind += 1 i = self.ind % len(freqs) ydata = np.sin(2*np.pi*freqs[i]*t) l.set_ydata(ydata) plt.draw() def prev(self, event): self.ind -= 1 i = self.ind % len(freqs) ydata = np.sin(2*np.pi*freqs[i]*t) l.set_ydata(ydata) plt.draw() callback = Index() axprev = plt.axes([0.7, 0.05, 0.1, 0.075]) axnext = plt.axes([0.81, 0.05, 0.1, 0.075]) bnext = Button(axnext, 'Next') bnext.on_clicked(callback.next) bprev = Button(axprev, 'Previous') bprev.on_clicked(callback.prev) plt.show() 选择按钮CheckButtons from matplotlib.widgets import CheckButtons t = np.arange(0.0, 2.0, 0.01) s0 = np.sin(2*np.pi*t) s1 = np.sin(4*np.pi*t) s2 = np.sin(6*np.pi*t) fig, ax = plt.subplots() l0, = ax.plot(t, s0, visible=False, lw=2) l1, = ax.plot(t, s1, lw=2) l2, = ax.plot(t, s2, lw=2) plt.subplots_adjust(left=0.2) rax = plt.axes([0.05, 0.4, 0.1, 0.15]) check = CheckButtons(rax, ('2 Hz', '4 Hz', '6 Hz'), (False, True, True)) def func(label): if label == '2 Hz': l0.set_visible(not l0.get_visible()) elif label == '4 Hz': l1.set_visible(not l1.get_visible()) elif label == '6 Hz': l2.set_visible(not l2.get_visible()) plt.draw() check.on_clicked(func) plt.show() 单选框RadioButtons from matplotlib.widgets import RadioButtons t = np.arange(0.0, 2.0, 0.01) s0 = np.sin(2*np.pi*t) s1 = np.sin(4*np.pi*t) s2 = np.sin(8*np.pi*t) fig, ax = plt.subplots() l, = ax.plot(t, s0, lw=2, color='red') plt.subplots_adjust(left=0.3) axcolor = 'lightgoldenrodyellow' rax = plt.axes([0.05, 0.7, 0.15, 0.15], axisbg=axcolor) radio = RadioButtons(rax, ('2 Hz', '4 Hz', '8 Hz')) def hzfunc(label): hzdict = {'2 Hz': s0, '4 Hz': s1, '8 Hz': s2} ydata = hzdict[label] l.set_ydata(ydata) plt.draw() radio.on_clicked(hzfunc) rax = plt.axes([0.05, 0.4, 0.15, 0.15], axisbg=axcolor) radio2 = RadioButtons(rax, ('red', 'blue', 'green')) def colorfunc(label): l.set_color(label) plt.draw() radio2.on_clicked(colorfunc) rax = plt.axes([0.05, 0.1, 0.15, 0.15], axisbg=axcolor) radio3 = RadioButtons(rax, ('-', '--', '-.', 'steps', ':')) def stylefunc(label): l.set_linestyle(label) plt.draw() radio3.on_clicked(stylefunc) plt.show() /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead. warnings.warn(message, mplDeprecation, stacklevel=1) 滑块Slider from matplotlib.widgets import Slider, Button, RadioButtons fig, ax = plt.subplots() plt.subplots_adjust(left=0.25, bottom=0.25) t = np.arange(0.0, 1.0, 0.001) a0 = 5 f0 = 3 s = a0*np.sin(2*np.pi*f0*t) l, = plt.plot(t, s, lw=2, color='red') plt.axis([0, 1, -10, 10]) axcolor = 'lightgoldenrodyellow' axfreq = plt.axes([0.25, 0.1, 0.65, 0.03], axisbg=axcolor) axamp = plt.axes([0.25, 0.15, 0.65, 0.03], axisbg=axcolor) sfreq = Slider(axfreq, 'Freq', 0.1, 30.0, valinit=f0) samp = Slider(axamp, 'Amp', 0.1, 10.0, valinit=a0) def update(val): amp = samp.val freq = sfreq.val l.set_ydata(amp*np.sin(2*np.pi*freq*t)) fig.canvas.draw_idle() sfreq.on_changed(update) samp.on_changed(update) resetax = plt.axes([0.8, 0.025, 0.1, 0.04]) button = Button(resetax, 'Reset', color=axcolor, hovercolor='0.975') def reset(event): sfreq.reset() samp.reset() button.on_clicked(reset) rax = plt.axes([0.025, 0.5, 0.15, 0.15], axisbg=axcolor) radio = RadioButtons(rax, ('red', 'blue', 'green'), active=0) def colorfunc(label): l.set_color(label) fig.canvas.draw_idle() radio.on_clicked(colorfunc) plt.show() /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead. warnings.warn(message, mplDeprecation, stacklevel=1) 光标Cursor from matplotlib.widgets import Cursor fig = plt.figure(figsize=(8, 6)) ax = fig.add_subplot(111, axisbg='#FFFFCC') x, y = 4*(np.random.rand(2, 100) - .5) ax.plot(x, y, 'o') ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) # set useblit = True on gtkagg for enhanced performance cursor = Cursor(ax, useblit=True, color='red', linewidth=2) plt.show() /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead. warnings.warn(message, mplDeprecation, stacklevel=1) 多路光标 from matplotlib.widgets import MultiCursor t = np.arange(0.0, 2.0, 0.01) s1 = np.sin(2*np.pi*t) s2 = np.sin(4*np.pi*t) fig = plt.figure() ax1 = fig.add_subplot(211) ax1.plot(t, s1) ax2 = fig.add_subplot(212, sharex=ax1) ax2.plot(t, s2) multi = MultiCursor(fig.canvas, (ax1, ax2), color='r', lw=1) plt.show() 矩形框 from matplotlib.widgets import RectangleSelector def line_select_callback(eclick, erelease): 'eclick and erelease are the press and release events' x1, y1 = eclick.xdata, eclick.ydata x2, y2 = erelease.xdata, erelease.ydata print(\"(%3.2f, %3.2f) --> (%3.2f, %3.2f)\" % (x1, y1, x2, y2)) print(\" The button you used were: %s %s\" % (eclick.button, erelease.button)) def toggle_selector(event): print(' Key pressed.') if event.key in ['Q', 'q'] and toggle_selector.RS.active: print(' RectangleSelector deactivated.') toggle_selector.RS.set_active(False) if event.key in ['A', 'a'] and not toggle_selector.RS.active: print(' RectangleSelector activated.') toggle_selector.RS.set_active(True) fig, current_ax = plt.subplots() # make a new plotingrange N = 100000 # If N is large one can see x = np.linspace(0.0, 10.0, N) # improvement by use blitting! plt.plot(x, +np.sin(.2*np.pi*x), lw=3.5, c='b', alpha=.7) # plot something plt.plot(x, +np.cos(.2*np.pi*x), lw=3.5, c='r', alpha=.5) plt.plot(x, -np.sin(.2*np.pi*x), lw=3.5, c='g', alpha=.3) print(\"\\n click --> release\") # drawtype is 'box' or 'line' or 'none' toggle_selector.RS = RectangleSelector(current_ax, line_select_callback, drawtype='box', useblit=True, button=[1, 3], # don't use middle button minspanx=5, minspany=5, spancoords='pixels', interactive=True) plt.connect('key_press_event', toggle_selector) plt.show() click --> release 选定区域SpanSelector from matplotlib.widgets import SpanSelector fig = plt.figure(figsize=(8, 6)) ax = fig.add_subplot(211, axisbg='#FFFFCC') x = np.arange(0.0, 5.0, 0.01) y = np.sin(2*np.pi*x) + 0.5*np.random.randn(len(x)) ax.plot(x, y, '-') ax.set_ylim(-2, 2) ax.set_title('Press left mouse button and drag to test') ax2 = fig.add_subplot(212, axisbg='#FFFFCC') line2, = ax2.plot(x, y, '-') def onselect(xmin, xmax): indmin, indmax = np.searchsorted(x, (xmin, xmax)) indmax = min(len(x) - 1, indmax) thisx = x[indmin:indmax] thisy = y[indmin:indmax] line2.set_data(thisx, thisy) ax2.set_xlim(thisx[0], thisx[-1]) ax2.set_ylim(thisy.min(), thisy.max()) fig.canvas.draw() # set useblit True on gtkagg for enhanced performance span = SpanSelector(ax, onselect, 'horizontal', useblit=True, rectprops=dict(alpha=0.5, facecolor='red')) plt.show() /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead. warnings.warn(message, mplDeprecation, stacklevel=1) Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/动画/动画.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/动画/动画.html","title":"动画","keywords":"","body":"动画 动画说白了就是隔段时间刷新一下画面,matplotlib.animation提供了构建动画的工具主要是这几个方法: 通过函数构建 通过作品构建 通过继承matplotlib.animation.TimedAnimation类制作动画 动画需要后渲染后端,一般使用ffmpeg,安装方法可以看]这里 无论哪种方式构建的动画都可以通过.save方法保存为希望的格式,也可以用.to_html5_video方法输出一份html5可以读取的文件. from __future__ import print_function from imp import reload import matplotlib reload(matplotlib) matplotlib.use('nbagg') import matplotlib.backends.backend_nbagg reload(matplotlib.backends.backend_nbagg) import matplotlib.backends.backend_webagg_core reload(matplotlib.backends.backend_webagg_core) import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation plt.interactive(False) 通过函数构建动画 matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs) FuncAnimation类是最常用的构建函数. 他的构造函数有这样一些参数 fig 一个figure对象,图像都是基于这个对象相当于一个画板 func(frame)->tuple(axes.plot) 一个构建动画函数,参数是frames传入的每一帧,而返回的是一个由参数构建的plot对象构成的tuple frames 一个可迭代对象,可以是生成器,可以是一个序列,也可以是数字,数字相当于传入一个xrange(n) init_func(frame)->tuple(axes.plot) 初始化函数,第0帧时候调用它构建图形 如果blit = True，func和init_func必须返回一个可迭代的作品对象来重绘 kwargs包括repeat，repeat_delay和interval： interval每隔interval毫秒绘制一个新帧。 repeat控制动画是否应在帧序列完成时重复。 repeat_delay可选地在重复动画之前添加以毫秒为单位的延迟。 from IPython.display import HTML %matplotlib inline plt.style.use(\"animation_support\") 使用数字定义帧 fig, ax = plt.subplots() x = np.arange(0, 2*np.pi, 0.01) line, = ax.plot(x, np.sin(x)) def animate(i): line.set_ydata(np.sin(x + i/10.0)) # update the data return line, # Init only required for blitting to give a clean slate. def init(): line.set_ydata(np.ma.array(x, mask=True)) return line, ani = animation.FuncAnimation(fig, animate, 200, init_func=init, interval=25, blit=True) ani Your browser does not support the video tag. 使用生成器定义帧 # 使用生成器构建每一帧的传入数据 def data_gen(t=0): cnt = 0 while cnt = xmax: ax.set_xlim(xmin, 2*xmax) ax.figure.canvas.draw() line.set_data(xdata, ydata) return line, ani = animation.FuncAnimation(fig, run, data_gen, blit=False, interval=10, repeat=False, init_func=init) ani Your browser does not support the video tag. 通过作品组合构建动画 matplotlib.animation.ArtistAnimation(fig, artists, *args, **kwargs) 这种方式和上面类似,只是先画好每一幅图,之后按顺序和指定的帧率制作动画 fig2 = plt.figure() x = np.arange(-9, 10) y = np.arange(-9, 10).reshape(-1, 1) base = np.hypot(x, y) ims = [] for add in np.arange(15): ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),)) im_ani = animation.ArtistAnimation(fig2, ims, interval=50, repeat_delay=3000, blit=True) im_ani Your browser does not support the video tag. 通过继承构建 matplotlib.animation.TimedAnimation是上面俩的基类,我们也可以直接继承它来构造动画,主要要重载的是 __init__()方法 _draw_frame方法,对应func new_frame_seq对应frames _init_draw对应init_func from matplotlib.lines import Line2D class SubplotAnimation(animation.TimedAnimation): def __init__(self): fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(2, 2, 2) ax3 = fig.add_subplot(2, 2, 4) self.t = np.linspace(0, 80, 400) self.x = np.cos(2 * np.pi * self.t / 10.) self.y = np.sin(2 * np.pi * self.t / 10.) self.z = 10 * self.t ax1.set_xlabel('x') ax1.set_ylabel('y') self.line1 = Line2D([], [], color='black') self.line1a = Line2D([], [], color='red', linewidth=2) self.line1e = Line2D( [], [], color='red', marker='o', markeredgecolor='r') ax1.add_line(self.line1) ax1.add_line(self.line1a) ax1.add_line(self.line1e) ax1.set_xlim(-1, 1) ax1.set_ylim(-2, 2) ax1.set_aspect('equal', 'datalim') ax2.set_xlabel('y') ax2.set_ylabel('z') self.line2 = Line2D([], [], color='black') self.line2a = Line2D([], [], color='red', linewidth=2) self.line2e = Line2D( [], [], color='red', marker='o', markeredgecolor='r') ax2.add_line(self.line2) ax2.add_line(self.line2a) ax2.add_line(self.line2e) ax2.set_xlim(-1, 1) ax2.set_ylim(0, 800) ax3.set_xlabel('x') ax3.set_ylabel('z') self.line3 = Line2D([], [], color='black') self.line3a = Line2D([], [], color='red', linewidth=2) self.line3e = Line2D( [], [], color='red', marker='o', markeredgecolor='r') ax3.add_line(self.line3) ax3.add_line(self.line3a) ax3.add_line(self.line3e) ax3.set_xlim(-1, 1) ax3.set_ylim(0, 800) animation.TimedAnimation.__init__(self, fig, interval=50, blit=True) def _draw_frame(self, framedata): i = framedata head = i - 1 head_len = 10 head_slice = (self.t > self.t[i] - 1.0) & (self.t ani = SubplotAnimation() ani Your browser does not support the video tag. 动画的输出 最简单的输出就是直接通过matplotlib输出,直接plt.show()即可,注意这种方式jupyter notebook并不支持.只能在脚本中使用 输出为html5可读的视屏 使用.to_html5_video()方法可以直接输出一段浏览器可以识别的带标签的html5字符串这种可以直接嵌入到网页前端 保存动画 动画保存是通过.save(filename, writer=None, fps=None, dpi=None, codec=None, bitrate=None, extra_args=None, metadata=None, extra_anim=None, savefig_kwargs=None)方法 writer可以自己定义转码工具,默认为ffmpeg fps为帧率 dpi控制动画每帧中的每英寸点数. codec指定保存的格式,默认使用filename的后缀作为格式 bitrate指定压缩影片每秒使用的位数，以千位/秒为单位。更高的数字意味着更高质量的电影，但是以增加的文件大小为代价。如果未指定值，则默认值为rcparam animation.bitrate给出的值。 metadata元数据包括在输出文件中的元数据的键和值的字典。可能有用的一些键包括：标题，艺术家，流派，主题，版权，srcform，注释。 动画生成和转码工具的设置 matplotlib本身就是个作图的工具,本身无法制作动画,但他可以通过其他工具提供了几个参数用来转码和设置,我们可以定义一个主题来实现需要的时候加载 animation.ffmpeg_path : /bin/ffmpeg animation.convert_path: /imagemagick/6.9.7-0/bin/convert animation.html: html5 通过这样的设置可以使用ffmpeg工具转码为常见的格式如MP4,也可以通过指定writer参数来使用imagemagick构建gif动画图片. 第三个参数是为了让jupyter notebook可以直接显示动画,如果不设置也可以使用 from IPython.display import HTML 然后HTML(ani.to_html5())来输出动画 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/matplitlib结合web技术.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/matplitlib结合web技术.html","title":"matplitlib结合web技术","keywords":"","body":"matplitlib结合web技术 现今的数据可视化早已不再局限于文章或者出个图就算了,更多的需要使用web技术动态的构建图像,使用web技术做数据可视化当然可以在前端做,比如结合d3.js实现,但如果要从后端生成,我们也可以使用matplotlib来实现. matplotlib的web端使用的绘图技术有两种 基于svg图像标签的图像技术 基于websocket的图像技术 根据实现也分为展示型和交互型,一般来说,交互型就会用到ajax技术,轮询,或者websocket了 当然了,对于生成的动画,也是可以结合web技术实现展示的 我们这次的来以股票数据作为例子来绘制图形,用来查看上证50股的历史k线图 上证50成分股内容如下 symbol_dict = { \"600000\": \"浦发银行\", \"600010\": \"包钢股份\", \"600015\": \"华夏银行\", \"600016\": \"民生银行\", \"600018\": \"上港集团\", \"600028\": \"中国石化\", \"600030\": \"中信证券\", \"600036\": \"招商银行\", \"600048\": \"保利地产\", \"600050\": \"中国联通\", \"600089\": \"特变电工\", \"600104\": \"上汽集团\", \"600109\": \"国金证券\", \"600111\": \"北方稀土\", \"600150\": \"中国船舶\", \"600256\": \"广汇能源\", \"600406\": \"国电南瑞\", \"600518\": \"康美药业\", \"600519\": \"贵州茅台\", \"600583\": \"海油工程\", \"600585\": \"海螺水泥\", \"600637\": \"东方明珠\", \"600690\": \"青岛海尔\", \"600837\": \"海通证券\", \"600887\": \"伊利股份\", \"600893\": \"中航动力\", \"600958\": \"东方证券\", \"600999\": \"招商证券\", \"601006\": \"大秦铁路\", \"601088\": \"中国神华\", \"601166\": \"兴业银行\", \"601169\": \"北京银行\", \"601186\": \"中国铁建\", \"601288\": \"农业银行\", \"601318\": \"中国平安\", \"601328\": \"交通银行\", \"601390\": \"中国中铁\", \"601398\": \"工商银行\", \"601601\": \"中国太保\", \"601628\": \"中国人寿\", \"601668\": \"中国建筑\", \"601688\": \"华泰证券\", \"601766\": \"中国中车\", \"601800\": \"中国交建\", \"601818\": \"光大银行\", \"601857\": \"中国石油\", \"601901\": \"方正证券\", \"601988\": \"中国银行\", \"601989\": \"中国重工\", \"601998\": \"中信银行\"} 我们通过输入编号和日期来进行查找 下面是基本的函数 from __future__ import print_function from imp import reload import matplotlib reload(matplotlib) matplotlib.use('nbagg') import matplotlib.backends.backend_nbagg reload(matplotlib.backends.backend_nbagg) import matplotlib.backends.backend_webagg_core reload(matplotlib.backends.backend_webagg_core) import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation plt.interactive(False) from matplotlib.finance import quotes_historical_yahoo_ochl from matplotlib.finance import candlestick_ochl from matplotlib.dates import YearLocator, MonthLocator, DateFormatter ,WeekdayLocator,MONDAY,DayLocator import datetime import numpy as np /Users/huangsizhe/LIB/CONDA/anaconda/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead. warnings.warn(message, mplDeprecation, stacklevel=1) symbol_dict = { \"600000\": u\"浦发银行\", \"600010\": u\"包钢股份\", \"600015\": u\"华夏银行\", \"600016\": u\"民生银行\", \"600018\": u\"上港集团\", \"600028\": u\"中国石化\", \"600030\": u\"中信证券\", \"600036\": u\"招商银行\", \"600048\": u\"保利地产\", \"600050\": u\"中国联通\", \"600089\": u\"特变电工\", \"600104\": u\"上汽集团\", \"600109\": u\"国金证券\", \"600111\": u\"北方稀土\", \"600150\": u\"中国船舶\", \"600256\": u\"广汇能源\", \"600406\": u\"国电南瑞\", \"600518\": u\"康美药业\", \"600519\": u\"贵州茅台\", \"600583\": u\"海油工程\", \"600585\": u\"海螺水泥\", \"600637\": u\"东方明珠\", \"600690\": u\"青岛海尔\", \"600837\": u\"海通证券\", \"600887\": u\"伊利股份\", \"600893\": u\"中航动力\", \"600958\": u\"东方证券\", \"600999\": u\"招商证券\", \"601006\": u\"大秦铁路\", \"601088\": u\"中国神华\", \"601166\": u\"兴业银行\", \"601169\": u\"北京银行\", \"601186\": u\"中国铁建\", \"601288\": u\"农业银行\", \"601318\": u\"中国平安\", \"601328\": u\"交通银行\", \"601390\": u\"中国中铁\", \"601398\": u\"工商银行\", \"601601\": u\"中国太保\", \"601628\": u\"中国人寿\", \"601668\": u\"中国建筑\", \"601688\": u\"华泰证券\", \"601766\": u\"中国中车\", \"601800\": u\"中国交建\", \"601818\": u\"光大银行\", \"601857\": u\"中国石油\", \"601901\": u\"方正证券\", \"601988\": u\"中国银行\", \"601989\": u\"中国重工\", \"601998\": u\"中信银行\" } plt.style.use('chinese_support') def draw_k(id_str,from_date_str,to_date_str): #设置x轴坐标刻度 mondays = WeekdayLocator(MONDAY) # 主要刻度 alldays = DayLocator() # 次要刻度 mondayFormatter = DateFormatter('%m-%d-%Y') # 如：2-29-2015 dayFormatter = DateFormatter('%d') from_date = tuple((int(i) for i in from_date_str.strip().split(\"-\"))) to_date = tuple((int(i) for i in to_date_str.strip().split(\"-\"))) quotes_ochl = quotes_historical_yahoo_ochl(id_str+'.ss', from_date ,to_date) fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) ax.xaxis.set_major_locator(mondays) ax.xaxis.set_minor_locator(alldays) ax.xaxis.set_major_formatter(mondayFormatter) candlestick_ochl(ax, quotes_ochl, width=0.6, colorup='r', colordown='g') ax.xaxis_date() ax.autoscale_view() plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right') ax.grid(True) plt.title(symbol_dict.get(id_str,u\"未知\")) plt.show() draw_k('600000','2016-6-20','2016-7-20') 基于SVG的展示型图像 一种方式是将svg图像写入一个伪造的文件,然后将其取出,把头部修改了,这样就可以直接用了 from pyquery import PyQuery as Q from io import BytesIO import json def deal_with_svg(f): # Create XML tree from the SVG file. value = f.getvalue() # Add attributes to the patch objects. # Add a transition effect result = Q(value) return result.__str__() def draw_k_svg(id_str,from_date_str,to_date_str): #设置x轴坐标刻度 mondays = WeekdayLocator(MONDAY) # 主要刻度 alldays = DayLocator() # 次要刻度 mondayFormatter = DateFormatter('%m-%d-%Y') # 如：2-29-2015 dayFormatter = DateFormatter('%d') from_date = tuple((int(i) for i in from_date_str.strip().split(\"-\"))) to_date = tuple((int(i) for i in to_date_str.strip().split(\"-\"))) quotes_ochl = quotes_historical_yahoo_ochl(id_str+'.ss', from_date ,to_date) fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) ax.xaxis.set_major_locator(mondays) ax.xaxis.set_minor_locator(alldays) ax.xaxis.set_major_formatter(mondayFormatter) candlestick_ochl(ax, quotes_ochl, width=0.6, colorup='r', colordown='g') ax.xaxis_date() ax.autoscale_view() plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right') ax.grid(True) plt.title(symbol_dict.get(id_str,u\"未知\")) f = BytesIO() plt.savefig(f, format=\"svg\") return deal_with_svg(f) from IPython.display import HTML HTML(draw_k_svg('600000','2016-6-20','2016-7-20')) *{stroke-linecap:butt;stroke-linejoin:round;} 样例代码可以在这里看到 基于websocket的绘图技术 通过这种方式可以提供交互,把图片连同工具栏一起发送到客户端,具体的方法可以看样例代码,这边提供了2种样例代码,分别使用: tornado flask+gevent-websocket 使用.to_html5_video()直接输出动画 在支持html5的的浏览器上完全可以直接用.to_html5_video()来获得可以输出视频 from matplotlib.animation import FuncAnimation def rain(): fig = plt.figure(figsize=(7, 7)) ax = fig.add_axes([0, 0, 1, 1], frameon=False) ax.set_xlim(0, 1), ax.set_xticks([]) ax.set_ylim(0, 1), ax.set_yticks([]) # Create rain data n_drops = 50 rain_drops = np.zeros(n_drops, dtype=[('position', float, 2), ('size', float, 1), ('growth', float, 1), ('color', float, 4)]) # Initialize the raindrops in random positions and with # random growth rates. rain_drops['position'] = np.random.uniform(0, 1, (n_drops, 2)) rain_drops['growth'] = np.random.uniform(50, 200, n_drops) # Construct the scatter which we will update during animation # as the raindrops develop. scat = ax.scatter(rain_drops['position'][:, 0], rain_drops['position'][:, 1], s=rain_drops['size'], lw=0.5, edgecolors=rain_drops['color'], facecolors='none') def update(frame_number): # Get an index which we can use to re-spawn the oldest raindrop. current_index = frame_number % n_drops # Make all colors more transparent as time progresses. rain_drops['color'][:, 3] -= 1.0/len(rain_drops) rain_drops['color'][:, 3] = np.clip(rain_drops['color'][:, 3], 0, 1) # Make all circles bigger. rain_drops['size'] += rain_drops['growth'] # Pick a new position for oldest rain drop, resetting its size, # color and growth factor. rain_drops['position'][current_index] = np.random.uniform(0, 1, 2) rain_drops['size'][current_index] = 5 rain_drops['color'][current_index] = (0, 0, 0, 1) rain_drops['growth'][current_index] = np.random.uniform(50, 200) # Update the scatter collection, with the new colors, sizes and positions. scat.set_edgecolors(rain_drops['color']) scat.set_sizes(rain_drops['size']) scat.set_offsets(rain_drops['position']) # Construct the animation, using the update function as the animation # director. animation = FuncAnimation(fig, update, interval=10) return animation.to_html5_video() HTML(rain()) Your browser does not support the video tag. 使用gif图片嵌入img输出动画 我们也可以用类似svg嵌入网页的方式嵌入gif from numpy import sin, cos import scipy.integrate as integrate import matplotlib.animation as animation plt.style.use(\"animation_support\") import os import base64 import time def double_pendulum(): G = 9.8 # acceleration due to gravity, in m/s^2 L1 = 1.0 # length of pendulum 1 in m L2 = 1.0 # length of pendulum 2 in m M1 = 1.0 # mass of pendulum 1 in kg M2 = 1.0 # mass of pendul def derivs(state, t): dydx = np.zeros_like(state) dydx[0] = state[1] del_ = state[2] - state[0] den1 = (M1 + M2)*L1 - M2*L1*cos(del_)*cos(del_) dydx[1] = (M2*L1*state[1]*state[1]*sin(del_)*cos(del_) + M2*G*sin(state[2])*cos(del_) + M2*L2*state[3]*state[3]*sin(del_) - (M1 + M2)*G*sin(state[0]))/den1 dydx[2] = state[3] den2 = (L2/L1)*den1 dydx[3] = (-M2*L2*state[3]*state[3]*sin(del_)*cos(del_) + (M1 + M2)*G*sin(state[0])*cos(del_) - (M1 + M2)*L1*state[1]*state[1]*sin(del_) - (M1 + M2)*G*sin(state[2]))/den2 return dydx # create a time array from 0..100 sampled at 0.05 second steps dt = 0.05 t = np.arange(0.0, 20, dt) # th1 and th2 are the initial angles (degrees) # w10 and w20 are the initial angular velocities (degrees per second) th1 = 120.0 w1 = 0.0 th2 = -10.0 w2 = 0.0 # initial state state = np.radians([th1, w1, th2, w2]) # integrate your ODE using scipy.integrate. y = integrate.odeint(derivs, state, t) x1 = L1*sin(y[:, 0]) y1 = -L1*cos(y[:, 0]) x2 = L2*sin(y[:, 2]) + x1 y2 = -L2*cos(y[:, 2]) + y1 fig = plt.figure() ax = fig.add_subplot(111, autoscale_on=False, xlim=(-2, 2), ylim=(-2, 2)) ax.grid() line, = ax.plot([], [], 'o-', lw=2) time_template = 'time = %.1fs' time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes) def init(): line.set_data([], []) time_text.set_text('') return line, time_text def animate(i): thisx = [0, x1[i], x2[i]] thisy = [0, y1[i], y2[i]] line.set_data(thisx, thisy) time_text.set_text(time_template % (i*dt)) return line, time_text ani = animation.FuncAnimation(fig, animate, np.arange(1, len(y)), interval=25, blit=True, init_func=init) timestmp = time.time() ani.save(\"{stmp}_temp.gif\".format(stmp=timestmp), writer='imagemagick',codec=\"gif\",fps=15) with open(\"{stmp}_temp.gif\".format(stmp=timestmp),\"rb\") as f: value = base64.b64encode(f.read()).decode() os.remove(\"{stmp}_temp.gif\".format(stmp=timestmp)) result = ''.format(value=value) return result HTML(double_pendulum()) IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`. 这种方式最好不要用,很慢 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"工具链篇/数据可视化工具/使用matplotlib做数据可视化/结语.html":{"url":"工具链篇/数据可视化工具/使用matplotlib做数据可视化/结语.html","title":"结语","keywords":"","body":"结语 数据可视化也算是人机交互的一种,当然更多的是展示,交互并不是重点.通常数据可视化用于辅助分析和辅助决策,且面向的用户通常是非技术人员,本身并不是什么关键性技术.但越是靠近人一端,应用就越是复杂.所以更多的时候数据可视化是体力活不是脑力活. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-13 21:01:04 "},"数据管理篇/":{"url":"数据管理篇/","title":"数据管理篇","keywords":"","body":"数据管理 当我们的数据进入了系统,如何管理这些数据就成了问题,数据管理是为了让可以使用数据的人可以有数据可用,可以快速找到需要的数据;同时确保数据的安全性和整个数据流转的性能. 因此数据管理是个偏向架构偏向工程的工作.需要对数据库,消息中间件,以及如何搭配使用这些工具有所了解;同时也要对数据的压缩,加密,序列化保存有一定了解. 学习数据科学的人可能最开始会认为机器学习,算法是数科学的核心,但实际上一旦接触了实际工程就会发现数据管理才是数据科学的核心.算法其实是数据本身的一个表现,算法可以决定数据使用效果的上限,而数据管理可以决定使用效果的下限. 内容介绍 本篇是相关技术的个人最佳实践总结,将介绍如下内容: 数据的标准化 数据的生命周期 数据规模与数据管理 数据的监控 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:54:09 "},"数据管理篇/数据规模与数据管理.html":{"url":"数据管理篇/数据规模与数据管理.html","title":"数据规模与数据管理","keywords":"","body":"数据规模和数据管理 随着数据规模的增加,数据管理手段和工具都需要做出响应变化,之所以不同规模的数据要用不通的技术手段,根本上来说是机器的资源无法满足大规模存储/计算的需求.再往深了说是高资源配置的机器太贵了.因此可以看到几乎所有的大数据解决方案都是集群化方案--利用大量便宜的低配置机器组成集群,通过调度管理程序统一调配资源: 将任务拆分重构为子任务构成的任务图放入调度器 借助队列将任务分发到多台机器处理然后再聚合 数据管理当然也需要根据不同的数据规模来调整数据管理方式和使用的工具. 不同规模下的技术选型 基本上技术选型需要考虑的是资源成本和预期收益,说到底数据是为盈利服务的. 比如一个赔钱的业务,那它就没有必要做什么数据管理,因为投的多赔的多; 如果是小本经营比如小区里开个小面包店,那它就比较适合将业务和数据结合管理,买台2000块的电脑,装个office,每天一张excel表就足以解决问题; 如果面包店开大了,连锁了有了两家分店,那不同分店间的数据对账就成了问题,这时候就需要一个所谓的数据中心(总店里电脑升级下配置,加块ssd,和运营商开下对外端口)来维护每个店的资源和资金; 面包店希望做到更好的提高回头客的比例,希望通过会员制等措施知道客户希望买到什么样的面包,这个时候就需要有算法,有分析的进入,就会有推荐,用户调研,决策支持等需求,这个时候就需要再升级数据中心的配置,更大的内存更多强的cpu,甚至多机组成集群,需要有定时任务分析每天的交易情况; 随着面包店的扩大数据越来越多,数据存在哪里成了问题,一来全存存不下,要想存下需要花很多钱买硬盘,二来数据多了搜索,调用性能堪忧,这时候数据就需要按热度分出生命周期,处在生命周期不同阶段的数据放在不同的工具中,用的多的数据放在高性能高价的存储介质中,用的少的只是留档就放在最便宜的设备中.数据的分析也分析不过来--用于分析的数据规模大于单机的内存,这个时候就要借助分布式计算框架,比如spark,比如dask. 再进一步的面包店希望数据的分析结果可以实时的反馈给门店,快速响应跟上市场的变化,这个时候就需要引入流的概念,使用流处理工具. 上面的例子可以看出技术只是手段,是为了解决问题才发展出来的,因此什么阶段使用什么样的技术,不同阶段间技术如何过度就成了技术选型需要考虑的问题. 本文给出的技术选型方案是渐进式的,没有替换只有增加和撤销,这点需要注意,另外在可以满足需求的情况下不要没事尝试新的技术或者使用下一阶段的技术,尝试新技术是个试错的过程,必然会有大量失败浪费大量人力物力,尤其是在公司还规模小的时候.不少公司是折腾死的. 另外一个需要注意的是关于部署的问题,原则上我个人不支持直接部署,更加支持使用docker部署一切需要部署的业务程序.利用容器配合k8s或者swarm技术可以大大降低运维成本.而一些基础组件比如数据库,缓存,消息中间件等为了更加可靠更加建议如果不是自建机房就购买运营商的专业服务,这样可以省很多运维成本.如果是自建机房则建议单独购买机器专门配置运维. 起步阶段 在业务初期,盲目的追求新技术往往得不偿失,业务初期的首要目标永远是赚钱,先能活下去再考虑如何提高效率.如果提高了效率也不能赚钱那就更加得不偿失了,因为业务小,所以数据量也小,一天的数据量可能也就一个100M以下的excel文件就可以保存.因此快速试错快速失败才是业务初期对数据的最大需求.正如上面的例子所说,excel(office,wps)就是初期最好的工具: 廉价 本身几乎免费,人力方面--几乎人人都会用excel而且学习成本极低.这项技能甚至都不用写到招聘需求里 功能够用 即便不会vb编程,不会写宏,excel也已经可以用作记录,筛选,查找,分类,可视化的功能,足以满足绝大多数数据方面的需求. 存储灵活 excel说白了是文件存储,这就意味着可以很灵活的存储,可以放硬盘,移动硬盘,u盘都行 足以应付简单分析需要 初期阶段 此处业务初期指业务已经找到了盈利模式,可以稳定增长,可以养得起技术团队并且有继续做大想法的阶段.这个阶段的特点是业务已经成型,但规模没有上去.这个阶段的主要任务就是修好内功打好地基,为后续扩大规模抢占市场提供支持. 这个阶段往往是企业成败的关键,往往一个企业的骨架就是在这个时候定型的,通常这个阶段只会存在最长1年时间,但这一年之后的所有行为可能都是这一年中决定的延续. 这个阶段企业要定好利益分配框架,权力分配架构,技术架构,当然了企业本身架构不是本文的讨论范围,而且远比技术架构的成型原因复杂的多.一个足够优秀技术架构可以让企业在快速扩张时在技术方面降低内耗,专心于业务.但技术架构往往也是由利益分配框架和权力分配架构影响形成的,比如创始人中有精通java的并且权力很大,那很容易就影响到技术栈会用全套java.没有人懂技术,那很可能就是谁出钱多就听谁瞎指挥.因此比较建议任何初创团队都应该有一个懂数据的创始人. 当然了能安然度过这个阶段其实已经是胜利了,毕竟很多企业活不过两年. 这个阶段的数据量会上来,一天的数据量可能可以上G,但基本不会超过100G,基本还是在单机可以处理的程度.但基本的数据相关架构就已经可以根据需求完善起来了. 这个阶段建议不要自建机房,而是使用阿里云腾讯云aws这样的服务商提供的虚拟主机和运维服务.自建机房不光是费钱的问题,更多的是需要耗费人力,运维工作非常专业化,专业人员贵,非专业人员又难以解决问题.救我了解有不少公司原本有自建机房的也都纷纷上了云.这个阶段并不适合自建机房.如果有自建机房的需求建议在稳定阶段逐步迁移. 存储方案 初期存储方案一般是使用数据库.需要用到数据库时比较建议直接使用postgresql,而不是跟风用什么mysql或者其他nosql数据库. 廉价 postgresql本身开源,任何人都可以安装使用,pg使用sql语句作为通用的交互接口,任何学过数据库的人一定都会使用sql,因此人力成本上相对也不高.pg的运维成本也不高,只是国内用的人少会让人觉得难,实际上它的文档相当齐全,本身设计优秀几乎不需要额外的运维.至于学习成本其实更低了,官方文档有高质量的中文翻译,大多数问题都可以看它解决. 功能全 postgresql几乎是最全能的数据库.丰富的扩展让它可以满足几乎所有需求. 使用的生命周期长 postgresql可以在进入整个业务技术栈后就一直有一席之地.开始可以全部使用它来存储数据,甚至可以做广播器.慢慢的业务扩大了可以分表,可以将一些特殊数据存入其扩展,比如时序数据库timescale,再然后可以作为热数据的存储工具. 当然这个阶段就需要有相关的技术人员参与了,建议招聘有编程能力的专业技术人员或者自己学习运维,而不是只会改配置的所谓运维.一个这样的人可以管整个数据层,在下一个阶段之前都可以不需要额外的人手,后续只是给他找帮手而已. 数据分析方案 当不再满足于存储和简单查询这样的方式来利用数据时,比如要分析用户时,就需要专业的数据分析人员了.这个阶段初期一样需要的技术还是postgresql和excel.但成本是在人力上,数据分析数据挖掘是技术活,并不是人人可以做,这个阶段一般技术栈有两种 sql进阶+excel进阶,sql语句本身图灵完备,pg完全支持复杂的sql查询语句,那问题就在人会不会写复杂的查询语句了.查出需要的数据后可以导入excel借助插件做方差分析等数据分析工作,并且可以使用excel自带的工具做数据可视化.这个技术栈好处是没有引入任何新的技术,只是在老技术上做深化.事实上很多传统行业都是这么做的.但瓶颈实际上在excel上,excel可以处理的数据量有限,文件一旦过大无论打开还是计算都会很低效;同时支持的算法少,自己写算法也麻烦.当然如果这个方案足够使用那就没必要引入新的技术栈了. sql进阶+python数据科学工具组,所谓的python数据科学工具组大致就是numpy+scipy+pandas+sklearn+StatsModels+pymc3,这一套基本涵盖了数据分析数据挖掘所需要用到的所有工具.好处是可以处理单机性能下的所有数据分析和数据挖掘任务.坏处就是这一串的学习成本不低,因此人力成本也就不低,因此还是那句话,如果没有必要那就不要引入,具体还是要视业务需求来定 数据ETL方案 既然有了数据分析的需求,自然的就会有从原始数据到分析结果全程的数整备需求以及定时执行任务的需求,这个时候就会需要使用数据ETL工具.ETL工具通常是定时执行的批处理任务,可以理解为以时间截断数据分批处理,最终获得到需要的结果. 相对比较常用的是airflow,这个工具现在在阿帕奇基金会下,完全开源,功能全面,后续的工具介绍部分会详细介绍.这个工具优点是可以和其他技术栈比较完美的结合 可扩展性极佳.它可以单机部署,在业务需求量不大的情况下足以应付,在下一阶段可以切换为基于dask的集群部署,老的单机实例也不需要舍弃,作为开发测试实例可以继续发挥余热. 使用python语言编写,也使用完整的python生态,对使用第二种数据分析方案的来说没有额外的心智负担. 可以编写计算图,每步的结果可以带入下一步作为参数 有监控ui,可以直观的管理执行流程. 事件驱动方案 另一方面一些对实时性有要求的任务需要事件驱动,这就得借助消息队列了,这个阶段依然是两个选择: 使用pg作为消息中间件,具体就是使用pg自带notify/listen命令做发布订阅.这个方案相对是学习成本最低的,,可以用作发布订阅模式,在不做分布式/多实例部署的情况下完全可以满足需求. 使用redis作为这个消息中间件,这个方案相对更具可扩展性. 性能够强,可以满足实时性要求,而且有集群模式,在后续阶段可以使用集群提高可用性. 数据结构多样,可以满足各种需求: Redis 5.0新增加了流数据结构,支持Consumer Group并且可以做消费确认, pub/sub模式可以直接作为广播器使用,一些不需要消费确认的任务完全可以使用它 list数据结构,一些不需要消费确认的任务可以用作传统的生产消费模式. 可以在其他地方做贡献 作为缓存将最热的数据放入其中降低数据库压力 作为布隆过滤器使用 用于去重 用于在线用户计数等 流处理方案 除了ETL批处理外,另一种数据处理方式叫流处理.数据中有用的信息实际上是有限的,流处理可以大致理解为缓存+筛选+统计函数好处是不依赖ETL,高效可持续 这个阶段可以使用的流处理方式比较推荐的有: 使用pg的pipelinedb插件做流处理,好处是学习成本低,依然是postgresql,sql语句就可以完成所有任务,并且可以复用pg的其他技术,也方便管理.坏处也在pg上,pg的集群化不好用,pipelinedb不利于扩展,要扩展只能为不同业务各自起一个pipelinedb实例,但pg本身又略重. 使用python包streamz,这个方案好处在于 它是python模块,对于使用python技术的来说没有额外的心智负担, 灵活可扩展,因为是python模块所以可以复用上面的python数据科学技术栈,同时支持与dask集群结合 当然了坏处也有,就是python本身性能低下. 数据服务方案 一些数据处理完后是要让业务端使用的,一些算法也是在数据这边训练好后要让业务端调用的,这种时候就需要数据提供对应的服务.可以看出数据侧提供的服务都是无状态的,以算法服务为主,因此没有必要使用RESTful接口,一般我们会使用rpc技术,比较通用的就是grpc了.这个技术我们后面也会讲到. 扩张阶段 这个阶段企业会寻求快速扩张,往往会大量招聘,大量引入新技术.这个阶段的技术选型主要是解决数据规模扩大的问题.在要处理的具体问题上其实没有任何变化.这个阶段主要需要引入的技术就是所谓的大数据技术,同时需要开始对数据的生命周期要做好控制. 使用hdfs/hive/对象存储保存冷数据 使用spark/dask做大规模数据的分布式计算 大数据技术选型 大数据技术几乎没有什么选择的余地,但在怎么用上可以给出一些建议 spark更加适合作为hive的sql引擎使用而非分布式计算工具,因为它是java体系下的产物,天生对python支持不好,并不适合以python作为基本工具的数据人员,也无法利用python自己的数据科学生态.也就是说spark/hive应该被看做是一个专用于大规模冷数据的分布式数据库. spark/hive体系适合做离线计算, pg依然可以在大数据环境下发挥作用Greenplum在pb以下优于hive/spark,适合作业务分析,同时,离线计算后的结果很适合放到Greenplum用于查询. 使用dask做为大数据计算工具构建算法提供服务,并结合k8s做好动态资源调配,这样可以节省资源. 除非不需要毫秒级延迟的实时性并且特别需要持久化流中的数据,否则不建议使用kafka替代redis的streams 数据中台 这应该是2019年新炒热的一个名词,来自阿里,核心思想是数据共享,其诞生是为了应对像双十一这样的业务高峰,应对大规模数据的线性可扩展问题,应对复杂业务系统的解耦问题.而在技术/组织架构等方面采取了一些变革,但其本质上还是一个平台,阿里称之为\"共享服务平台(Shared Platform as Service/SPAS)\".SPAS采用的是基于面向服务的架构SOA理念的\"去中心化\"的服务架构,所有的服务都是以\"点对点\"的方式进行交互.数据中台是指通过数据技术,对海量数据进行采集,计算,存储,加工,同时统一标准和口径.可以理解为是本文所讲内容的一个统一管理平台.更多的是统一不同业务间的数据接口,做到数据,算法可以轻易共享.在这个概念之前出现过的概念有: 数据仓库--提供统一的数据存储,查询平台 数据中心--提供大数据的计算能力 实际上所谓数据中台也是在这俩个概念上发展而来,做的事情其实就是 业务数据统一收集,存储,清洗,计算,抽取并最终构成数据资产(就是特定领域数据资源)以服务的形式提供统一接口供业务调用 由数据训练得到的算法同样作为资源以服务的形式提供统一的接口给业务层调用. 因此其概念核心是数据共享,背后依然还是平台化,服务化的思路. 稳定阶段 这个阶段基本上只是在前面的基础上做加法而已.本身结构已经不再需要变动了.这个时候做的更多的是 提高数据安全性 提高算法质量 提高执行效率 如果有想法从云上下来自己建机房,这个时候需要做数据迁移,数据迁移一定要谨慎. 淘汰阶段 任何事物都有生命周期,江山代有才人出,各领风骚数百年.这个阶段并非不再盈利,而是往往业务被其他东西替代,比如胶卷照相机基本上已经退出了历史舞台,但柯达的胶卷依然在小众市场活得好好的,这个阶段更多的是找出路--缩减规模并找到合适的小众市场活下去.因此这个阶段主要要做的是逐步的从复杂庞大的数据系统中整理出核心业务,化繁为简节省资源.这个过程思路基本是上面的过程反过程,使用的技术栈视需求和数据规模而定,最终可以退回到使用execl做最简单数据收集分析的阶段. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:54:09 "},"数据管理篇/数据的标准化.html":{"url":"数据管理篇/数据的标准化.html","title":"数据的标准化","keywords":"","body":"数据的标准化 人是一种很健忘的生物,尤其是在许多人做同一个事情的时候.这点在工程上体现的尤其明显.数据标准化目的就是为了确定一个标准,让参与的人使用同一标准处理同类数据,以避免因为忘记口头定义而丢失信息的情况. 每处的数据都应该标准化 从数据采集,到数据流转的各处,每处数据都应该标准化,有对应文档,有版本管理,这样虽然繁琐,但可以有效的避免数据混乱.做到任何一处可追溯.而且机制一旦建立实际上繁琐的程度有限. 使用兼容性好的通用结构化数据格式序列化数据 数据标准化最忌讳的是自己为数据做编码解码.这种行为往往是偷懒的结果,很容易最后尾大不掉.最直接的方案就是使用现有的兼容性好的通用结构化数据格式.比较推荐的有两种: protobuf json结合json schema 我们从中根据自己的需求选择一种就好,尽量不要两种混用,造成管理上的混乱. 使用protobuf protobuf是目前最快最小的序列化协议,由google开发推动,已经广泛应用于游戏开发,深度学习甚至嵌入式领域.它几乎支持所有主流编程语言. protobuf是结构化的,需要使用专用语法proto3先定义数据的格式(schema)文件*.proto,然后使用专用的编译工具protoc编译到需要的对应编程语言成为一个模块, 最后引用这个模块调用其中的接口就可以针对定义好格式的数据实例进行序列化和反序列化. 使用python做protobuf的序列化反序列化操作可以看这篇 使用protobuf的优势是 性能好,因为实际上格式信息都在编译好的模块中,实际传输的信息就可以很小,也得益于此消息可以很快的传递.pb应该是目前性能最好的结构化序列化协议了 代码即文档,格式文件*.proto本身可读性不错,也有版本管理相关的关键字,而且格式严谨完全可以直接作为文档使用. 劣势也是有的: 可读性,序列化后结果是bytes,基本不可读,因此不容易发现错误 版本更新麻烦,更新依赖格式文件必须编译好后作为模块引用,在不使用容器技术或者批量部署工具的情况下如果要在多台机器上部署更新将会非常考验运维的耐心. 只能验证数据类型,无法验证数据内容. 总结起来就是追求性能就用protobuf 使用json结合json schema json是一种半结构化数据格式,几乎任何编程语言都支持json格式,因此使用起来十分方便;而且兼容性非常好,比如python中它可以直接反序列化为字典,js中更是直接可以作为对象处理;同时json也在各种数据库中作为一种数据类型被支持. 单纯使用json并不好,因为不利于数据验证和版本管理,因此这边可以引入json schema来实现这两个功能. 使用python做json的序列化反序列化操作可以看这篇 使用python做json的数据验证操作可以看这篇 使用json的优势是 可读性,本身json就是一种基于文本的序列化格式,人可以直接读懂. 代码即文档,格式文件schema.json本身可读性不错,也有版本管理相关的关键字,更可以设定数值型的上下界,字符串的re匹配等内容校验功能,数据校验更加严格,而且格式严谨完全可以直接作为文档使用. 可以方便的部署专门的格式文件服务统一管理.因为实际上格式文件还是个json文件,起一个http服务器就可以了,而版本更新更加是可以直接通过重载文件或者直接http访问格式服务器拉取最新格式文件即可.完全可以做到热更新. 劣势也是有的: 性能,json是基于文本的序列化格式,必然很大,性能就比较差了 总结起来就是追求便利就用json+jsonschema 数据格式版本管理 数据标准化是一个必须在顶层做设计的东西,而且很难做渐进式迭代.因此必须在开始就设计好.每次更新都必须小心谨慎,还要考虑兼容性问题. 数据格式的版本管理主要是两个方面: 版本形式和更新模式的约定,这个部分关心的是如何更新数据格式的版本 格式文件获取方式约定,这个部分关心的是外部如何探知版本是否更新了,如何获取到需要版本的格式文件 版本形式和更新模式的约定 通常我们用形如x.y.z的版本号来区分版本,x代表主版本,一般是重构才会修改;y表示特性修改版本,增删字段护着修改字段含义类型才会更新,z表示常规更新版本,一般是修改验证匹配规则或者描述才会更新.x不同的通常完全不兼容,y不同的保持最近3个版本兼容,z则要求完全兼容. 我们的数据应该在序列化后加上被序列化的版本号,数据创建的时间戳等元信息以方便程序按版本号做反序列化.我们以json为例 { \"meta\":{ \"version\":\"1.2.3\", \"source_id\":\"xxxxx\", \"createdby\": \"server A\", \"ctime\":1234125412 }, \"data\":{ ... } } 注意,上面的source_id为一个全局唯一的id号,在数据进入系统时生成,用于追踪每条进入系统的原始数据最终的流转情况 格式文件获取方式约定 现在的软件/系统开发往往时迭代更新的,在数据方面就很容易出现依赖的数据形式更新了版本但处理用的程序并没有做好对接的情况,这种时候就需要约定好格式文件的获取方式和格式文件更新了这个事件的通知方式. 最原始的方法自然是--更新了数据形式后用外部方式(群发邮件/拉个qq群)手工通知依赖这个数据的下游并向他们发送格式文件.这种方式问题在于: 上游的数据开发需要知道有哪些下游,这个在复杂系统中几乎是不可能的 下游有没有收到这个消息及时跟进无法追踪 因此比较好的一种解决方式是起一个中心化的RESTful服务管理所有格式文件,并且提供一个websocket接口向整个组织的人推送版本更新行为,并且需要可以点击确认提供反馈,这个系统就需要运维开发人员来做了. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:54:09 "},"数据管理篇/数据的生命周期.html":{"url":"数据管理篇/数据的生命周期.html","title":"数据的生命周期","keywords":"","body":"数据的生命周期 和其他任何东西一样,数据也有生命周期,数据生命周期管理(data life cycle management,DLM)则是一种基于策略的方法,用于管理信息系统的数据在整个生命周期内的流动--从创建和初始存储到它过时被删除整个过程.大致可以分为如下几个大块 创建阶段,从业务信息中提取有价值的部分构造成数据并放入整个数系统中进行流转 热数据阶段,一般数据在越新的时候越有价值,刚进入系统的数据往往可以作为消息使用(流处理),稍微过时一点的数据也会被查询使用.再过期一些,使用近期数据的一些统计量往往可以比较客观的反映当前的业务状况. 冷数据阶段,当数据失去了时效性,往往我们不会再直接使用它,而是将它作为一些离线统计任务或者预测模型的训练样本使用. 删除阶段,当数据已经几乎不再被任何地方用到了,那从成本考虑我们往往会将其从这个数据系统中删除. 不同生命周期时数据的载体 由于机器成本和数据的价值的不同,我们通常会按上面的顺序选择从贵到便宜,从高性能到低性能的工具作为载体. 创建阶段 创建阶段时,我们希望数据尽量有时效性,因此主要的载体是消息队列或者广播器,如果对实时性要求很高,但可以接受消息丢失,那么我们可以选择redis的PUB/SUB模式;如果要保证消息的完整性和顺序性,则可以使用kafka来做.当然我们也可以将其组合使用. 热数据阶段 热数据阶段通常由两种载体 流数据库,比如pg的插件piplinedb,kafka/redis+spark-streaming/dask+streamz,这些方案并没有太大区别,只是做好消费端的并行化处理防止消息不能及时处理玩阻塞就好,流数基本不会直接落地到数据库,而是被处理成需要的聚合量后再存入指定位置(业务数据库,特征库,模型库甚至直接推送给客户等). 缓存数据库,由于常见的热数据都与时间密切相关,通常可以是时间序列数据库.这里就需要做出选择了,influxdb开源版本只有单机,配套工具并不多,而且查询不是完整的sql语言,表现力不足,限制也多,更关键的是数据量一大,写入一多就极易oom,因此不建议使用,更加推荐的是postgresql的插件timescaledb.它的内存使用率增长相对平缓,而且支持完整sql,虽然在吞吐量,数据压缩性能上比不上influxdb但应付热数据阶段的需求就足够了 冷数据阶段 数据在什么时候由热数据变成冷数据这个要看业务需求,但基本可以这样判定: 数据不会再被单独取出来使用 数据不会再被频繁的取出来使用,使用间隔以天计数. 有以上特征的数据基本就可以作为冷数据处理了.通常我们会使用ETL工具定期的将数据从缓存库转移到存放冷数据的媒介,比较推荐的ETL工具有airflow.和easy scheduler尤其是后者,虽然年轻,但确实方便管理,目前的版本还有些小bug但已经足够支持业务使用了. 而冷数据通常比较大,其存放一般也是使用的相对廉价的介质,比如aws上的s3对象存储,或者是hdfs集群.其存储形式一般也以文件为主,通常为了压缩率和查询效率会使用列存储格式,比如Parquet,每次存储一般都是以时间段为划分,比如按小时,按天来划分文件,具体如何划分需要考虑业务和单文件的容量 删除阶段 删除阶段通常是为了用更加低廉的方式保存过期数据.比如我们保留1年的数据作为冷数据那比较可能的操作是--在每年的1月一日将去年一整年的数据压缩打包后下载到本地的廉价大容量硬盘或者磁带同时每个月初固定删除上一年上个月的数据. 通常存储的数据格式和冷数据阶段一样,但其中数据的间隔时间更长,比如是按月记录,然后再进行压缩. 业务数据生命周期 业务数据往往分为两种: 静态数据 这种数据往往是业务的起点,比如音乐软件库中的音乐就是静态数据.这种数据的特点是更新不频繁,一直需要保持可访问状态,同时一有上架下架消息就会影响到许多的业务,因此比较合适的处理方法是在创建阶段创建的是上架/下架/修改的事件数据,通过在中间件中广播上架下架修改事件让依赖对应数据的业务进行处理. 这种数据最好更多的关注其并发性能,因为他们往往再业务上需要被频繁访问术语OLTP业务,这种比较好的方式有两个: 1. 数据库读写分离,一写多读,分摊查询压力 2. 使用redis缓存,查询访问全部走redis缓存 如果要针对静态数据做OLAP业务,那么比较好的方法是将对饮表复制到OLAP工具.这种数据一般也不用区分冷热,也都不会删除. 事件数据 这种数据就是比较典型的了,它一般会走完从创建到删除的整个流程.通常它们不会被直接取出来用,而是分批的做统计数据. 统计数据生命周期 统计数据通常是由事件数据得来,很多公司为了省钱只会保留最近的一次切面数据,但实际上统计数据的变化情况对分析业务状况非常有价值,所以也应该像事件数据一样走完整个生命周期. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:54:09 "},"数据管理篇/数据的监控.html":{"url":"数据管理篇/数据的监控.html","title":"数据的监控","keywords":"","body":"数据监控 数据监控是数据管理的一个很重要的环节,主要是为了确保数据/模型的质量,从而确保业务的稳定. 数据监控 数据监控的维度主要包括: 完整性 准确性 一致性 及时性 往往这些维度不可兼得,就要根据业务需求来设计不同的监控方案了 针对实时性要求高的数据 我们对实时性要求高的数据应该要在每个步骤给他打上时间戳,利用sentry及时上报延迟过大的数据业务用于调整. 针对实时性要求不高的数据 这种数据我们可以简单的通过不同渠道收集数据,然后定时做交叉检验来监控. 监控业务的变化情况 通常监控业务变化也是数据的工作,一般会构造一个后台服务,监控固定的指标,通常这些指标也是由定时任务执行得到,最终会落入数据库里.业务监控后台说白了就是从数据库中找到报表数据然后做个可视化而已.如果对界面不太在意,完全可以使用开源软件来做这个事情,个人比较推荐redash,需要更多功能也可以基于它做定制.redash是python2写的,我fork了一个python3的版本在这里,需要的可以拿去使用. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:54:09 "},"数据采集篇/":{"url":"数据采集篇/","title":"数据采集篇","keywords":"","body":"数据采集 数据采集很大程度上是一个架构问题,通常一家公司的数据有4种来源: 客户端埋点上报数据,通常用于上报分析用户操作行为 业务后台数据,通常用于保存用户在业务上的状态 后台log数据,通常用于分析业务负载 外部采集的网络数据,用于收集竞品数据或者业务相关数据分析热点和趋势 一般来说数据采集这块并不是数据科学家的工作,更多的是架构师的工作,不过为了内容的完整性这边顺带也介绍下. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:54:00 "},"数据采集篇/埋点数据采集.html":{"url":"数据采集篇/埋点数据采集.html","title":"埋点数据采集","keywords":"","body":"埋点数据采集 数据埋点是什么?数据埋点是产品经理,运营以及数据分析师基于: 业务需求(例如:CPC点击付费广告中统计每一个广告位的点击次数) 产品需求(例如:推荐系统中推荐商品的曝光次数以及点击的人数) 对用户行为的每一个事件对应的位置进行开发埋点,并通过SDK在客户端上报埋点的数据结果,记录数据汇总后进行分析,推动产品优化或指导运营. 数据埋点分为初级,中级,高级三种方式. 初级的数据埋点:在产品流程关键部位植相关统计代码,用来追踪每次用户的行为,统计关键流程的使用程度. 中级的数据埋点:在产品中植入多段代码追踪用户连续行为,建立用户访问的上下文模型来具体化用户在使用产品中的操作行为. 高级的数据埋点:与研发及数据分析师团队合作,通过数据埋点还原出用户画像及用户行为,建立数据分析后台,通过数据分析,优化产品. 数据埋点的部署方式 数据埋点主流部署的方式有两种: 私有化部署,即部署在自己公司的服务器上,如果期望提高数据安全性,或者定制化的埋点方案较多,则适合私有部署,并开发一套针对自己公司业务定制化的数据后台查询系统保证数据的安全性和精确性,缺点是成本较高. 接入第三方服务,比如Google Analytics统计,优点是成本较低,部分基础服务免费,缺点是:数据会存在不安全的风险,同时只能进行通用的简单分析,无法定制化埋点方案. 数据埋点的内容 数据埋点可以分为: 产品内部埋点:通常分析用户使用产品的行为及流程,提升用户体验.比如用户对栏目的点击行为,栏目的曝光行为等 市场埋点:分析该产品在市场上的表现及用户使用场景,比如埋点下载渠道,使用时的ip地址用于分析产品在不同市场和地域的下载量,不同地域人群使用时间等等 产品流程通常分为主干流程和分支流程,所以相应的数据埋点可以分为主干埋点和分支埋点,数据埋点通常不会一步搞定,在产品的第一次上线时通常会埋以下几个点: PV 页面浏览人次数 UV 指访问某个站点或点击某条新闻的不同IP地址的人数.在同一天内uv只记录第一次进入网站的具有独立IP的访问者,在同一天内再次访问该网站则不计数 注册量 主要流程页面之间的转化率 日活人数 移动端还要统计产品在Appstore,各大安卓市场的下载量. 之后的埋点会根据产品目标及上线后的问题进行分析.比如,当你发现产品首页的UV很高,注册量却非常低,你就需要分析出用户在首页的行为,如30%的用户退出了产品,60%的用户进入了注册页,但只有1%的用户注册了该产品.这也就意味着注册流程可能出现了问题,需要进一步细化注册各个流程,增加数据埋点,分析各个流程之间的转化率,找到产品出现的问题并解决.这是一个由粗到细,优化迭代的过程. 数据埋点分析方法 任务流程分析法:根据产品设计的任务流,在任务流开始和结束处埋点,分析用户处理任务的情况. 页面转化分析法:统计相关页面的转化率及页面元素点击率,分析用户行为. 情景分析法:列出各种用户使用场景,自己或多人体验不同场景下产品的使用流程,寻找依据设立数据埋点,通过数据反馈验证用户行为. 产品的基本数据指标 新增:新用户增加的数量和速度.如:日新增,月新增等. 活跃:有多少人正在使用产品.如日活跃(DAU),月活跃(MAU)等.用户的活跃数越多,越有可能为产品带来价值. 留存率:用户会在多长时间内使用产品.如:次日留存率,周留存率等. 传播:平均每位老用户会带来几位新用户. 流失率:一段时间内流失的用户,占这段时间内活跃用户数的比例. 基本的埋点事件分类 点击事件 点击事件,用户点击按钮即算点击事件,不管点击后有无结果; 曝光事件 成功打开一次页面记一次,刷新页面一次记一次,加载下一页新页,加载一次记一次.home键切换到后台再进入页面,曝光事件不记. 页面停留时间事件 表示一个用户在X页面的停留时长记为停留时长.例如:小明9:00访问了X网站首页,此时sdk则开始为小明这个访问者记录1个Session(会话).接着9:01小明又浏览了另外一个页面列表页,然后离开了网站(离开网站可以是通过关闭浏览器,或在地址栏键入一个不同的网址,或是点击了你网站上链接到其他网站的链接……). 埋点数据的上报格式 通常埋点数据上报会上报到固定的上报接口,一般会是一个http服务,一般使用两种Method来发送: POST,最通用的http方法,通常我们会用post方法传递.如果觉得埋点数据过大,可以使用数据压缩技术,在请求的header上设定Content-Encoding为gzip,并将Content-Length的值设置为压缩后的大小.不过是否压缩是需要权衡的,要考虑客户端的性能,避免因为数据压缩影响用户体验. HEAD,最快的http方法,因为没有body,所以我们要将数据放在header中.因为一般服务器会对header长度做限制,加上很多时候我们还使用JWT做用户身份认证,所以可能要在数据编码上下点功夫 埋点中必须要有的字段有: eventID:int事件id,一般需要一张事件表用于维护事件id,事件名,事件的定义说明 eventTime:int一般是秒级的时间戳 ip:string 用户请求的ip,通常一天中头一次打开或唤醒app,或者请求到页面时的ip地址 platform:string用户使用的是什么平台的客户端(web页面也可被视为客户端) isVisitor: bool 用户是否是游客 uID:int 用户的id,游客就为None routeName:string 页面路径 sessionID:int 用户全局唯一的id,每次打开或唤醒app,或者请求到页面时就会创建,用于标识访问的上下文,比较简单的办法是使用打开或唤醒app,或者请求到页面时的时间戳. 这些数据基本是每个埋点业务都应该上报的内容,我们可以将其归结为eventMeta即事件的元数据. 而各个埋点业务的不同之处也就是业务数据则需要做进一步的抽象,比如当前页面信息描述需要有 pageID页面ID pageStyleID页面样式ID pageExposure,页面的曝光程度 等等 这些可以统称为pageMeta即页面元数据,这些数据通常跟着业务走,并不需要全部上报. 一些数据需要描述物品的情况,通常会还有一个itemMeta即物品元数据,比如用户点击行为事件的上报,就会需要.通常会包含字段: itemID物品id itemLocalizedAt物品所在元素的位置 我们可以看到实际上埋点数据上报的应该是一个半结构化数据,因此最好是使用半结构化数据结构做序列化,比较典型的就是json.当然protobuf也是一个非常好的选择 json的优点是可读性强,通用性灵活性强方便快速开发,缺点是解析慢数据大传输效率低.protobuf则恰恰相反,性能更好但可读性灵活性不好,埋点时选型需要做好取舍. 埋点的注意事项 客户端是不可靠的,因此能在服务端做的事情尽量在服务端做,数据埋点也是这个原则,比如收藏,交易这种事件本质上都是操作业务后台数据库,必然需要调用业务后台的接口,因此完全不需要在客户端埋点,而是将相应的数据发送到后台后由后台将其处理为事件丢入消息队列,即服务端埋点. 客户端埋点 通常客户端会做埋点的数据是服务端无法做,只有客户端可以做的,通常与页面有关,因为现在页面逻辑通常不会告知后台,主要是: 页面间的跳转事件 商品曝光事件 商品点击事件 视频/音乐/小说/漫画等的使用时间 页面驻留时间 等 在客户端的上报过程中需要留心时间校准问题,由于上报的时间信息实际上是客户端的本地时间,这就很容易造成问题,对待时间校准问题,基本的方法是: 统一使用精确到毫秒的时间戳(13位整型数)以避免因为时区引起的问题 服务端校准,即上报上报时间tp和事件发生时间te两个时间戳,同时在收到消息后记录服务器时间ts,记录实际时间则为tr = te + (ts-tp),当然了这依然不能解决恶意修改上报数据的问题,通常如果有恶意上报的数据那么服务端删掉就好了,毕竟埋点数据通常不会很重要,并且埋点应该尽量减少对服务端的压力.但如果涉及一些比较隐私的内容,上报数据应该要有一个验证方式,jwt就是一个比较通用的校验方式,但毕竟token比较大并且还要做hash这样的处理,会增加服务端负载,因此能不用就不用. 客户端的数据上报有两种形式: 同步上报 最直白的上报形式,用户一有动作就上报,这种上报考验网络环境,对于有实时性要求的数据我们应该使用这种方式,比如我们的把所有可以连通到推荐页的页面的进入事件用作触发,就可以在进入推荐页前先算好要个性化推荐的结果,增加推荐系统的可用响应时间.又比如我们根据对商品的点击事件实时的搜索相近的商品用于下一次推荐 异步上报 一方面为了不影响用户体验,另一方面由于客户端数据本来就不可靠,所以有些埋点数据就可以异步发送,异步发送需要用好本地存储.一般使用Cookie或者LocalStorage来保存本地事件,然后在负载不高时取出来发送.这种事件通常是同步上报由于一些原因未成功上报的数据以及一些对实时性不敏感的数据.这种数据通常只是用于做用研或者产品优化分析的,并不会用于驱动任务 服务端埋点 服务端埋点说白了就是事件上报.通常业务服务的服务端只会讲业务的变化落到业务数据库中,服务埋点说白了是打通业务数据和数据中台的数据,而服务端要做的事情其实就是将用户的请求在落入业务数据库的同时转化为事件上报给数据中台而已,通常就是在数据消息中间件(kafka,redis,rabbitmq等)中按埋点数据规范发送一个事件而已. 可以想见服务端埋点比客户端实时性更好,也更可靠,但推广更加不易,一般为了简单,数据中台会给后端提供一个sdk用于上报. 上报后的数据流向 上报后的数据一般有几种处理方式: 记录到log并通过log收集工具(Logstash,filebeat等)收集数据,定时落库到hive,对象存储等廉价存储方案,这个流向相当于直接将埋点数据作为冷数据处理,适合定期做业务分析,资源需求最少,但对业务影响也最小,实时性最差,适合业务已经成熟,只需要做简单维护监控的业务埋点 落库到近期数据缓存工具(pg,elasticsearch等),最终根据数据热度进行下沉,这个流向实时性不错,可以实时分析业务状态,查看最近的业务状态以及提取一些轻量级的特征用于业务本身.但更加偏向于分析用途. 将埋点事件发送到消息队列(kafka,redis,rabitmq等) 这个流向可以用于事件驱动的一些业务,也可以结合流处理工具(spark-streaming,streamz等)做流处理直接提取特征,但明显的资源需求最多系统最复杂. 这三种路径往往并不是孤立的,而是会根据业务需求不同进行分流.因此埋点必须结合业务进行处理. 埋点配置化 又或者叫所谓的可视化埋点,埋点通常是个前端的工作,但前端开发人员普遍软件架构素养和数据敏感度偏低,于是近年来出现了所谓的\"可视化埋点\",也就是利用配置生成埋点.可视化只是人机交互界面而已.而配置化埋点其实并不复杂,我们可以对整个埋点行为做一个抽象--系统行为/用户交互行为触发sdk执行各种操作,因此实际上埋点配置化要做的就是: 抽象出可以用于触发的系统/用户行为,比如app启动,点击事件,加载事件等 抽象出可以缓存或者行为事件本身自带的数据用于上报的数据并规范字段比如页面id,页面被点击元素名,登录用户的id,ip地址,事件时间等 抽象出可以被触发后执行的行为比如刷新会话id,立即上报事件,插入上报事件队列等 剩下的只是定义事件名,事件id,将这些组件组合构造成配置,然后由配置生成代码而已了. 当然了说的轻松,实际要做出一个这样的系统需要花费相当多的时间人力,而且往往会和业务高度耦合,如果还要做对应的用户画像什么的则会更加深层次的和业务耦合,因此可视化埋点往往实用的没有通用性,通用的不实用. 一些第三方或开源埋点选择 awesome-analytics提供了一个常见的第三方或者开源埋点的可选列表,不希望自建埋点系统的可以在里面找找有没有合适的. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-12-31 23:19:45 "},"数据采集篇/日志数据采集.html":{"url":"数据采集篇/日志数据采集.html","title":"日志数据采集","keywords":"","body":"日志数据采集 log数据一般是整个后台都需要记录的东西,用于错误分析和负载分析,从而为架构师优化架构提供数据支持.比较常见的技术是ELK即 ElasticSearch 用于存储和查询日志数据 logstash/fluentd/filebeat用于收集日志数据 Kibana 用于作为日志分析和查询的交互界面 在使用docker的条件下,我们可以使用log-pilot来快速部署ELK,这是一个阿里开源出来的日志采集系统,支持配置化部署可以选择使用fluentd或者filebeat,跟具收集器的不同也支持输出到其他除ES外的工具上.个人更加推荐filebeat,毕竟性能更好,资源占用更低. 需要注意,目前的默认镜像使用的filebeat版本过低不支持es7.0+,可以修改项目下的dockerfile文件重新打包镜像或者干脆使用es6.0+ 在swarm上部署 log-pilot默认不支持的swarm,但我们可以在每个节点上不用swarm模式部署log-pilot,这样也可以实现在swarm集群上做日志收集,最简单的执行命令 sudo docker run --rm -it \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime \\ -v /:/host:ro \\ --cap-add SYS_ADMIN \\ -e LOGGING_OUTPUT=elasticsearch \\ -e ELASTICSEARCH_HOST=${ELASTICSEARCH_HOST} \\ -e ELASTICSEARCH_PORT=${ELASTICSEARCH_PORT} \\ registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat 使用方法 使用的时候只要使用对应的方式写上labels即可,例子如下: version: '3.2' services: xxx: image: xxx labels: - aliyun.logs.xxx=stdout ##注意就是这行 - aliyun.logs.xxx.format=json - aliyun.logs.xxx-access=/usr/local/tomcat/logs/localhost_access_log.*.txt logging: driver: json-file # 这个是默认的可以不写 options: max-size: \"10m\" max-file: \"5\" ... command: xxx label说明 可以在应用容器上添加多个标签以定义log的收集方式 指定收集的log位置 xxx指代项目名,log-pilot可以收集stdout或者指定路径上的log文件上的信息.如果是指定文件路劲,则必须是文件或者用通配符指定的多个文件,不能是文件夹. 指定log格式 quant-online.logs.$name.format为日志格式，这个格式为要收集的日志的格式，不是收集后变成的格式目前支持以下格式. none：无格式纯文本。 json：json 格式，每行一个完整的 json 字符串。 csv：csv 格式。 添加log标签 quant-online.logs.$name.tags,上报日志时，额外增加的字段，格式为 k1=v1,k2=v2，每个 key-value 之间使用逗号分隔，例如: quant-online.logs.access.tags=\"name=hello,stage=test\" 上报到存储的日志里就会出现 name 字段和 stage 字段.如果使用 ElasticSearch 作为日志存储，target 这个 tag 具有特殊含义，表示 ElasticSearch 里对应的 index.标签用于区分环境 logging说明 为了限制docker容器本身的log不过度增长,可以使用这个字段做设置,其中driver表示收集的日志驱动,一般可以不填,主要是通过如下配置限制log文件规模 options: max-size: \"10m\" max-file: \"5\" 如果使用json个是则需要填driver这个字段 log调出 查看日志有两种方案: 调用kibana查看 调用docker的api查看 推荐使用第一种方式,第二种可以作为备选方案 调用kibana查看 进入kibana,按下图管理索引, 如果之前没有索引,则可以如下图这样新建一个 有索引后就可以在如下位置查看log了 左侧可以选择要展示的key,也可以通过搜索栏搜索需要的log Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2020-01-06 22:51:56 "},"术语表/":{"url":"术语表/","title":"术语表","keywords":"","body":"数学部分的术语表 本部分的目标读者是cs或者自控做数据相关的人,对于这类人恐怕数学比计算机难的多,这部分并不是系统的讲相关数学知识,只是梳理罗列下相关概念,要深入挖掘还是找书看比较好,我会尽量列出各个部分觉得好的参考书,但顾及版权问题不会提供电子版下载地址,真有需要的可以发邮件联系我要. 离散数学 通常计算机科学作为工科使用的数学工具是离散数学,下面是其大致的结构和学习次序: 本部分主要是介绍理论部分的术语,包括: 集合论 测度论 数理逻辑 布尔代数 范畴论 图论 统计学 而数据科学,机器学习,包括人工智能实际上是应用统计学的分支,他们的理论基础来自: 微积分 概率论 线性代数 凸优化 当然统计学是建立在数学之上偏向应用的一级学科,它本身也有术语. 符号约定 通常数学表达为了简练和跨语言交流使用一套符号系统,本文现在这边列出常见的通用符号和对应的latex表达式以及含义.有时候在不同的学科领域中相同的符号代表不同的含义,这边先列出一些通用的或者默认的符号约定 符号 latex表达式 语言含义 $ A \\implies B $ A \\implies B 如果A,那么B $ A \\iff B $ A \\iff B 当且仅当,iff本身也可以代替这个符号 $ \\because $ \\because 因为 $ \\therefore $ \\therefore 因此 $ A = B $ A = B A相等于B $ A \\neq B $ A \\neq B A不等于B 计算机部分 这部分目标读者是学数学或统计但计算机不熟悉的人,相对来说这部分内容就不像上面数学部分那么艰深难懂. 计算机组成原理 编程术语 信息论 控制论 系统论 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:54:00 "},"术语表/统计/":{"url":"术语表/统计/","title":"统计","keywords":"","body":"概率与统计术语 数据科学,机器学习可以说就是应用统计学的一个分支,因此本文会有大量统计学术语.这部分罗列相关的术语和对应的解释 Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:54:00 "},"术语表/统计/误差.html":{"url":"术语表/统计/误差.html","title":"误差","keywords":"","body":"误差(errors) 一个量的观测值或计算值与其真实值或约定值之差;特指统计误差,即一个量在测量,计算或观察过程中由于某些错误或通常由于某些不可控制的因素的影响而造成的变化偏离标准值或规定值的数量,误差是不可避免的. 误差的成因 科学计算的答案通常都是近似解,近似解虽然不是我们真正想要的精确解,但是可以非常接近精确解.\"非常接近\"的意思是这个解十分接近实际或仿真成功获得的结果,因为它们实现了目标.这类近似解或相似解会受到许多因素的影响.影响因素按照产生阶段可以分成两类: 一类是在计算开始之前就有的 另一类是在计算过程中出现的。 在计算开始之前就出现近似值,主要由以下因素造成. 建模假设或无知:建模过程中可能使用了一些假设条件,没注意或者忽略了一些概念和现象的影响,最终导致了近似或可接受的误差. 观测或实验数据:从一些低精度的设备中获取的数据可能会不准确.计算过程中使用一些常量,比如π,这些常量的取值都是近似值,这也是造成计算结果与真实值有差距的重要原因. 计算的先决条件:输入数据是从前一个实验或仿真中获取的值,可能有点误差,而经过计算误差被进一步放大了.前一步的处理可能会成为之后实验的先决条件. 计算过程中导致近似值的主要因素如下. 简化问题:为了解决大而复杂的问题需要使用分治法,并不断将小难题转换成简单问题.这可能会产生近似值.而且将无限序列替换成有限序列也可能会产生近似值. 截断和舍入:许多仿真都会对中间结果进行截断和舍入操作.类似地,计算机内部表示浮点数的方法和算术运算过程也会导致些许不准确. 误差分析 误差分析(error analysis)是评估近似解对算法或计算过程准确性的影响程度的过程. 通过前面对误差成因的介绍我们可知--误差既可能出现在输入数据中,也可能在对输入数据的计算过程中产生. 如果进一步细分,计算误差还可以分为两类: 截断误差(truncation error) 截断误差是将复杂问题简化成简单问题时造成的.例如在机器学习算法中在得到需要的准确率之前粗略地中断算法迭代 舍入误差(rounding error)舍入误差是使用计算机计算时数字系统表示数字精度的规则造成的,也是在对数字进行算术运算时造成的 最终误差究竟是十分显著还是可以忽略不计,由最终数值的规模决定.例如误差10对数值15来说是十分显著的,但对785来说就不算大了,对17 685来说甚至可以忽略不计.通常误差值的影响程度与结果数值具有相关性.如果知道结果数值的量级,那么看看误差值的量级,就可以判断误差究竟是可以忽略不计还是十分显著.如果误差十分显著就要考虑引入改进手段了. 敏感度 敏感度(sensitivity或conditioning)是问题/算法的一种属性,在某些条件下,问题可以被称为敏感的或不敏感的,或者是良态的或病态的.如果输入值发生相对变化时,输出结果也会发生等比例的相对变化,就说问题是不敏感的或良态的.另一方面，如果输出结果发生的变化比输入值的变化幅度大,那么就认为问题是敏感的或病态的. 后向与前向误差估计 假设我们通过映射函数f对x进行计算获得了y*,即y*=f(x).现在真实值是y，那么微量y'=y*-y被称为前向误差(forward error),对应的估计方法称为前向误差分析.通常很难获取该估计值.另一种方法是认为y*就是同样问题带有修正 x的精确值,即y*=f (x').现在x*=x'-x就被称为y*的后向误差(backward error).后向误差分析就是对x*的参数估计过程. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:54:00 "},"术语表/信息论/信息空间.html":{"url":"术语表/信息论/信息空间.html","title":"信息空间","keywords":"","body":"信息空间(Cyber Space) 信息空间或称赛博空间是哲学和计算机领域中的一个抽象概念,指在计算机以及计算机网络里的虚拟现实,赛博空间一词是控制论(cybernetics)和空间(space)两个词的组合,是由居住在加拿大的科幻小说作家威廉·吉布森在1982年发表于\"omni\"杂志的短篇小说\"全息玫瑰碎片(Burning Chrome)\"中首次创造出来的. 2008年,美国第54号总统令对Cyberspace进行了定义: Cyberspace是信息环境中的一个整体域,它由独立且互相依存的信息基础设施和网络组成.包括互联网,电信网,计算机系统,嵌入式处理器和控制器系统. Copyright © hsz 2017 all right reserved，powered by Gitbook该文件修订时间： 2019-10-08 21:54:00 "}}