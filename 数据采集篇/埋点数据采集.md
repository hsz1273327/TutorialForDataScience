# 埋点数据采集

数据埋点是什么?数据埋点是产品经理,运营以及数据分析师基于:

+ 业务需求(例如:CPC点击付费广告中统计每一个广告位的点击次数)
+ 产品需求(例如:推荐系统中推荐商品的曝光次数以及点击的人数)

对用户行为的每一个事件对应的位置进行开发埋点,并通过SDK在客户端上报埋点的数据结果,记录数据汇总后进行分析,推动产品优化或指导运营.

数据埋点分为初级,中级,高级三种方式.

+ 初级的数据埋点:在产品流程关键部位植相关统计代码,用来追踪每次用户的行为,统计关键流程的使用程度.

+ 中级的数据埋点:在产品中植入多段代码追踪用户连续行为,建立用户访问的上下文模型来具体化用户在使用产品中的操作行为.

+ 高级的数据埋点:与研发及数据分析师团队合作,通过数据埋点还原出用户画像及用户行为,建立数据分析后台,通过数据分析,优化产品.



## 数据埋点的部署方式

数据埋点主流部署的方式有两种:

+ 私有化部署,即部署在自己公司的服务器上,如果期望提高数据安全性,或者定制化的埋点方案较多,则适合私有部署,并开发一套针对自己公司业务定制化的数据后台查询系统保证数据的安全性和精确性,缺点是成本较高.

+ 接入第三方服务,比如Google Analytics统计,优点是成本较低,部分基础服务免费,缺点是:数据会存在不安全的风险,同时只能进行通用的简单分析,无法定制化埋点方案.

## 数据埋点的内容

数据埋点可以分为:

+ 产品内部埋点:通常分析用户使用产品的行为及流程,提升用户体验.比如用户对栏目的点击行为,栏目的曝光行为等
+ 市场埋点:分析该产品在市场上的表现及用户使用场景,比如埋点下载渠道,使用时的ip地址用于分析产品在不同市场和地域的下载量,不同地域人群使用时间等等

产品流程通常分为主干流程和分支流程,所以相应的数据埋点可以分为主干埋点和分支埋点,数据埋点通常不会一步搞定,在产品的第一次上线时通常会埋以下几个点:

+ PV 页面浏览人次数
+ UV 指访问某个站点或点击某条新闻的不同IP地址的人数.在同一天内uv只记录第一次进入网站的具有独立IP的访问者,在同一天内再次访问该网站则不计数
+ 注册量
+ 主要流程页面之间的转化率
+ 日活人数
+ 移动端还要统计产品在Appstore,各大安卓市场的下载量.

之后的埋点会根据产品目标及上线后的问题进行分析.比如,当你发现产品首页的UV很高,注册量却非常低,你就需要分析出用户在首页的行为,如30%的用户退出了产品,60%的用户进入了注册页,但只有1%的用户注册了该产品.这也就意味着注册流程可能出现了问题,需要进一步细化注册各个流程,增加数据埋点,分析各个流程之间的转化率,找到产品出现的问题并解决.这是一个由粗到细,优化迭代的过程.

## 数据埋点分析方法

+ 任务流程分析法:根据产品设计的任务流,在任务流开始和结束处埋点,分析用户处理任务的情况.

+ 页面转化分析法:统计相关页面的转化率及页面元素点击率,分析用户行为.

+ 情景分析法:列出各种用户使用场景,自己或多人体验不同场景下产品的使用流程,寻找依据设立数据埋点,通过数据反馈验证用户行为.


## 产品的基本数据指标

+ 新增:新用户增加的数量和速度.如:日新增,月新增等.

+ 活跃:有多少人正在使用产品.如日活跃(DAU),月活跃(MAU)等.用户的活跃数越多,越有可能为产品带来价值.

+ 留存率:用户会在多长时间内使用产品.如:次日留存率,周留存率等.

+ 传播:平均每位老用户会带来几位新用户.

+ 流失率:一段时间内流失的用户,占这段时间内活跃用户数的比例.


## 基本的埋点事件分类

+ 点击事件

    点击事件,用户点击按钮即算点击事件,不管点击后有无结果;

+ 曝光事件

    成功打开一次页面记一次,刷新页面一次记一次,加载下一页新页,加载一次记一次.home键切换到后台再进入页面,曝光事件不记.

+ 页面停留时间事件

    表示一个用户在X页面的停留时长记为停留时长.例如:小明9:00访问了X网站首页,此时sdk则开始为小明这个访问者记录1个Session(会话).接着9:01小明又浏览了另外一个页面列表页,然后离开了网站(离开网站可以是通过关闭浏览器,或在地址栏键入一个不同的网址,或是点击了你网站上链接到其他网站的链接……).
    
## 埋点数据的上报格式

通常埋点数据上报会上报到固定的上报接口,一般会是一个http服务,一般使用两种Method来发送:

+ `POST`,最通用的http方法,通常我们会用post方法传递json.如果觉得埋点数据过大,可以使用数据压缩技术,在请求的header上设定`Content-Encoding`为gzip,并将`Content-Length`的值设置为压缩后的大小.不过是否压缩是需要权衡的,要考虑客户端的性能,避免因为数据压缩影响用户体验.

+ `HEAD`,最快的http方法,因为没有body,所以我们要将数据放在header中.因为一般服务器会对header长度做限制,加上很多时候我们还使用JWT做用户身份认证,所以可能要在数据编码上下点功夫

埋点中必须要有的字段有:

+ `eventID:int`事件id,一般需要一张事件表用于维护事件id,事件名,事件的定义说明
+ `eventTime:int`一般是秒级的时间戳
+ `ip:string` 用户请求的ip,通常一天中头一次打开或唤醒app,或者请求到页面时的ip地址
+ `isVisitor: bool` 用户是否是游客
+ `uID:int` 用户的id,游客就为None
+ `routeName:string` 页面路径
+ `sessionID:int` 用户全局唯一的id,每次打开或唤醒app,或者请求到页面时就会创建,用于标识访问的上下文,比较简单的办法是使用打开或唤醒app,或者请求到页面时的时间戳.


## 客户端的埋点的注意事项

客户端是不可靠的,因此能在服务端做的事情尽量在服务端做,数据埋点也是这个原则,比如收藏,交易这种事件本质上都是操作业务后台数据库,必然需要调用业务后台的接口,因此完全不需要在客户端埋点,而是将相应的数据发送到后台后由后台将其处理为事件丢入kafka.

通常客户端会做埋点的数据是服务端无法做,只有客户端可以做的,通常与页面有关,因为现在页面逻辑通常不会告知后台,主要是:

+ 页面间的跳转事件
+ 商品曝光事件
+ 商品点击事件
+ 视屏/音乐/小说/漫画等的使用时间
+ 页面驻留时间

客户端的数据上报有两种形式:

+ 同步上报

    最直白的上报形式,用户一有动作就上报,这种上报考验网络环境,但如果做得好可以用它来做异步请求的触发条件,比如我们的把所有可以连通到推荐页的页面的进入事件用作触发,就可以在进入推荐页前先算好要个性化推荐的结果,增加推荐系统的可用响应时间.又比如我们根据对商品的点击事件实时的搜索相近的商品用于下一次推荐

+ 异步上报
    
    一方面为了不影响用户体验,另一方面由于客户端数据本来就不可靠,所以有些埋点数据就可以异步发送,异步发送需要用好本地存储.一般使用Cookie或者LocalStorage来保存本地事件,然后在负载不高时取出来发送.这种事件通常是同步上报由于一些原因未成功上报的数据.

## 上报后的数据流向

服务端收到上报的数据后一般直接丢进kafka作为事件,然后根据时间戳落库,一般是1小时存一次,存到hdfs.



